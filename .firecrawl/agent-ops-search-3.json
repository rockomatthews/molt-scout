{"success":true,"data":{"web":[{"url":"https://developers.openai.com/api/docs/guides/prompt-caching/","title":"Prompt caching | OpenAI API","description":"Prompt Caching can reduce latency by up to 80% and input token costs by up to 90%. Prompt Caching works automatically on all your API requests (no code changes ...","position":1,"markdown":"## Search the API docs\n\nClose\n\nClear\n\nPrimary navigation\n\nClear\n\n### Get started\n\n- [Overview](https://developers.openai.com/api/docs)\n- [Quickstart](https://developers.openai.com/api/docs/quickstart)\n- [Models](https://developers.openai.com/api/docs/models)\n- [Pricing](https://developers.openai.com/api/docs/pricing)\n- [Libraries](https://developers.openai.com/api/docs/libraries)\n- [Docs MCP](https://developers.openai.com/resources/docs-mcp)\n- [Latest: GPT-5.2](https://developers.openai.com/api/docs/guides/latest-model)\n\n### Core concepts\n\n- [Text generation](https://developers.openai.com/api/docs/guides/text)\n- [Code generation](https://developers.openai.com/api/docs/guides/code-generation)\n- [Images and vision](https://developers.openai.com/api/docs/guides/images-vision)\n- [Audio and speech](https://developers.openai.com/api/docs/guides/audio)\n- [Structured output](https://developers.openai.com/api/docs/guides/structured-outputs)\n- [Function calling](https://developers.openai.com/api/docs/guides/function-calling)\n- [Responses API](https://developers.openai.com/api/docs/guides/migrate-to-responses)\n\n### Agents\n\n- [Overview](https://developers.openai.com/api/docs/guides/agents)\n- Build agents\n\n  - [Agent Builder](https://developers.openai.com/api/docs/guides/agent-builder)\n  - [Node reference](https://developers.openai.com/api/docs/guides/node-reference)\n  - [Safety in building agents](https://developers.openai.com/api/docs/guides/agent-builder-safety)\n  - [Agents SDK](https://developers.openai.com/api/docs/guides/agents-sdk)\n\n- Deploy in your product\n\n  - [ChatKit](https://developers.openai.com/api/docs/guides/chatkit)\n  - [Custom theming](https://developers.openai.com/api/docs/guides/chatkit-themes)\n  - [Widgets](https://developers.openai.com/api/docs/guides/chatkit-widgets)\n  - [Actions](https://developers.openai.com/api/docs/guides/chatkit-actions)\n  - [Advanced integration](https://developers.openai.com/api/docs/guides/custom-chatkit)\n\n- Optimize\n\n  - [Agent evals](https://developers.openai.com/api/docs/guides/agent-evals)\n  - [Trace grading](https://developers.openai.com/api/docs/guides/trace-grading)\n\n- [Voice agents](https://developers.openai.com/api/docs/guides/voice-agents)\n\n### Tools\n\n- [Using tools](https://developers.openai.com/api/docs/guides/tools)\n- [Connectors and MCP](https://developers.openai.com/api/docs/guides/tools-connectors-mcp)\n- [Skills](https://developers.openai.com/api/docs/guides/tools-skills)\n- [Shell](https://developers.openai.com/api/docs/guides/tools-shell)\n- [Web search](https://developers.openai.com/api/docs/guides/tools-web-search)\n- [Code interpreter](https://developers.openai.com/api/docs/guides/tools-code-interpreter)\n- File search and retrieval\n\n  - [File search](https://developers.openai.com/api/docs/guides/tools-file-search)\n  - [Retrieval](https://developers.openai.com/api/docs/guides/retrieval)\n\n- More tools\n\n  - [Image generation](https://developers.openai.com/api/docs/guides/tools-image-generation)\n  - [Computer use](https://developers.openai.com/api/docs/guides/tools-computer-use)\n  - [Local shell tool](https://developers.openai.com/api/docs/guides/tools-local-shell)\n  - [Apply patch](https://developers.openai.com/api/docs/guides/tools-apply-patch)\n\n### Run and scale\n\n- [Conversation state](https://developers.openai.com/api/docs/guides/conversation-state)\n- [Compaction](https://developers.openai.com/api/docs/guides/compaction)\n- [Background mode](https://developers.openai.com/api/docs/guides/background)\n- [Streaming](https://developers.openai.com/api/docs/guides/streaming-responses)\n- [Webhooks](https://developers.openai.com/api/docs/guides/webhooks)\n- [File inputs](https://developers.openai.com/api/docs/guides/pdf-files)\n- Prompting\n\n  - [Overview](https://developers.openai.com/api/docs/guides/prompting)\n  - [Prompt caching](https://developers.openai.com/api/docs/guides/prompt-caching)\n  - [Prompt engineering](https://developers.openai.com/api/docs/guides/prompt-engineering)\n\n- Reasoning\n\n  - [Reasoning models](https://developers.openai.com/api/docs/guides/reasoning)\n  - [Reasoning best practices](https://developers.openai.com/api/docs/guides/reasoning-best-practices)\n\n### Evaluation\n\n- [Getting started](https://developers.openai.com/api/docs/guides/evaluation-getting-started)\n- [Working with evals](https://developers.openai.com/api/docs/guides/evals)\n- [Prompt optimizer](https://developers.openai.com/api/docs/guides/prompt-optimizer)\n- [External models](https://developers.openai.com/api/docs/guides/external-models)\n- [Best practices](https://developers.openai.com/api/docs/guides/evaluation-best-practices)\n\n### Realtime API\n\n- [Overview](https://developers.openai.com/api/docs/guides/realtime)\n- Connect\n\n  - [WebRTC](https://developers.openai.com/api/docs/guides/realtime-webrtc)\n  - [WebSocket](https://developers.openai.com/api/docs/guides/realtime-websocket)\n  - [SIP](https://developers.openai.com/api/docs/guides/realtime-sip)\n\n- Usage\n\n  - [Using realtime models](https://developers.openai.com/api/docs/guides/realtime-models-prompting)\n  - [Managing conversations](https://developers.openai.com/api/docs/guides/realtime-conversations)\n  - [Webhooks and server-side controls](https://developers.openai.com/api/docs/guides/realtime-server-controls)\n  - [Managing costs](https://developers.openai.com/api/docs/guides/realtime-costs)\n  - [Realtime transcription](https://developers.openai.com/api/docs/guides/realtime-transcription)\n  - [Voice agents](https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/)\n\n### Model optimization\n\n- [Optimization cycle](https://developers.openai.com/api/docs/guides/model-optimization)\n- Fine-tuning\n\n  - [Supervised fine-tuning](https://developers.openai.com/api/docs/guides/supervised-fine-tuning)\n  - [Vision fine-tuning](https://developers.openai.com/api/docs/guides/vision-fine-tuning)\n  - [Direct preference optimization](https://developers.openai.com/api/docs/guides/direct-preference-optimization)\n  - [Reinforcement fine-tuning](https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning)\n  - [RFT use cases](https://developers.openai.com/api/docs/guides/rft-use-cases)\n  - [Best practices](https://developers.openai.com/api/docs/guides/fine-tuning-best-practices)\n\n- [Graders](https://developers.openai.com/api/docs/guides/graders)\n\n### Specialized models\n\n- [Image generation](https://developers.openai.com/api/docs/guides/image-generation)\n- [Video generation](https://developers.openai.com/api/docs/guides/video-generation)\n- [Text to speech](https://developers.openai.com/api/docs/guides/text-to-speech)\n- [Speech to text](https://developers.openai.com/api/docs/guides/speech-to-text)\n- [Deep research](https://developers.openai.com/api/docs/guides/deep-research)\n- [Embeddings](https://developers.openai.com/api/docs/guides/embeddings)\n- [Moderation](https://developers.openai.com/api/docs/guides/moderation)\n\n### Coding agents\n\n- [Codex cloud](https://developers.openai.com/codex/cloud)\n- [Agent internet access](https://developers.openai.com/codex/cloud/agent-internet)\n- [Codex CLI](https://developers.openai.com/codex/cli)\n- [Codex IDE](https://developers.openai.com/codex/ide)\n- [Codex changelog](https://developers.openai.com/codex/changelog)\n\n### Going live\n\n- [Production best practices](https://developers.openai.com/api/docs/guides/production-best-practices)\n- Latency optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/latency-optimization)\n  - [Predicted Outputs](https://developers.openai.com/api/docs/guides/predicted-outputs)\n  - [Priority processing](https://developers.openai.com/api/docs/guides/priority-processing)\n\n- Cost optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/cost-optimization)\n  - [Batch](https://developers.openai.com/api/docs/guides/batch)\n  - [Flex processing](https://developers.openai.com/api/docs/guides/flex-processing)\n\n- [Accuracy optimization](https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy)\n- Safety\n\n  - [Safety best practices](https://developers.openai.com/api/docs/guides/safety-best-practices)\n  - [Safety checks](https://developers.openai.com/api/docs/guides/safety-checks)\n  - [Under 18 API Guidance](https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance)\n\n### Legacy APIs\n\n- Assistants API\n\n  - [Migration guide](https://developers.openai.com/api/docs/assistants/migration)\n  - [Deep dive](https://developers.openai.com/api/docs/assistants/deep-dive)\n  - [Tools](https://developers.openai.com/api/docs/assistants/tools)\n\n### Resources\n\n- [Terms and policies](https://openai.com/policies)\n- [Changelog](https://developers.openai.com/api/docs/changelog)\n- [Your data](https://developers.openai.com/api/docs/guides/your-data)\n- [Permissions](https://developers.openai.com/api/docs/guides/rbac)\n- [Rate limits](https://developers.openai.com/api/docs/guides/rate-limits)\n- [Deprecations](https://developers.openai.com/api/docs/deprecations)\n- [MCP for deep research](https://developers.openai.com/api/docs/mcp)\n- [Developer mode](https://developers.openai.com/api/docs/guides/developer-mode)\n- ChatGPT Actions\n\n  - [Introduction](https://developers.openai.com/api/docs/actions/introduction)\n  - [Getting started](https://developers.openai.com/api/docs/actions/getting-started)\n  - [Actions library](https://developers.openai.com/api/docs/actions/actions-library)\n  - [Authentication](https://developers.openai.com/api/docs/actions/authentication)\n  - [Production](https://developers.openai.com/api/docs/actions/production)\n  - [Data retrieval](https://developers.openai.com/api/docs/actions/data-retrieval)\n  - [Sending files](https://developers.openai.com/api/docs/actions/sending-files)\n\n[API Dashboard](https://platform.openai.com/login)\n\nSearch\n⌘\n\nK\n\nCopy PageMore page actions\n\nCopy PageMore page actions\n\nModel prompts often contain repetitive content, like system prompts and common instructions. OpenAI routes API requests to servers that recently processed the same prompt, making it cheaper and faster than processing a prompt from scratch. Prompt Caching can reduce latency by up to 80% and input token costs by up to 90%. Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it. Prompt Caching is enabled for all recent [models](https://developers.openai.com/api/docs/models), gpt-4o and newer.\n\nThis guide describes how Prompt Caching works in detail, so that you can optimize your prompts for lower latency and cost.\n\n## Structuring prompts\n\nCache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n\n![Prompt Caching visualization](https://openaidevs.retool.com/api/file/8593d9bb-4edb-4eb6-bed9-62bfb98db5ee)\n\n## How it works\n\nCaching is enabled automatically for prompts that are 1024 tokens or longer. When you make an API request, the following steps occur:\n\n1. **Cache Routing**:\n\n- Requests are routed to a machine based on a hash of the initial prefix of the prompt. The hash typically uses the first 256 tokens, though the exact length varies depending on the model.\n- If you provide the [`prompt_cache_key`](https://developers.openai.com/api/docs/api-reference/responses/create#responses-create-prompt_cache_key) parameter, it is combined with the prefix hash, allowing you to influence routing and improve cache hit rates. This is especially beneficial when many requests share long, common prefixes.\n- If requests for the same prefix and `prompt_cache_key` combination exceed a certain rate (approximately 15 requests per minute), some may overflow and get routed to additional machines, reducing cache effectiveness.\n\n2. **Cache Lookup**: The system checks if the initial portion (prefix) of your prompt exists in the cache on the selected machine.\n3. **Cache Hit**: If a matching prefix is found, the system uses the cached result. This significantly decreases latency and reduces costs.\n4. **Cache Miss**: If no matching prefix is found, the system processes your full prompt, caching the prefix afterward on that machine for future requests.\n\n## Prompt cache retention\n\nPrompt Caching can either use in-memory or extended retention policies. When available, Extended Prompt Caching aims to retain the cache for longer, so that subsequent requests are more likely to match the cache.\n\nPrompt cache pricing is the same for both retention policies.\n\nTo configure the prompt cache retention policy, set the `prompt_cache_retention` parameter on your `Responses.create` request (or `chat.completions.create` if using Chat Completions).\n\n### In-memory prompt cache retention\n\nIn-memory prompt cache retention is available for all models that support Prompt Caching.\n\nWhen using the in-memory policy, cached prefixes generally remain active for 5 to 10 minutes of inactivity, up to a maximum of one hour. In-memory cached prefixes are only held within volatile GPU memory.\n\n### Extended prompt cache retention\n\nExtended prompt cache retention is available for the following models:\n\n- gpt-5.2\n- gp5-5.1-codex-max\n- gpt-5.1\n- gpt-5.1-codex\n- gpt-5.1-codex-mini\n- gpt-5.1-chat-latest\n- gpt-5\n- gpt-5-codex\n- gpt-4.1\n\nExtended prompt cache retention keeps cached prefixes active for longer, up to a maximum of 24 hours. Extended Prompt Caching works by offloading the key/value tensors to GPU-local storage when memory is full, significantly increasing the storage capacity available for caching.\n\nkey/value tensors are the intermediate representation from the model’s attention layers produced during prefill. Only the key/value tensors may be persisted in local storage; the original customer content, such as prompt text, is only retained in memory.\n\n### Configure per request\n\nIf you don’t specify a retention policy, the default is `in_memory`. Allowed values are `in_memory` and `24h`.\n\n```\n1\n2\n3\n4\n5\n{\n  \"model\": \"gpt-5.1\",\n  \"input\": \"Your prompt goes here...\",\n  \"prompt_cache_retention\": \"24h\"\n}\n```\n\n## Requirements\n\nCaching is available for prompts containing 1024 tokens or more.\n\nAll requests, including those with fewer than 1024 tokens, will display a `cached_tokens` field of the `usage.prompt_tokens_details` [Response object](https://developers.openai.com/api/docs/api-reference/responses/object) or [Chat object](https://developers.openai.com/api/docs/api-reference/chat/object) indicating how many of the prompt tokens were a cache hit. For requests under 1024 tokens, `cached_tokens` will be zero.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\"usage\": {\n  \"prompt_tokens\": 2006,\n  \"completion_tokens\": 300,\n  \"total_tokens\": 2306,\n  \"prompt_tokens_details\": {\n    \"cached_tokens\": 1920\n  },\n  \"completion_tokens_details\": {\n    \"reasoning_tokens\": 0,\n    \"accepted_prediction_tokens\": 0,\n    \"rejected_prediction_tokens\": 0\n  }\n}\n```\n\n### What can be cached\n\n- **Messages:** The complete messages array, encompassing system, user, and assistant interactions.\n- **Images:** Images included in user messages, either as links or as base64-encoded data, as well as multiple images can be sent. Ensure the detail parameter is set identically, as it impacts image tokenization.\n- **Tool use:** Both the messages array and the list of available `tools` can be cached, contributing to the minimum 1024 token requirement.\n- **Structured outputs:** The structured output schema serves as a prefix to the system message and can be cached.\n\n## Best practices\n\n- Structure prompts with **static or repeated content at the beginning** and dynamic, user-specific content at the end.\n- Use the **[`prompt_cache_key`](https://developers.openai.com/api/docs/api-reference/responses/create#responses-create-prompt_cache_key) parameter** consistently across requests that share common prefixes. Select a granularity that keeps each unique prefix-`prompt_cache_key` combination below 15 requests per minute to avoid cache overflow.\n- **Monitor your cache performance metrics**, including cache hit rates, latency, and the proportion of tokens cached, to refine your strategy. You can monitor your cached token counts by logging the usage field results as shown above, or in the OpenAI Usage dashboard.\n- **Maintain a steady stream of requests** with identical prompt prefixes to minimize cache evictions and maximize caching benefits.\n\n## Frequently asked questions\n\n1. **How is data privacy maintained for caches?**\n\nPrompt caches are not shared between organizations. Only members of the same organization can access caches of identical prompts.\n\n2. **Does Prompt Caching affect output token generation or the final response of the API?**\n\nPrompt Caching does not influence the generation of output tokens or the final response provided by the API. Regardless of whether caching is used, the output generated will be identical. This is because only the prompt itself is cached, while the actual response is computed anew each time based on the cached prompt.\n\n3. **Is there a way to manually clear the cache?**\n\nManual cache clearing is not currently available. Prompts that have not been encountered recently are automatically cleared from the cache. Typical cache evictions occur after 5-10 minutes of inactivity, though sometimes lasting up to a maximum of one hour during off-peak periods.\n\n4. **Will I be expected to pay extra for writing to Prompt Caching?**\n\nNo. Caching happens automatically, with no explicit action needed or extra cost paid to use the caching feature.\n\n5. **Do cached prompts contribute to TPM rate limits?**\n\nYes, as caching does not affect rate limits.\n\n6. **Does Prompt Caching work on Zero Data Retention requests?**\n\nIn-memory cache retention is Zero Data Retention eligible.\nIf you specify extended caching in the request, then that request is not considered Zero Data Retention eligible because the key/value tensors may be held in GPU-local storage, and the key-value tensors are derived from customer content.\nHowever, the extended caching request will not be blocked if Zero Data Retention is enabled for your project. The other Zero Data Retention still applies, such as excluding customer content from abuse logs and preventing use of `store=True`.\nSee the [Your data](https://developers.openai.com/api/docs/guides/your-data) guide for more context on Zero Data Retention.\n\n7. **Does Prompt Caching work with Data Residency?**\n\nIn-memory Prompt Caching is compatable with all Data Residency regions.\n\nExtended caching is only compatible with Data Residency regions that include Regional Inference.","metadata":{"og:type":"website","title":"Prompt caching | OpenAI API","og:image":"https://developers.openai.com/open-graph.png","twitter:card":"summary_large_image","description":"Learn how prompt caching reduces latency and cost for long prompts in OpenAI's API.","twitter:description":"Learn how prompt caching reduces latency and cost for long prompts in OpenAI's API.","ogUrl":"https://developers.openai.com/api/docs/guides/prompt-caching/","og:url":"https://developers.openai.com/api/docs/guides/prompt-caching/","og:description":"Learn how prompt caching reduces latency and cost for long prompts in OpenAI's API.","twitter:url":"https://developers.openai.com/api/docs/guides/prompt-caching/","generator":"Astro v5.16.15","ogImage":"https://developers.openai.com/open-graph.png","ogDescription":"Learn how prompt caching reduces latency and cost for long prompts in OpenAI's API.","twitter:title":"Prompt caching | OpenAI API","twitter:image":"https://developers.openai.com/open-graph.png","og:title":"Prompt caching | OpenAI API","language":"en","ogTitle":"Prompt caching | OpenAI API","viewport":"width=device-width,initial-scale=1","favicon":"https://developers.openai.com/favicon.png","scrapeId":"019c62eb-509c-7409-9bdc-0e87eff63fee","sourceURL":"https://developers.openai.com/api/docs/guides/prompt-caching/","url":"https://developers.openai.com/api/docs/guides/prompt-caching","statusCode":200,"contentType":"text/html; charset=utf-8","proxyUsed":"basic","cacheState":"hit","cachedAt":"2026-02-14T00:10:58.995Z","creditsUsed":1},"links":["https://developers.openai.com/api/docs","https://developers.openai.com/api/docs/quickstart","https://developers.openai.com/api/docs/models","https://developers.openai.com/api/docs/pricing","https://developers.openai.com/api/docs/libraries","https://developers.openai.com/resources/docs-mcp","https://developers.openai.com/api/docs/guides/latest-model","https://developers.openai.com/api/docs/guides/text","https://developers.openai.com/api/docs/guides/code-generation","https://developers.openai.com/api/docs/guides/images-vision","https://developers.openai.com/api/docs/guides/audio","https://developers.openai.com/api/docs/guides/structured-outputs","https://developers.openai.com/api/docs/guides/function-calling","https://developers.openai.com/api/docs/guides/migrate-to-responses","https://developers.openai.com/api/docs/guides/agents","https://developers.openai.com/api/docs/guides/agent-builder","https://developers.openai.com/api/docs/guides/node-reference","https://developers.openai.com/api/docs/guides/agent-builder-safety","https://developers.openai.com/api/docs/guides/agents-sdk","https://developers.openai.com/api/docs/guides/chatkit","https://developers.openai.com/api/docs/guides/chatkit-themes","https://developers.openai.com/api/docs/guides/chatkit-widgets","https://developers.openai.com/api/docs/guides/chatkit-actions","https://developers.openai.com/api/docs/guides/custom-chatkit","https://developers.openai.com/api/docs/guides/agent-evals","https://developers.openai.com/api/docs/guides/trace-grading","https://developers.openai.com/api/docs/guides/voice-agents","https://developers.openai.com/api/docs/guides/tools","https://developers.openai.com/api/docs/guides/tools-connectors-mcp","https://developers.openai.com/api/docs/guides/tools-skills","https://developers.openai.com/api/docs/guides/tools-shell","https://developers.openai.com/api/docs/guides/tools-web-search","https://developers.openai.com/api/docs/guides/tools-code-interpreter","https://developers.openai.com/api/docs/guides/tools-file-search","https://developers.openai.com/api/docs/guides/retrieval","https://developers.openai.com/api/docs/guides/tools-image-generation","https://developers.openai.com/api/docs/guides/tools-computer-use","https://developers.openai.com/api/docs/guides/tools-local-shell","https://developers.openai.com/api/docs/guides/tools-apply-patch","https://developers.openai.com/api/docs/guides/conversation-state","https://developers.openai.com/api/docs/guides/compaction","https://developers.openai.com/api/docs/guides/background","https://developers.openai.com/api/docs/guides/streaming-responses","https://developers.openai.com/api/docs/guides/webhooks","https://developers.openai.com/api/docs/guides/pdf-files","https://developers.openai.com/api/docs/guides/prompting","https://developers.openai.com/api/docs/guides/prompt-caching","https://developers.openai.com/api/docs/guides/prompt-engineering","https://developers.openai.com/api/docs/guides/reasoning","https://developers.openai.com/api/docs/guides/reasoning-best-practices","https://developers.openai.com/api/docs/guides/evaluation-getting-started","https://developers.openai.com/api/docs/guides/evals","https://developers.openai.com/api/docs/guides/prompt-optimizer","https://developers.openai.com/api/docs/guides/external-models","https://developers.openai.com/api/docs/guides/evaluation-best-practices","https://developers.openai.com/api/docs/guides/realtime","https://developers.openai.com/api/docs/guides/realtime-webrtc","https://developers.openai.com/api/docs/guides/realtime-websocket","https://developers.openai.com/api/docs/guides/realtime-sip","https://developers.openai.com/api/docs/guides/realtime-models-prompting","https://developers.openai.com/api/docs/guides/realtime-conversations","https://developers.openai.com/api/docs/guides/realtime-server-controls","https://developers.openai.com/api/docs/guides/realtime-costs","https://developers.openai.com/api/docs/guides/realtime-transcription","https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/","https://developers.openai.com/api/docs/guides/model-optimization","https://developers.openai.com/api/docs/guides/supervised-fine-tuning","https://developers.openai.com/api/docs/guides/vision-fine-tuning","https://developers.openai.com/api/docs/guides/direct-preference-optimization","https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning","https://developers.openai.com/api/docs/guides/rft-use-cases","https://developers.openai.com/api/docs/guides/fine-tuning-best-practices","https://developers.openai.com/api/docs/guides/graders","https://developers.openai.com/api/docs/guides/image-generation","https://developers.openai.com/api/docs/guides/video-generation","https://developers.openai.com/api/docs/guides/text-to-speech","https://developers.openai.com/api/docs/guides/speech-to-text","https://developers.openai.com/api/docs/guides/deep-research","https://developers.openai.com/api/docs/guides/embeddings","https://developers.openai.com/api/docs/guides/moderation","https://developers.openai.com/codex/cloud","https://developers.openai.com/codex/cloud/agent-internet","https://developers.openai.com/codex/cli","https://developers.openai.com/codex/ide","https://developers.openai.com/codex/changelog","https://developers.openai.com/api/docs/guides/production-best-practices","https://developers.openai.com/api/docs/guides/latency-optimization","https://developers.openai.com/api/docs/guides/predicted-outputs","https://developers.openai.com/api/docs/guides/priority-processing","https://developers.openai.com/api/docs/guides/cost-optimization","https://developers.openai.com/api/docs/guides/batch","https://developers.openai.com/api/docs/guides/flex-processing","https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy","https://developers.openai.com/api/docs/guides/safety-best-practices","https://developers.openai.com/api/docs/guides/safety-checks","https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance","https://developers.openai.com/api/docs/assistants/migration","https://developers.openai.com/api/docs/assistants/deep-dive","https://developers.openai.com/api/docs/assistants/tools","https://openai.com/policies","https://developers.openai.com/api/docs/changelog","https://developers.openai.com/api/docs/guides/your-data","https://developers.openai.com/api/docs/guides/rbac","https://developers.openai.com/api/docs/guides/rate-limits","https://developers.openai.com/api/docs/deprecations","https://developers.openai.com/api/docs/mcp","https://developers.openai.com/api/docs/guides/developer-mode","https://developers.openai.com/api/docs/actions/introduction","https://developers.openai.com/api/docs/actions/getting-started","https://developers.openai.com/api/docs/actions/actions-library","https://developers.openai.com/api/docs/actions/authentication","https://developers.openai.com/api/docs/actions/production","https://developers.openai.com/api/docs/actions/data-retrieval","https://developers.openai.com/api/docs/actions/sending-files","https://platform.openai.com/login","https://developers.openai.com/api/docs/api-reference/responses/create#responses-create-prompt_cache_key","https://developers.openai.com/api/docs/api-reference/responses/object","https://developers.openai.com/api/docs/api-reference/chat/object"]},{"url":"https://developers.openai.com/cookbook/examples/prompt_caching101/","title":"Prompt Caching 101 - OpenAI for developers","description":"OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts ...","position":2,"markdown":"## Search the cookbook\n\nClose\n\nClear\n\nPrimary navigation\n\nClear\n\nResources  Cookbook  Blog\n\n- [Home](https://developers.openai.com/cookbook)\n\n### Topics\n\n- [Agents](https://developers.openai.com/cookbook/topic/agents)\n- [Evals](https://developers.openai.com/cookbook/topic/evals)\n- [Multimodal](https://developers.openai.com/cookbook/topic/multimodal)\n- [Text](https://developers.openai.com/cookbook/topic/text)\n- [Guardrails](https://developers.openai.com/cookbook/topic/guardrails)\n- [Optimization](https://developers.openai.com/cookbook/topic/optimization)\n- [ChatGPT](https://developers.openai.com/cookbook/topic/chatgpt)\n- [Codex](https://developers.openai.com/cookbook/topic/codex)\n- [gpt-oss](https://developers.openai.com/cookbook/topic/gpt-oss)\n\n### Contribute\n\n- [Cookbook on GitHub](https://github.com/openai/openai-cookbook)\n\n[API Dashboard](https://platform.openai.com/login)\n\nSearch\n⌘\n\nK\n\nCopy PageMore page actions\n\nCopy PageMore page actions\n\nOct 1, 2024\n\n# Prompt Caching 101\n\n[![Charu Jaiswal](https://avatars.githubusercontent.com/u/18404643?v=4) CJ](https://www.linkedin.com/in/charu-j-8a866471)\n\n[Charu Jaiswal\\\\\n(OpenAI)](https://www.linkedin.com/in/charu-j-8a866471)\n\n[View on GitHub](https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb) [Download raw](https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Prompt_Caching101.ipynb)\n\nOpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts over 10,000 tokens. By caching repetitive information across LLM API requests, you can greatly reduce both latency and costs. Prompt caching is scoped at the organization level, meaning only members of the same organization can access shared caches. Additionally, caching is eligible for zero data retention, as no data is stored during the process.\n\nPrompt caching automatically activates for prompts longer than 1024 tokens— you don’t have to change anything in your completions request. When an API request is made, the system first checks if the beginning portion (prefix) of the prompt has already been cached. If a match is found (cache hit), the cached prompt is used, leading to reduced latency and costs. If there’s no match, the system processes the full prompt from scratch and caches the prefix for future use.\n\nWith these benefits in mind, some of the key use cases where prompt caching can be especially advantageous are:\n\n- **Agents using tools and structured outputs**: Cache the extended list of tools and schemas.\n- **Coding and writing assistants**: Insert large sections or summaries of codebases and workspaces directly in prompts.\n- **Chatbots**: Cache static portions of multi-turn conversations to maintain context efficiently over extended dialogues.\n\nIn this cookbook, we’ll go through a couple examples of caching tools and images. Recall that in general, you’ll want to put static content like instructions and examples at the beginning of your prompt, and variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical even in their ordering between requests. All requests, including those with fewer than 1024 tokens, will display a cached\\_tokens field of the `usage.prompt_tokens_details` chat completions object indicating how many of the prompt tokens were a cache hit. For requests under 1024 tokens, cached\\_tokens will be zero. Caching discounts are based on the actual number of tokens processed, including those used for images, which also count toward your rate limits.\n\n## Example 1: Caching tools and multi-turn conversations\n\nIn this example, we define tools and interactions for a customer support assistant, capable of handling tasks such as checking delivery dates, canceling orders, and updating payment methods. The assistant processes two separate messages, first responding to an initial query, followed by a delayed response to a follow-up query.\n\nWhen caching tools, it is important that the tool definitions and their order remain identical for them to be included in the prompt prefix. To cache message histories in a multi-turn conversation, append new elements to the end of the messages array. In the response object and the output below, for the second completion `run2`, you can see that the `cached_tokens` value is greater than zero, indicating successful caching.\n\n```\nfrom openai import OpenAI\nimport os\nimport json\nimport time\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(organization='org-l89177bnhkme4a44292n5r3j', api_key=api_key)\n\n\n```\n\n```\nimport time\nimport json\n\n# Define tools\ntools = [\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"get_delivery_date\",\\\n            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"order_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The customer's order ID.\",\\\n                    },\\\n                },\\\n                \"required\": [\"order_id\"],\\\n                \"additionalProperties\": False,\\\n            },\\\n        }\\\n    },\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"cancel_order\",\\\n            \"description\": \"Cancel an order that has not yet been shipped. Use this when a customer requests order cancellation.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"order_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The customer's order ID.\"\\\n                    },\\\n                    \"reason\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The reason for cancelling the order.\"\\\n                    }\\\n                },\\\n                \"required\": [\"order_id\", \"reason\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    },\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"return_item\",\\\n            \"description\": \"Process a return for an order. This should be called when a customer wants to return an item and the order has already been delivered.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"order_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The customer's order ID.\"\\\n                    },\\\n                    \"item_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The specific item ID the customer wants to return.\"\\\n                    },\\\n                    \"reason\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The reason for returning the item.\"\\\n                    }\\\n                },\\\n                \"required\": [\"order_id\", \"item_id\", \"reason\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    },\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"update_shipping_address\",\\\n            \"description\": \"Update the shipping address for an order that hasn't been shipped yet. Use this if the customer wants to change their delivery address.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"order_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The customer's order ID.\"\\\n                    },\\\n                    \"new_address\": {\\\n                        \"type\": \"object\",\\\n                        \"properties\": {\\\n                            \"street\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new street address.\"\\\n                            },\\\n                            \"city\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new city.\"\\\n                            },\\\n                            \"state\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new state.\"\\\n                            },\\\n                            \"zip\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new zip code.\"\\\n                            },\\\n                            \"country\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new country.\"\\\n                            }\\\n                        },\\\n                        \"required\": [\"street\", \"city\", \"state\", \"zip\", \"country\"],\\\n                        \"additionalProperties\": False\\\n                    }\\\n                },\\\n                \"required\": [\"order_id\", \"new_address\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    },\\\n    # New tool: Update payment method\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"update_payment_method\",\\\n            \"description\": \"Update the payment method for an order that hasn't been completed yet. Use this if the customer wants to change their payment details.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"order_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The customer's order ID.\"\\\n                    },\\\n                    \"payment_method\": {\\\n                        \"type\": \"object\",\\\n                        \"properties\": {\\\n                            \"card_number\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new credit card number.\"\\\n                            },\\\n                            \"expiry_date\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new credit card expiry date in MM/YY format.\"\\\n                            },\\\n                            \"cvv\": {\\\n                                \"type\": \"string\",\\\n                                \"description\": \"The new credit card CVV code.\"\\\n                            }\\\n                        },\\\n                        \"required\": [\"card_number\", \"expiry_date\", \"cvv\"],\\\n                        \"additionalProperties\": False\\\n                    }\\\n                },\\\n                \"required\": [\"order_id\", \"payment_method\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    }\\\n]\n\n# Enhanced system message with guardrails\nmessages = [\\\n    {\\\n        \"role\": \"system\",\\\n        \"content\": (\\\n            \"You are a professional, empathetic, and efficient customer support assistant. Your mission is to provide fast, clear, \"\\\n            \"and comprehensive assistance to customers while maintaining a warm and approachable tone. \"\\\n            \"Always express empathy, especially when the user seems frustrated or concerned, and ensure that your language is polite and professional. \"\\\n            \"Use simple and clear communication to avoid any misunderstanding, and confirm actions with the user before proceeding. \"\\\n            \"In more complex or time-sensitive cases, assure the user that you're taking swift action and provide regular updates. \"\\\n            \"Adapt to the user’s tone: remain calm, friendly, and understanding, even in stressful or difficult situations.\"\\\n            \"\\n\\n\"\\\n            \"Additionally, there are several important guardrails that you must adhere to while assisting users:\"\\\n            \"\\n\\n\"\\\n            \"1. **Confidentiality and Data Privacy**: Do not share any sensitive information about the company or other users. When handling personal details such as order IDs, addresses, or payment methods, ensure that the information is treated with the highest confidentiality. If a user requests access to their data, only provide the necessary information relevant to their request, ensuring no other user's information is accidentally revealed.\"\\\n            \"\\n\\n\"\\\n            \"2. **Secure Payment Handling**: When updating payment details or processing refunds, always ensure that payment data such as credit card numbers, CVVs, and expiration dates are transmitted and stored securely. Never display or log full credit card numbers. Confirm with the user before processing any payment changes or refunds.\"\\\n            \"\\n\\n\"\\\n            \"3. **Respect Boundaries**: If a user expresses frustration or dissatisfaction, remain calm and empathetic but avoid overstepping professional boundaries. Do not make personal judgments, and refrain from using language that might escalate the situation. Stick to factual information and clear solutions to resolve the user's concerns.\"\\\n            \"\\n\\n\"\\\n            \"4. **Legal Compliance**: Ensure that all actions you take comply with legal and regulatory standards. For example, if the user requests a refund, cancellation, or return, follow the company’s refund policies strictly. If the order cannot be canceled due to being shipped or another restriction, explain the policy clearly but sympathetically.\"\\\n            \"\\n\\n\"\\\n            \"5. **Consistency**: Always provide consistent information that aligns with company policies. If unsure about a company policy, communicate clearly with the user, letting them know that you are verifying the information, and avoid providing false promises. If escalating an issue to another team, inform the user and provide a realistic timeline for when they can expect a resolution.\"\\\n            \"\\n\\n\"\\\n            \"6. **User Empowerment**: Whenever possible, empower the user to make informed decisions. Provide them with relevant options and explain each clearly, ensuring that they understand the consequences of each choice (e.g., canceling an order may result in loss of loyalty points, etc.). Ensure that your assistance supports their autonomy.\"\\\n            \"\\n\\n\"\\\n            \"7. **No Speculative Information**: Do not speculate about outcomes or provide information that you are not certain of. Always stick to verified facts when discussing order statuses, policies, or potential resolutions. If something is unclear, tell the user you will investigate further before making any commitments.\"\\\n            \"\\n\\n\"\\\n            \"8. **Respectful and Inclusive Language**: Ensure that your language remains inclusive and respectful, regardless of the user’s tone. Avoid making assumptions based on limited information and be mindful of diverse user needs and backgrounds.\"\\\n        )\\\n    },\\\n    {\\\n        \"role\": \"user\",\\\n        \"content\": (\\\n            \"Hi, I placed an order three days ago and haven’t received any updates on when it’s going to be delivered. \"\\\n            \"Could you help me check the delivery date? My order number is #9876543210. I’m a little worried because I need this item urgently.\"\\\n        )\\\n    }\\\n]\n\n# Enhanced user_query2\nuser_query2 = {\n    \"role\": \"user\",\n    \"content\": (\n        \"Since my order hasn't actually shipped yet, I would like to cancel it. \"\n        \"The order number is #9876543210, and I need to cancel because I’ve decided to purchase it locally to get it faster. \"\n        \"Can you help me with that? Thank you!\"\n    )\n}\n\n# Function to run completion with the provided message history and tools\ndef completion_run(messages, tools):\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        tools=tools,\n        messages=messages,\n        tool_choice=\"required\"\n    )\n    usage_data = json.dumps(completion.to_dict(), indent=4)\n    return usage_data\n\n# Main function to handle the two runs\ndef main(messages, tools, user_query2):\n    # Run 1: Initial query\n    print(\"Run 1:\")\n    run1 = completion_run(messages, tools)\n    print(run1)\n\n    # Delay for 7 seconds\n    time.sleep(7)\n\n    # Append user_query2 to the message history\n    messages.append(user_query2)\n\n    # Run 2: With appended query\n    print(\"\\nRun 2:\")\n    run2 = completion_run(messages, tools)\n    print(run2)\n\n# Run the main function\nmain(messages, tools, user_query2)\n\n\n```\n\n```\nRun 1:\n{\n    \"id\": \"chatcmpl-ADeOueQSi2DIUMdLXnZIv9caVfnro\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": null,\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\",\\\n                \"tool_calls\": [\\\n                    {\\\n                        \"id\": \"call_5TnLcdD9tyVMVbzNGdejlJJa\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\\\"order_id\\\":\\\"9876543210\\\"}\",\\\n                            \"name\": \"get_delivery_date\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    }\\\n                ]\\\n            }\\\n        }\\\n    ],\n    \"created\": 1727816928,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_f85bea6784\",\n    \"usage\": {\n        \"completion_tokens\": 17,\n        \"prompt_tokens\": 1079,\n        \"total_tokens\": 1096,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 0\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        }\n    }\n}\n\nRun 2:\n{\n    \"id\": \"chatcmpl-ADeP2i0frELC4W5RVNNkKz6TQ7hig\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": null,\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\",\\\n                \"tool_calls\": [\\\n                    {\\\n                        \"id\": \"call_viwwDZPuQh8hJFPf2Co1dYJK\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\\\"order_id\\\": \\\"9876543210\\\"}\",\\\n                            \"name\": \"get_delivery_date\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    },\\\n                    {\\\n                        \"id\": \"call_t1FFdAhrfvRc5IgqA6WkPKYj\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\\\"order_id\\\": \\\"9876543210\\\", \\\"reason\\\": \\\"Decided to purchase locally to get it faster.\\\"}\",\\\n                            \"name\": \"cancel_order\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    }\\\n                ]\\\n            }\\\n        }\\\n    ],\n    \"created\": 1727816936,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_f85bea6784\",\n    \"usage\": {\n        \"completion_tokens\": 64,\n        \"prompt_tokens\": 1136,\n        \"total_tokens\": 1200,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 1024\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        }\n    }\n}\n\n\n```\n\n## Example 2: Images\n\nIn our second example we include multiple image URLs of grocery items in the messages array, along with a user query, run three times with delays. Images—whether linked or encoded in base64 within user messages—qualify for caching. Make sure the detail parameter remains consistent, as it affects how images are tokenized. Note that GPT-4o-mini adds extra tokens to cover image processing costs, even though it uses a low-cost token model for text. Caching discounts are based on the actual number of tokens processed, including those used for images, which also count toward your rate limits.\n\nThe output for this example shows that a cache was hit for the second run, however it was not hit for the third run because of a different first url (eggs\\_url instead of veggie\\_url), even though the user query is the same.\n\n```\nsauce_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/12-04-20-saucen-by-RalfR-15.jpg/800px-12-04-20-saucen-by-RalfR-15.jpg\"\nveggie_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Veggies.jpg/800px-Veggies.jpg\"\neggs_url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Egg_shelf.jpg/450px-Egg_shelf.jpg\"\nmilk_url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Lactaid_brand.jpg/800px-Lactaid_brand.jpg\"\n\ndef multiimage_completion(url1, url2, user_query):\n    completion = client.chat.completions.create(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\\\n        {\\\n        \"role\": \"user\",\\\n        \"content\": [\\\n            {\\\n            \"type\": \"image_url\",\\\n            \"image_url\": {\\\n                \"url\": url1,\\\n                \"detail\": \"high\"\\\n            },\\\n            },\\\n            {\\\n            \"type\": \"image_url\",\\\n            \"image_url\": {\\\n                \"url\": url2,\\\n                \"detail\": \"high\"\\\n            },\\\n            },\\\n            {\"type\": \"text\", \"text\": user_query}\\\n        ],\\\n        }\\\n    ],\n    max_tokens=300,\n    )\n    print(json.dumps(completion.to_dict(), indent=4))\n\n\ndef main(sauce_url, veggie_url):\n    multiimage_completion(sauce_url, veggie_url, \"Please list the types of sauces are shown in these images\")\n    #delay for 20 seconds\n    time.sleep(20)\n    multiimage_completion(sauce_url, veggie_url, \"Please list the types of vegetables are shown in these images\")\n    time.sleep(20)\n    multiimage_completion(milk_url, sauce_url, \"Please list the types of sauces are shown in these images\")\n\nif __name__ == \"__main__\":\n    main(sauce_url, veggie_url)\n\n\n```\n\n```\n{\n    \"id\": \"chatcmpl-ADeV3IrUqhpjMXEgv29BFHtTQ0Pzt\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": \"The images show the following types of sauces:\\n\\n1. **Soy Sauce** - Kikkoman brand.\\n2. **Worcester Sauce** - Appel brand, listed as \\\"Dresdner Art.\\\"\\n3. **Tabasco Sauce** - Original pepper sauce.\\n\\nThe second image shows various vegetables, not sauces.\",\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\"\\\n            }\\\n        }\\\n    ],\n    \"created\": 1727817309,\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_2f406b9113\",\n    \"usage\": {\n        \"completion_tokens\": 65,\n        \"prompt_tokens\": 1548,\n        \"total_tokens\": 1613,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 0\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        }\n    }\n}\n{\n    \"id\": \"chatcmpl-ADeVRSI6zFINkx99k7V6ux1v5iF5f\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": \"The images show different types of items. In the first image, you'll see bottles of sauces like soy sauce, Worcester sauce, and Tabasco. The second image features various vegetables, including:\\n\\n1. Napa cabbage\\n2. Kale\\n3. Carrots\\n4. Bok choy\\n5. Swiss chard\\n6. Leeks\\n7. Parsley\\n\\nThese vegetables are arranged on shelves in a grocery store setting.\",\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\"\\\n            }\\\n        }\\\n    ],\n    \"created\": 1727817333,\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_2f406b9113\",\n    \"usage\": {\n        \"completion_tokens\": 86,\n        \"prompt_tokens\": 1548,\n        \"total_tokens\": 1634,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 1280\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        }\n    }\n}\n{\n    \"id\": \"chatcmpl-ADeVphj3VALQVrdnt2efysvSmdnBx\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": \"The second image shows three types of sauces:\\n\\n1. Soy Sauce (Kikkoman)\\n2. Worcestershire Sauce\\n3. Tabasco Sauce\",\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\"\\\n            }\\\n        }\\\n    ],\n    \"created\": 1727817357,\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_2f406b9113\",\n    \"usage\": {\n        \"completion_tokens\": 29,\n        \"prompt_tokens\": 1548,\n        \"total_tokens\": 1577,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 0\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        }\n    }\n}\n\n\n```\n\n## Overall tips\n\nTo get the most out of prompt caching, consider following these best practices:\n\n- Place static or frequently reused content at the beginning of prompts: This helps ensure better cache efficiency by keeping dynamic data towards the end of the prompt.\n\n- Maintain consistent usage patterns: Prompts that aren’t used regularly are automatically removed from the cache. To prevent cache evictions, maintain consistent usage of prompts.\n\n- Monitor key metrics: Regularly track cache hit rates, latency, and the proportion of cached tokens. Use these insights to fine-tune your caching strategy and maximize performance.\n\n\nBy implementing these practices, you can take full advantage of prompt caching, ensuring that your applications are both responsive and cost-efficient. A well-managed caching strategy will significantly reduce processing times, lower costs, and help maintain smooth user experiences.","metadata":{"generator":"Astro v5.16.15","og:description":"OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts","og:image":"https://developers.openai.com/open-graph.png","ogTitle":"Prompt Caching 101","description":"OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts","og:url":"https://developers.openai.com/cookbook/examples/prompt_caching101/","twitter:url":"https://developers.openai.com/cookbook/examples/prompt_caching101/","viewport":"width=device-width,initial-scale=1","twitter:title":"Prompt Caching 101","twitter:description":"OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts","twitter:image":"https://developers.openai.com/open-graph.png","ogUrl":"https://developers.openai.com/cookbook/examples/prompt_caching101/","language":"en","ogImage":"https://developers.openai.com/open-graph.png","ogDescription":"OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prompts","title":"Prompt Caching 101","og:title":"Prompt Caching 101","twitter:card":"summary_large_image","og:type":"website","favicon":"https://developers.openai.com/favicon.png","scrapeId":"019c62eb-509c-7409-9bdc-11394633d382","sourceURL":"https://developers.openai.com/cookbook/examples/prompt_caching101/","url":"https://developers.openai.com/cookbook/examples/prompt_caching101/","statusCode":200,"contentType":"text/html; charset=utf-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"afee06fa-8f55-4f5a-ad1b-3773a22bc1da","creditsUsed":1},"links":["https://developers.openai.com/cookbook","https://developers.openai.com/cookbook/topic/agents","https://developers.openai.com/cookbook/topic/evals","https://developers.openai.com/cookbook/topic/multimodal","https://developers.openai.com/cookbook/topic/text","https://developers.openai.com/cookbook/topic/guardrails","https://developers.openai.com/cookbook/topic/optimization","https://developers.openai.com/cookbook/topic/chatgpt","https://developers.openai.com/cookbook/topic/codex","https://developers.openai.com/cookbook/topic/gpt-oss","https://github.com/openai/openai-cookbook","https://platform.openai.com/login","https://www.linkedin.com/in/charu-j-8a866471","https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb","https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Prompt_Caching101.ipynb"]},{"url":"https://developers.openai.com/api/docs/guides/realtime-costs","title":"Managing costs | OpenAI API","description":"Caching. Realtime API supports prompt caching, which is applied automatically and can dramatically reduce the costs of input tokens during multi-turn sessions.","position":3,"markdown":"## Search the API docs\n\nClose\n\nClear\n\nPrimary navigation\n\nClear\n\n### Get started\n\n- [Overview](https://developers.openai.com/api/docs)\n- [Quickstart](https://developers.openai.com/api/docs/quickstart)\n- [Models](https://developers.openai.com/api/docs/models)\n- [Pricing](https://developers.openai.com/api/docs/pricing)\n- [Libraries](https://developers.openai.com/api/docs/libraries)\n- [Docs MCP](https://developers.openai.com/resources/docs-mcp)\n- [Latest: GPT-5.2](https://developers.openai.com/api/docs/guides/latest-model)\n\n### Core concepts\n\n- [Text generation](https://developers.openai.com/api/docs/guides/text)\n- [Code generation](https://developers.openai.com/api/docs/guides/code-generation)\n- [Images and vision](https://developers.openai.com/api/docs/guides/images-vision)\n- [Audio and speech](https://developers.openai.com/api/docs/guides/audio)\n- [Structured output](https://developers.openai.com/api/docs/guides/structured-outputs)\n- [Function calling](https://developers.openai.com/api/docs/guides/function-calling)\n- [Responses API](https://developers.openai.com/api/docs/guides/migrate-to-responses)\n\n### Agents\n\n- [Overview](https://developers.openai.com/api/docs/guides/agents)\n- Build agents\n\n  - [Agent Builder](https://developers.openai.com/api/docs/guides/agent-builder)\n  - [Node reference](https://developers.openai.com/api/docs/guides/node-reference)\n  - [Safety in building agents](https://developers.openai.com/api/docs/guides/agent-builder-safety)\n  - [Agents SDK](https://developers.openai.com/api/docs/guides/agents-sdk)\n\n- Deploy in your product\n\n  - [ChatKit](https://developers.openai.com/api/docs/guides/chatkit)\n  - [Custom theming](https://developers.openai.com/api/docs/guides/chatkit-themes)\n  - [Widgets](https://developers.openai.com/api/docs/guides/chatkit-widgets)\n  - [Actions](https://developers.openai.com/api/docs/guides/chatkit-actions)\n  - [Advanced integration](https://developers.openai.com/api/docs/guides/custom-chatkit)\n\n- Optimize\n\n  - [Agent evals](https://developers.openai.com/api/docs/guides/agent-evals)\n  - [Trace grading](https://developers.openai.com/api/docs/guides/trace-grading)\n\n- [Voice agents](https://developers.openai.com/api/docs/guides/voice-agents)\n\n### Tools\n\n- [Using tools](https://developers.openai.com/api/docs/guides/tools)\n- [Connectors and MCP](https://developers.openai.com/api/docs/guides/tools-connectors-mcp)\n- [Skills](https://developers.openai.com/api/docs/guides/tools-skills)\n- [Shell](https://developers.openai.com/api/docs/guides/tools-shell)\n- [Web search](https://developers.openai.com/api/docs/guides/tools-web-search)\n- [Code interpreter](https://developers.openai.com/api/docs/guides/tools-code-interpreter)\n- File search and retrieval\n\n  - [File search](https://developers.openai.com/api/docs/guides/tools-file-search)\n  - [Retrieval](https://developers.openai.com/api/docs/guides/retrieval)\n\n- More tools\n\n  - [Image generation](https://developers.openai.com/api/docs/guides/tools-image-generation)\n  - [Computer use](https://developers.openai.com/api/docs/guides/tools-computer-use)\n  - [Local shell tool](https://developers.openai.com/api/docs/guides/tools-local-shell)\n  - [Apply patch](https://developers.openai.com/api/docs/guides/tools-apply-patch)\n\n### Run and scale\n\n- [Conversation state](https://developers.openai.com/api/docs/guides/conversation-state)\n- [Compaction](https://developers.openai.com/api/docs/guides/compaction)\n- [Background mode](https://developers.openai.com/api/docs/guides/background)\n- [Streaming](https://developers.openai.com/api/docs/guides/streaming-responses)\n- [Webhooks](https://developers.openai.com/api/docs/guides/webhooks)\n- [File inputs](https://developers.openai.com/api/docs/guides/pdf-files)\n- Prompting\n\n  - [Overview](https://developers.openai.com/api/docs/guides/prompting)\n  - [Prompt caching](https://developers.openai.com/api/docs/guides/prompt-caching)\n  - [Prompt engineering](https://developers.openai.com/api/docs/guides/prompt-engineering)\n\n- Reasoning\n\n  - [Reasoning models](https://developers.openai.com/api/docs/guides/reasoning)\n  - [Reasoning best practices](https://developers.openai.com/api/docs/guides/reasoning-best-practices)\n\n### Evaluation\n\n- [Getting started](https://developers.openai.com/api/docs/guides/evaluation-getting-started)\n- [Working with evals](https://developers.openai.com/api/docs/guides/evals)\n- [Prompt optimizer](https://developers.openai.com/api/docs/guides/prompt-optimizer)\n- [External models](https://developers.openai.com/api/docs/guides/external-models)\n- [Best practices](https://developers.openai.com/api/docs/guides/evaluation-best-practices)\n\n### Realtime API\n\n- [Overview](https://developers.openai.com/api/docs/guides/realtime)\n- Connect\n\n  - [WebRTC](https://developers.openai.com/api/docs/guides/realtime-webrtc)\n  - [WebSocket](https://developers.openai.com/api/docs/guides/realtime-websocket)\n  - [SIP](https://developers.openai.com/api/docs/guides/realtime-sip)\n\n- Usage\n\n  - [Using realtime models](https://developers.openai.com/api/docs/guides/realtime-models-prompting)\n  - [Managing conversations](https://developers.openai.com/api/docs/guides/realtime-conversations)\n  - [Webhooks and server-side controls](https://developers.openai.com/api/docs/guides/realtime-server-controls)\n  - [Managing costs](https://developers.openai.com/api/docs/guides/realtime-costs)\n  - [Realtime transcription](https://developers.openai.com/api/docs/guides/realtime-transcription)\n  - [Voice agents](https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/)\n\n### Model optimization\n\n- [Optimization cycle](https://developers.openai.com/api/docs/guides/model-optimization)\n- Fine-tuning\n\n  - [Supervised fine-tuning](https://developers.openai.com/api/docs/guides/supervised-fine-tuning)\n  - [Vision fine-tuning](https://developers.openai.com/api/docs/guides/vision-fine-tuning)\n  - [Direct preference optimization](https://developers.openai.com/api/docs/guides/direct-preference-optimization)\n  - [Reinforcement fine-tuning](https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning)\n  - [RFT use cases](https://developers.openai.com/api/docs/guides/rft-use-cases)\n  - [Best practices](https://developers.openai.com/api/docs/guides/fine-tuning-best-practices)\n\n- [Graders](https://developers.openai.com/api/docs/guides/graders)\n\n### Specialized models\n\n- [Image generation](https://developers.openai.com/api/docs/guides/image-generation)\n- [Video generation](https://developers.openai.com/api/docs/guides/video-generation)\n- [Text to speech](https://developers.openai.com/api/docs/guides/text-to-speech)\n- [Speech to text](https://developers.openai.com/api/docs/guides/speech-to-text)\n- [Deep research](https://developers.openai.com/api/docs/guides/deep-research)\n- [Embeddings](https://developers.openai.com/api/docs/guides/embeddings)\n- [Moderation](https://developers.openai.com/api/docs/guides/moderation)\n\n### Coding agents\n\n- [Codex cloud](https://developers.openai.com/codex/cloud)\n- [Agent internet access](https://developers.openai.com/codex/cloud/agent-internet)\n- [Codex CLI](https://developers.openai.com/codex/cli)\n- [Codex IDE](https://developers.openai.com/codex/ide)\n- [Codex changelog](https://developers.openai.com/codex/changelog)\n\n### Going live\n\n- [Production best practices](https://developers.openai.com/api/docs/guides/production-best-practices)\n- Latency optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/latency-optimization)\n  - [Predicted Outputs](https://developers.openai.com/api/docs/guides/predicted-outputs)\n  - [Priority processing](https://developers.openai.com/api/docs/guides/priority-processing)\n\n- Cost optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/cost-optimization)\n  - [Batch](https://developers.openai.com/api/docs/guides/batch)\n  - [Flex processing](https://developers.openai.com/api/docs/guides/flex-processing)\n\n- [Accuracy optimization](https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy)\n- Safety\n\n  - [Safety best practices](https://developers.openai.com/api/docs/guides/safety-best-practices)\n  - [Safety checks](https://developers.openai.com/api/docs/guides/safety-checks)\n  - [Under 18 API Guidance](https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance)\n\n### Legacy APIs\n\n- Assistants API\n\n  - [Migration guide](https://developers.openai.com/api/docs/assistants/migration)\n  - [Deep dive](https://developers.openai.com/api/docs/assistants/deep-dive)\n  - [Tools](https://developers.openai.com/api/docs/assistants/tools)\n\n### Resources\n\n- [Terms and policies](https://openai.com/policies)\n- [Changelog](https://developers.openai.com/api/docs/changelog)\n- [Your data](https://developers.openai.com/api/docs/guides/your-data)\n- [Permissions](https://developers.openai.com/api/docs/guides/rbac)\n- [Rate limits](https://developers.openai.com/api/docs/guides/rate-limits)\n- [Deprecations](https://developers.openai.com/api/docs/deprecations)\n- [MCP for deep research](https://developers.openai.com/api/docs/mcp)\n- [Developer mode](https://developers.openai.com/api/docs/guides/developer-mode)\n- ChatGPT Actions\n\n  - [Introduction](https://developers.openai.com/api/docs/actions/introduction)\n  - [Getting started](https://developers.openai.com/api/docs/actions/getting-started)\n  - [Actions library](https://developers.openai.com/api/docs/actions/actions-library)\n  - [Authentication](https://developers.openai.com/api/docs/actions/authentication)\n  - [Production](https://developers.openai.com/api/docs/actions/production)\n  - [Data retrieval](https://developers.openai.com/api/docs/actions/data-retrieval)\n  - [Sending files](https://developers.openai.com/api/docs/actions/sending-files)\n\n[API Dashboard](https://platform.openai.com/login)\n\nSearch\n⌘\n\nK\n\nCopy PageMore page actions\n\nCopy PageMore page actions\n\nThis document describes how Realtime API billing works and offer strategies for optimizing costs. Costs are accrued as input and output tokens of different modalities: text, audio, and image. Token costs vary per model, with prices listed on the model pages (e.g. for [`gpt-realtime`](https://developers.openai.com/api/docs/models/gpt-realtime) and [`gpt-realtime-mini`](https://developers.openai.com/api/docs/models/gpt-realtime-mini)).\n\nConversational Realtime API sessions are a series of _turns_, where the user adds input that triggers a _Response_ to produce the model output. The server maintains a _Conversation_, which is a list of _Items_ that form the input for the next turn. When a Response is returned the output is automatically added to the Conversation.\n\n## Per-Response costs\n\nRealtime API costs are accrued when a Response is created, and is charged based on the numbers of input and output tokens (except for input transcription costs, see below). There is no cost currently for network bandwidth or connections. A Response can be created manually or automatically if voice activity detection (VAD) is turned on. VAD will effectively filter out empty input audio, so empty audio does not count as input tokens unless the client manually adds it as conversation input.\n\nThe entire conversation is sent to the model for each Response. The output from a turn will be added as Items to the server Conversation and become the input to subsequent turns, thus turns later in the session will be more expensive.\n\nText token costs can be estimated using our [tokenization tools](https://platform.openai.com/tokenizer). Audio tokens in user messages are 1 token per 100 ms of audio, while audio tokens in assistant messages are 1 token per 50ms of audio. Note that token counts include special tokens aside from the content of a message which will surface as small variations in these counts, for example a user message with 10 text tokens of content may count as 12 tokens.\n\n### Example\n\nHere’s a simple example to illustrate token costs over a multi-turn Realtime API session.\n\nFor the first turn in the conversation we’ve added 100 tokens of instructions, a user message of 20 audio tokens (for example added by VAD based on the user speaking), for a total of 120 input tokens. Creating a Response generates an assistant output message (20 audio, 10 text tokens).\n\nThen we create a second turn with another user audio message. What will the tokens for turn 2 look like? The Conversation at this point includes the initial instructions, first user message, the output assistant message from the first turn, plus the second user message (25 audio tokens). This turn will have 110 text and 64 audio tokens for input, plus the output tokens of another assistant output message.\n\n![tokens on successive conversation turns](https://cdn.openai.com/API/docs/images/realtime-costs-turns.png)\n\nThe messages from the first turn are likely to be cached for turn 2, which reduces the input cost. See below for more information on caching.\n\nThe tokens used for a Response can be read from the `response.done` event, which looks like the following.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n{\n  \"type\": \"response.done\",\n  \"response\": {\n    ...\n    \"usage\": {\n      \"total_tokens\": 253,\n      \"input_tokens\": 132,\n      \"output_tokens\": 121,\n      \"input_token_details\": {\n        \"text_tokens\": 119,\n        \"audio_tokens\": 13,\n        \"image_tokens\": 0,\n        \"cached_tokens\": 64,\n        \"cached_tokens_details\": {\n          \"text_tokens\": 64,\n          \"audio_tokens\": 0,\n          \"image_tokens\": 0\n        }\n      },\n      \"output_token_details\": {\n        \"text_tokens\": 30,\n        \"audio_tokens\": 91\n      }\n    }\n  }\n}\n```\n\n## Input transcription costs\n\nAside from conversational Responses, the Realtime API bills for input transcriptions, if enabled. Input transcription uses a different model than the speech2speech model, such as [`whisper-1`](https://developers.openai.com/api/docs/models/whisper-1) or [`gpt-4o-transcribe`](https://developers.openai.com/api/docs/models/gpt-4o-transcribe), and thus are billed from a different rate card. Transcription is performed when audio is written to the input audio buffer and then committed, either manually or by VAD.\n\nInput transcription token counts can be read from the `conversation.item.input_audio_transcription.completed` event, as in the following example.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n{\n  \"type\": \"conversation.item.input_audio_transcription.completed\",\n  ...\n  \"transcript\": \"Hi, can you hear me?\",\n  \"usage\": {\n    \"type\": \"tokens\",\n    \"total_tokens\": 26,\n    \"input_tokens\": 17,\n    \"input_token_details\": {\n      \"text_tokens\": 0,\n      \"audio_tokens\": 17\n    },\n    \"output_tokens\": 9\n  }\n}\n```\n\n## Caching\n\nRealtime API supports [prompt caching](https://developers.openai.com/api/docs/guides/prompt-caching), which is applied automatically and can dramatically reduce the costs of input tokens during multi-turn sessions. Caching applies when the input tokens of a Response match tokens from a previous Response, though this is best-effort and not guaranteed.\n\nThe best strategy for maximizing cache rate is keep a session’s history static. Removing or changing content in the conversation will “bust” the cache up to the point of the change — the input no longer matches as much as before. Note that instructions and tool definitions are at the beginning of a conversation, thus changing these mid-session will reduce the cache rate for subsequent turns.\n\n## Truncation\n\nWhen the number of tokens in a conversation exceeds the model’s input token limit the conversation be truncated, meaning messages (starting from the oldest) will be dropped from the Response input. A 32k context model with 4,096 max output tokens can only include 28,224 tokens in the context before truncation occurs.\n\nClients can set a smaller token window than the model’s maximum, which is a good way to control token usage and cost. This is controlled with the `token_limits.post_instructions` configuration (if you configure truncation with a `retention_ratio` type as shown below). As the name indicates, this controls the maximum number of input tokens for a Response, except for the instruction tokens. Setting `post_instructions` to 1,000 means that items over the 1,000 input token limit will not be sent to the model for a Response.\n\nTruncation busts the cache near the beginning of the conversation, and if truncation occurs on every turn then cache rate will be very low. To mitigate this issue clients can configure truncation to drop more messages than necessary, which will extend the headroom before another truncation is needed. This can be controlled with the `session.truncation.retention_ratio` setting. The server defaults to a value of `1.0` , meaning truncation will remove only the items necessary. A value of `0.8` means a truncation would retain 80% of the maximum, dropping an additional 20%.\n\nIf you’re attempting to reduce Realtime API cost per session (for a given model), we recommend reducing limiting the number of tokens and setting a `retention_ratio` less than 1, as in the following example. Remember that there may be a tradeoff here in terms of lower cost but lower model memory for a given turn.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n{\n  \"event\": \"session.update\",\n  \"session\": {\n    \"truncation\": {\n      \"type\": \"retention_ratio\",\n      \"retention_ratio\": 0.8,\n      \"token_limits\": {\n        \"post_instructions\": 8000\n      }\n    }\n  }\n}\n```\n\nTruncation can also be completely disabled, as shown below. When disabled an error will be returned if the Conversation is too long to create a Response. This may be useful if you intend to manage the Conversation size manually.\n\n```\n1\n2\n3\n4\n5\n6\n{\n  \"event\": \"session.update\",\n  \"session\": {\n    \"truncation\": \"disabled\"\n  }\n}\n```\n\n## Other optimization strategies\n\n### Using a mini model\n\nThe Realtime speech2speech models come in a “normal” size and a mini size, which is significantly cheaper. The tradeoff here tends to be intelligence related to instruction following and function calling, which will not be as effective in the mini model. We recommend first testing applications with the larger model, refining your application and prompt, then attempting to optimize using the mini model.\n\n### Editing the Conversation\n\nWhile truncation will occur automatically on the server, another cost management strategy is to manually edit the Conversation. A principle of the API is to allow full client control of the server-side Conversation, allowing the client to add and remove items at will.\n\n```\n1\n2\n3\n4\n{\n  \"type\": \"conversation.item.delete\",\n  \"item_id\": \"item_CCXLecNJVIVR2HUy3ABLj\"\n}\n```\n\nClearing out old messages is a good way to reduce input token sizes and cost. This might remove important content, but a common strategy is to replace these old messages with a summary. Items can be deleted from the Conversation with a `conversation.item.delete` message as above, and can be added with a `conversation.item.create` message.\n\n## Estimating costs\n\nGiven the complexity in Realtime API token usage it can be difficult to estimate your costs ahead of time. A good approach is to use the Realtime Playground with your intended prompts and functions, and measure the token usage over a sample session. The token usage for a session can be found under the Logs tab in the Realtime Playground next to the session id.\n\n![showing tokens in the playground](https://cdn.openai.com/API/docs/images/realtime-playground-tokens.png)","metadata":{"ogImage":"https://developers.openai.com/open-graph.png","twitter:url":"https://developers.openai.com/api/docs/guides/realtime-costs/","title":"Managing costs | OpenAI API","language":"en","viewport":"width=device-width,initial-scale=1","og:type":"website","twitter:description":"Learn how to monitor and optimize your costs when using the Realtime API.","ogUrl":"https://developers.openai.com/api/docs/guides/realtime-costs/","twitter:card":"summary_large_image","ogTitle":"Managing costs | OpenAI API","twitter:title":"Managing costs | OpenAI API","og:description":"Learn how to monitor and optimize your costs when using the Realtime API.","twitter:image":"https://developers.openai.com/open-graph.png","description":"Learn how to monitor and optimize your costs when using the Realtime API.","og:image":"https://developers.openai.com/open-graph.png","generator":"Astro v5.16.15","og:url":"https://developers.openai.com/api/docs/guides/realtime-costs/","ogDescription":"Learn how to monitor and optimize your costs when using the Realtime API.","og:title":"Managing costs | OpenAI API","favicon":"https://developers.openai.com/favicon.png","scrapeId":"019c62eb-509c-7409-9bdc-1700a89466cd","sourceURL":"https://developers.openai.com/api/docs/guides/realtime-costs","url":"https://developers.openai.com/api/docs/guides/realtime-costs","statusCode":200,"contentType":"text/html; charset=utf-8","proxyUsed":"basic","cacheState":"hit","cachedAt":"2026-02-14T00:42:50.935Z","creditsUsed":1},"links":["https://developers.openai.com/api/docs","https://developers.openai.com/api/docs/quickstart","https://developers.openai.com/api/docs/models","https://developers.openai.com/api/docs/pricing","https://developers.openai.com/api/docs/libraries","https://developers.openai.com/resources/docs-mcp","https://developers.openai.com/api/docs/guides/latest-model","https://developers.openai.com/api/docs/guides/text","https://developers.openai.com/api/docs/guides/code-generation","https://developers.openai.com/api/docs/guides/images-vision","https://developers.openai.com/api/docs/guides/audio","https://developers.openai.com/api/docs/guides/structured-outputs","https://developers.openai.com/api/docs/guides/function-calling","https://developers.openai.com/api/docs/guides/migrate-to-responses","https://developers.openai.com/api/docs/guides/agents","https://developers.openai.com/api/docs/guides/agent-builder","https://developers.openai.com/api/docs/guides/node-reference","https://developers.openai.com/api/docs/guides/agent-builder-safety","https://developers.openai.com/api/docs/guides/agents-sdk","https://developers.openai.com/api/docs/guides/chatkit","https://developers.openai.com/api/docs/guides/chatkit-themes","https://developers.openai.com/api/docs/guides/chatkit-widgets","https://developers.openai.com/api/docs/guides/chatkit-actions","https://developers.openai.com/api/docs/guides/custom-chatkit","https://developers.openai.com/api/docs/guides/agent-evals","https://developers.openai.com/api/docs/guides/trace-grading","https://developers.openai.com/api/docs/guides/voice-agents","https://developers.openai.com/api/docs/guides/tools","https://developers.openai.com/api/docs/guides/tools-connectors-mcp","https://developers.openai.com/api/docs/guides/tools-skills","https://developers.openai.com/api/docs/guides/tools-shell","https://developers.openai.com/api/docs/guides/tools-web-search","https://developers.openai.com/api/docs/guides/tools-code-interpreter","https://developers.openai.com/api/docs/guides/tools-file-search","https://developers.openai.com/api/docs/guides/retrieval","https://developers.openai.com/api/docs/guides/tools-image-generation","https://developers.openai.com/api/docs/guides/tools-computer-use","https://developers.openai.com/api/docs/guides/tools-local-shell","https://developers.openai.com/api/docs/guides/tools-apply-patch","https://developers.openai.com/api/docs/guides/conversation-state","https://developers.openai.com/api/docs/guides/compaction","https://developers.openai.com/api/docs/guides/background","https://developers.openai.com/api/docs/guides/streaming-responses","https://developers.openai.com/api/docs/guides/webhooks","https://developers.openai.com/api/docs/guides/pdf-files","https://developers.openai.com/api/docs/guides/prompting","https://developers.openai.com/api/docs/guides/prompt-caching","https://developers.openai.com/api/docs/guides/prompt-engineering","https://developers.openai.com/api/docs/guides/reasoning","https://developers.openai.com/api/docs/guides/reasoning-best-practices","https://developers.openai.com/api/docs/guides/evaluation-getting-started","https://developers.openai.com/api/docs/guides/evals","https://developers.openai.com/api/docs/guides/prompt-optimizer","https://developers.openai.com/api/docs/guides/external-models","https://developers.openai.com/api/docs/guides/evaluation-best-practices","https://developers.openai.com/api/docs/guides/realtime","https://developers.openai.com/api/docs/guides/realtime-webrtc","https://developers.openai.com/api/docs/guides/realtime-websocket","https://developers.openai.com/api/docs/guides/realtime-sip","https://developers.openai.com/api/docs/guides/realtime-models-prompting","https://developers.openai.com/api/docs/guides/realtime-conversations","https://developers.openai.com/api/docs/guides/realtime-server-controls","https://developers.openai.com/api/docs/guides/realtime-costs","https://developers.openai.com/api/docs/guides/realtime-transcription","https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/","https://developers.openai.com/api/docs/guides/model-optimization","https://developers.openai.com/api/docs/guides/supervised-fine-tuning","https://developers.openai.com/api/docs/guides/vision-fine-tuning","https://developers.openai.com/api/docs/guides/direct-preference-optimization","https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning","https://developers.openai.com/api/docs/guides/rft-use-cases","https://developers.openai.com/api/docs/guides/fine-tuning-best-practices","https://developers.openai.com/api/docs/guides/graders","https://developers.openai.com/api/docs/guides/image-generation","https://developers.openai.com/api/docs/guides/video-generation","https://developers.openai.com/api/docs/guides/text-to-speech","https://developers.openai.com/api/docs/guides/speech-to-text","https://developers.openai.com/api/docs/guides/deep-research","https://developers.openai.com/api/docs/guides/embeddings","https://developers.openai.com/api/docs/guides/moderation","https://developers.openai.com/codex/cloud","https://developers.openai.com/codex/cloud/agent-internet","https://developers.openai.com/codex/cli","https://developers.openai.com/codex/ide","https://developers.openai.com/codex/changelog","https://developers.openai.com/api/docs/guides/production-best-practices","https://developers.openai.com/api/docs/guides/latency-optimization","https://developers.openai.com/api/docs/guides/predicted-outputs","https://developers.openai.com/api/docs/guides/priority-processing","https://developers.openai.com/api/docs/guides/cost-optimization","https://developers.openai.com/api/docs/guides/batch","https://developers.openai.com/api/docs/guides/flex-processing","https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy","https://developers.openai.com/api/docs/guides/safety-best-practices","https://developers.openai.com/api/docs/guides/safety-checks","https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance","https://developers.openai.com/api/docs/assistants/migration","https://developers.openai.com/api/docs/assistants/deep-dive","https://developers.openai.com/api/docs/assistants/tools","https://openai.com/policies","https://developers.openai.com/api/docs/changelog","https://developers.openai.com/api/docs/guides/your-data","https://developers.openai.com/api/docs/guides/rbac","https://developers.openai.com/api/docs/guides/rate-limits","https://developers.openai.com/api/docs/deprecations","https://developers.openai.com/api/docs/mcp","https://developers.openai.com/api/docs/guides/developer-mode","https://developers.openai.com/api/docs/actions/introduction","https://developers.openai.com/api/docs/actions/getting-started","https://developers.openai.com/api/docs/actions/actions-library","https://developers.openai.com/api/docs/actions/authentication","https://developers.openai.com/api/docs/actions/production","https://developers.openai.com/api/docs/actions/data-retrieval","https://developers.openai.com/api/docs/actions/sending-files","https://platform.openai.com/login","https://developers.openai.com/api/docs/models/gpt-realtime","https://developers.openai.com/api/docs/models/gpt-realtime-mini","https://platform.openai.com/tokenizer","https://developers.openai.com/api/docs/models/whisper-1","https://developers.openai.com/api/docs/models/gpt-4o-transcribe"]},{"url":"https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs","title":"Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs - Finout","description":"Integrate caching at the application or middleware layer, and monitor cache hit rates to assess savings. This practice not only helps cap ...","position":4,"markdown":"[Our AI-Powered, Automated Cost Allocation VTags Are Live - Learn More](https://www.finout.io/megabill/ai-virtual-tag)\n\n[Blog posts](https://www.finout.io/blog)\n\n# Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs\n\nDec 24th, 2025\n\n![Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs](https://www.finout.io/hs-fs/hubfs/Azure%20OpenAI%20Pricing.png?width=522&height=300&name=Azure%20OpenAI%20Pricing.png)\n\nURL Copied\n\n## What Is Azure OpenAI Service?\n\nAzure OpenAI Service is a managed platform from Microsoft that provides access to natural language processing (NLP) models developed by OpenAI, including GPT-5, GPT-4, Codex, and embeddings. This service allows developers to integrate AI capabilities into their applications, such as language generation, text summarization, code generation, and semantic search, using familiar REST APIs while benefiting from Azure’s reliability, scalability, and security measures.\n\nThe service offers access controls, compliance features, and support for private networking. Unlike the publicly accessible OpenAI API, Azure OpenAI Service integrates with other Azure resources, such as Azure Cognitive Services and Azure Machine Learning, to streamline workflows for model deployment, monitoring, and scaling. Enterprises can leverage these integrations for more finely tuned solutions and easier handling of sensitive data.\n\nThis is part of a series of articles about Azure pricing\n\n**In this article:**\n\n- Understanding Azure OpenAI Pricing Models\n- Azure OpenAI Pricing: A Deep Dive\n- Cost Optimization Strategies for Azure OpenAI\n\n## Understanding Azure OpenAI Pricing Models\n\nAzure OpenAI Service offers multiple pricing models to suit different usage patterns and budget requirements. These models include standard (on-demand), provisioned throughput units (PTUs), and batch API pricing, each providing varying levels of cost control, scalability, and performance.\n\n- **Standard (on-demand) pricing** operates on a pay-as-you-go model. Charges are based on the number of tokens processed—both for inputs and outputs—making it suitable for variable workloads where usage may fluctuate.\n- **Provisioned throughput units (PTUs)** are ideal for predictable workloads. Organizations can reserve a fixed amount of throughput, ensuring consistent performance and more predictable monthly or annual costs. This model helps reduce overall spending through long-term commitments.\n- **Batch API** offers a cost-effective option for non-interactive workloads. It allows organizations to submit large volumes of requests for processing, with results returned within 24 hours. This delayed-response approach provides a 50% discount compared to standard global pricing, making it suitable for large-scale, non-time-sensitive tasks.\n\nDeployment options further affect pricing and performance. Organizations can choose **global deployments**, **data zone deployments** (limited to the US or EU), or **regional deployments** across up to 27 Azure regions. These options help meet data residency requirements and optimize for latency or cost, depending on operational needs.\n\n## Azure OpenAI Pricing: A Deep Dive\n\nAzure allows organizations cloud-based access to OpenAI’s AI models. Below is a breakdown of example pricing for each major OpenAI model family and service type.\n\n### Pricing By OpenAI Model\n\n#### GPT-5 Series\n\n- **GPT-5 Global**: $1.25 per million input tokens, $10 per million output tokens. Cached input tokens cost $0.13.\n- **GPT-5 Pro Global**: Higher-tier performance at $15 per million input tokens and $120 per million output tokens.\n- **GPT-5-mini**: A more affordable variant, with input at $0.25 and output at $2.\n- **GPT-5-nano**: Lowest-cost option at $0.05 input and $0.40 output per million tokens.\n\n#### GPT-4.1 Series\n\n- **GPT-4.1 Global**: Input at $2, output at $8 per million tokens.\n- **GPT-4.1-mini**: Costs $0.40 for input and $1.60 for output per million tokens.\n- **GPT-4.1-nano**: For ultra-low-cost applications at $0.10 input and $0.40 output.\n\n#### GPT-4o Models\n\n- **GPT-4o (Global)**: Input is $2.50, output is $10 per million tokens.\n- **Batch API pricing**: For example, GPT-4o Global offers input at $1.25 and output at $5.\n- **GPT-4o-mini**: Extremely cost-efficient with input at $0.15 and output at $0.60.\n\n#### O-Series Reasoning Models\n\n- **O3 (Global)**: Input priced at $2 and output at $8 per million tokens. Batch API: $1 input, $4 output.\n- **O4-mini (Global)**: Priced at $1.10 input, $4.40 output. With Batch API, input drops to $0.55 and output to $2.20.\n- **O1 (Global)**: High-performance model with input at $15 and output at $60 per million tokens.\n\n#### Deep Research\n\n- **O3-deep research**: Input tokens cost $10/million, cached input $2.50, and output $40/million tokens. Bing Search grounding is charged separately.\n\n#### Multimodal and Visual Models\n\n- **GPT-Image-1 (Global)**: Input text at $5, input image at $10, and output image at $40 per million tokens.\n- **DALL·E 3**: $4.40 per 100 standard-resolution images (1024x1024), and $8.80–$13.20 for HD or wide formats.\n\n#### Audio and Realtime Models\n\n- **GPT-realtime (Global)**: $4 input and $16 output per million text tokens. Audio input is $32 and output $64.\n- **GPT-audio (Global)**: Text input at $2.50, output at $10. Audio input is $40 and output $80.\n- **GPT-4o-realtime-preview**: Text input costs $5 and output $20. Audio input is $40, output $80.\n\n#### Chat and Realtime Mini Models\n\n- **GPT-4o-mini-audio-preview (Global)**: Text input at $0.15, output at $0.60. Audio input/output at $10/$20.\n- **GPT-4o-mini-realtime (Global)**: Text input at $0.60 and output at $2.40. Audio input/output at $10/$20.\n\n#### Embedding Models\n\n- **Text-embedding-3-large**: $0.000143 per 1,000 tokens.\n- **Text-embedding-3-small**: $0.000022 per 1,000 tokens.\n- **Ada**: $0.00011 per 1,000 tokens.\n\n### Pricing by Provisioning Model\n\n#### Provisioned Throughput Units (PTUs)\n\nFor stable and consistent throughput, PTUs are available:\n\n- **GPT-5 (Global)**: $1/hour, $260/month, or $2,652/year for 15 PTUs minimum.\n- **GPT-4o-mini (Global)**: Same rate as GPT-5.\n- **Regional deployments** require higher PTUs and cost $2/hour.\n\n#### Batch API Discounts\n\nBatch API cuts costs by up to 50%. For example:\n\n- **GPT-4.1 Global**: Batch input at $1, output at $4 per million tokens.\n- **O3-mini Global**: Batch input/output at $0.55 and $2.20, respectively.\n\n#### Fine-Tuning and Hosting\n\n- **Fine-tuning (O4-mini)**: $110/hour for training, $1.70/hour for hosting.\n- **Input/output pricing** aligns with the base model (e.g., $1.21 input, $4.84 output per million tokens for O4-mini Regional).\n\n![Asaf-2](https://www.finout.io/hs-fs/hubfs/Asaf-2.png?width=120&height=121&name=Asaf-2.png)\n\nAsaf Liveanu\n\nCo-Founder & CPO\n\nTips from the expert\n\nIn my experience, here are tips that can help you better control and optimize Azure OpenAI costs:\n\n1. **Design “cost classes” for workloads**: Classify use cases into tiers like _gold/silver/bronze_ with explicit caps on model families, context window, and pricing model (on-demand vs PTU vs batch). Enforce these classes in code so experiments can’t silently use GPT-5 Global where a GPT-5-nano or GPT-4.1-mini batch job would suffice.\n\n2. **Make your prompts cache-friendly by design**: Caching was mentioned, but the edge comes from _engineering for cache hits_: normalize prompts (same ordering, whitespace, and field names), reduce randomness (low temperature / fixed seeds where possible), and separate “dynamic” fields from the core instruction text. This can easily 2–3x your hit rate in high-volume flows.\n\n3. **Use a router model to downshift expensive requests**: Put a cheap model (e.g., mini/nano) in front as a “traffic cop” that decides whether a request really needs a premium model, or can be handled by a cheaper one or even a static rule/template. In practice, 30–60% of real traffic can often be downgraded once you measure quality impact.\n\n4. **Split flows into cheap + expensive stages**: Instead of one big GPT-5 call, decompose into stages: inexpensive models for classification, extraction, and validation; premium models only for the final “creative” or complex step. This keeps your most expensive tokens for the work that truly needs them.\n\n5. **Build a per-request cost estimator into your app**: Add a middleware that estimates token counts and _predicted_ cost before calling Azure OpenAI. Use that to enforce guardrails: reject or downscale requests that exceed a per-user or per-feature cost budget, or automatically switch to batch processing for large jobs.\n\n## Cost Optimization Strategies for Azure OpenAI\n\nHere are a few ways your organization can optimize costs for OpenAI models consumed through the Azure platform.\n\n### 1\\. Model Selection and Right-Sizing\n\nSelecting the appropriate model for your workload is crucial in optimizing both cost and performance on Azure OpenAI Service. Models like GPT-5 Global are more expensive but also significantly more capable, while GPT-5-mini or Codex may suffice for simpler language or code tasks at a lower price point per token. Mapping business requirements to the lowest viable model family ensures operational efficiency, and periodically reevaluating your selection as new models or capabilities are released can reveal further savings.\n\nRight-sizing also involves choosing appropriate context window sizes and throughput settings. If your use case does not require extended prompt lengths or high throughput, opting for smaller variants can control costs. Evaluate the need for higher throughput units only during peak operational times, and scale back during off-hours. Regularly reviewing model usage and application performance helps align resource allocation with actual demand, thus avoiding over-provisioning.\n\n### 2\\. Token Efficiency (Prompt + Response)\n\nToken consumption is central to how expenses accrue on Azure OpenAI Service. Efficient use of tokens, both in prompts and responses, directly reduces overall costs. Craft prompts to be concise while retaining necessary context and instruct the model to limit its output length. Unnecessarily verbose input or requesting elaborate outputs increases token usage; iterative prompt engineering often reveals opportunities to streamline these exchanges without sacrificing quality.\n\nImplement safeguards within applications to constrain maximum response lengths using model parameters like max\\_tokens, and actively monitor token consumption patterns in production. Regular audits of user interactions may highlight predictable inefficiencies, such as redundant context passed in each prompt or overly generous output caps. By focusing on both sides of the token transaction, organizations can realize immediate cost reductions in ongoing Azure OpenAI workloads.\n\n### 3\\. Reuse / Caching Where Possible\n\nReducing redundant calls to Azure OpenAI models lowers token consumption and speeds up application response times. Implementing caching mechanisms to store and reuse model outputs for frequently repeated prompts prevents unnecessary API calls. For deterministic queries or fixed templates, cache the returned results for a defined period or until underlying data changes.\n\nIn scenarios where prompts and responses follow predictable patterns—such as FAQs, template-based text generation, or code snippets—reuse cached completions wherever possible. Integrate caching at the application or middleware layer, and monitor cache hit rates to assess savings. This practice not only helps cap expenses but contributes to better system scalability and user experience during periods of high usage.\n\n### 4\\. Optimizing Region and Throughput Allocation\n\nThe geographic region selected for deploying Azure OpenAI resources influences latency, availability, and pricing. Certain Azure regions may offer reduced rates or higher resource availability, making them more cost-effective choices for organizations not bound by strict data residency requirements. Review regional pricing tables regularly and consider moving workloads to less expensive areas where compliance requirements allow.\n\nThroughput allocation determines how many concurrent requests your deployment can handle. Overestimating throughput needs can result in paying for unused capacity, while underestimating leads to throttled requests and subpar performance. Assess historical request patterns to adjust allocated throughput dynamically, using automation where possible, and optimize spending in relation to predictable usage peaks and troughs.\n\n### 5\\. Basic Monitoring With Azure Cost Management\n\nAzure Cost Management provides basic tools for tracking, analyzing, and forecasting expenditure on Azure OpenAI Service. Set up custom cost alerts and budget thresholds to detect anomalies early and avoid overruns. Azure provides usage analytics that breaks down consumption by model, project, or department, enabling granular visibility into token usage and associated expenses.\n\nRegularly review these metrics in conjunction with business operations to align resource allocation and spending with actual demand. Azure Cost Management’s reporting features also help identify trends and forecast future costs.\n\n### 6\\. Advanced Cost Optimization with Finout for Azure OpenAI\n\nWhile Azure offers basic tools for monitoring usage, organizations scaling AI workloads require deeper visibility and precise cost attribution. Finout expands on native Azure capabilities by offering an enterprise-grade FinOps platform that helps teams analyze, allocate, and control Azure OpenAI costs with greater precision. By combining Azure billing data, Azure OpenAI usage metrics, and business context, Finout supports end-to-end financial management across complex, multi-cloud environments.\n\nHere is how Finout enhances your Azure OpenAI FinOps maturity:\n\n- **Unified Cost Visibility with MegaBill:** Automatically ingests Azure OpenAI charges and consolidates them with all cloud (AWS, GCP, etc.) and SaaS expenses (like Snowflake). This MegaBill provides a single view to analyze how model families, throughput, and related Azure services contribute to total spend.\n- **Granular Allocation with Virtual Tags:** Overcomes the limitations of native Azure tags by using Virtual Tags to instantly map complex costs (like token usage or PTU consumption) to specific business dimensions: teams, projects, features, or customers. This supports accurate showback and chargeback without code changes.\n- **Detailed Unit Cost Analysis:** Allows teams to calculate specific unit economics for AI workloads, such as cost per request, cost per user, or cost per feature. These insights drive decisions regarding scaling, model selection, and workload prioritization.\n- **AI-Specific Optimization Features:** Provides dedicated capabilities to evaluate token footprints, identify workloads suited for Batch API or Provisioned Throughput Units (PTUs), and analyze how prompt structure influences total costs.\n- **Proactive Anomaly Detection:** **CostGuard** identifies optimization opportunities and presents actionable recommendations (combining Finout and Azure Advisor insights). It also alerts teams to unexpected increases in token usage or throughput consumption before costs escalate.\n- **Complete Financial Governance:** By integrating Finout with Azure data, every token, model call, and throughput unit is attributed to the correct business entity. This ensures clear budgets, accurate forecasting, and predictable spending as Azure OpenAI workloads grow.\n\nReady to gain full traceability and control over your Azure OpenAI spending?\n\n[Book a demo](https://www.finout.io/book-a-demo-page?__hstc=252782486.7bead9410e1b900f5311ebcf5ec1bd75.1708868691142.1736084133978.1736089135157.392&__hssc=252782486.6.1736089135157&__hsfp=3435076530&hsutk=7bead9410e1b900f5311ebcf5ec1bd75&contentType=blog-post) today and see how Finout can transform the way you manage cloud spend.\n\n[![Top 6 AI Cost Drivers and GenAI Cost Examples in 2026](https://www.finout.io/hs-fs/hubfs/Top%206%20AI%20Cost%20Drivers%20and%20GenAI.png?width=1200&height=688&name=Top%206%20AI%20Cost%20Drivers%20and%20GenAI.png)\\\\\n\\\\\nFeb 12th, 2026\\\\\n\\\\\nTop 6 AI Cost Drivers and GenAI Cost Examples in 2026\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/top-6-ai-cost-drivers-and-genai-cost-examples-in-2026) [![Best Kubernetes Cost Management Services: Top 5 in 2026](https://www.finout.io/hs-fs/hubfs/Best%20Kubernetes%20Cost%20Management%20Services_.png?width=1200&height=688&name=Best%20Kubernetes%20Cost%20Management%20Services_.png)\\\\\n\\\\\nFeb 10th, 2026\\\\\n\\\\\nBest Kubernetes Cost Management Services: Top 5 in 2026\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/best-kubernetes-cost-management-services-top-5-in-2026) [![Ultimate Guide to FinOps: Principles, Phases, and Technology](https://www.finout.io/hs-fs/hubfs/Ultimate%20Guide%20to%20FinOps.png?width=1200&height=688&name=Ultimate%20Guide%20to%20FinOps.png)\\\\\n\\\\\nFeb 8th, 2026\\\\\n\\\\\nUltimate Guide to FinOps: Principles, Phases, and Technology\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/ultimate-guide-to-finops-principles-phases-and-technology) [![Snowflake Cortex AI: Generative AI Inside Your Data Cloud (part 1)](https://www.finout.io/hs-fs/hubfs/Gemini_Generated_Image_55pbbo55pbbo55pb.jpeg?width=1200&height=688&name=Gemini_Generated_Image_55pbbo55pbbo55pb.jpeg)\\\\\n\\\\\nFeb 3rd, 2026\\\\\n\\\\\nSnowflake Cortex AI: Generative AI Inside Your Data Cloud (part 1)\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/snowflake-cortex-ai-generative-ai-inside-your-data-cloud-part-1) [![Top 50 FinOps Tools to Consider in 2026](https://www.finout.io/hs-fs/hubfs/Finops35-1.png?width=1200&height=688&name=Finops35-1.png)\\\\\n\\\\\nJan 30th, 2026\\\\\n\\\\\nTop 50 FinOps Tools to Consider in 2026\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/finops-tools-guide) [![Best Kubernetes Cost Management Solutions: Top 4 in 2026](https://www.finout.io/hs-fs/hubfs/Kubernetes%20Cost%20Management%20Services.png?width=1200&height=688&name=Kubernetes%20Cost%20Management%20Services.png)\\\\\n\\\\\nJan 25th, 2026\\\\\n\\\\\nBest Kubernetes Cost Management Solutions: Top 4 in 2026\\\\\n\\\\\nAzureAIAzure PricingAzure Cost ManagementAzure Cost Optimzation\\\\\n\\\\\nRead more](https://www.finout.io/blog/best-kubernetes-cost-management-solutions-top-4-in-2026)\n\nReceive blog and product updates\n\nMain topics\n\n[![Finout logo](https://www.finout.io/hubfs/finout2022/images/New_product_page/footer_new_logo.svg)](https://www.finout.io/)\n\nFinout is an enterprise-grade FinOps solution that helps companies easily allocate, manage and reduce their cloud spending across their entire infrastructure.\n\n[![X](https://www.finout.io/hubfs/xLogo.svg)](https://x.com/finout_io)[![Slack](https://www.finout.io/hubfs/slack-logo.svg)](https://start-chat.com/slack/finout/HwVAXB)[![Linkedin](https://www.finout.io/hubfs/Linkedin_outline.svg)](https://www.linkedin.com/company/finout-io/)[![Facebook](https://www.finout.io/hubfs/facebook_outline.svg)](https://www.facebook.com/finout.io)[![Dribbble](https://www.finout.io/hubfs/DribbbleLogo.svg)](https://dribbble.com/Finout)\n\nSOLUTION\n\n\n- Main Features\n- [MegaBill](https://www.finout.io/megabill)\n- [Virtual Tags](https://www.finout.io/virtual-tagging)\n- [AI-Powered VTags](https://www.finout.io/megabill/ai-virtual-tag)\n- [Shared Cost](https://www.finout.io/shared-cost)\n- [Financial Plans](https://www.finout.io/financial-planning)\n- Cost Optimization\n- [CostGuard](https://www.finout.io/costguard)\n- [CostGuard Scans](https://www.finout.io/costguard-scans)\n- FinOps Features\n- [Anomaly Detection](https://www.finout.io/anomaly-detection)\n- [FinOps Dashboards](https://www.finout.io/dashboards-and-reporting)\n- [AI Cost Management](https://www.finout.io/artificial-intelligence)\n- [Data Layer](https://www.finout.io/data-layer)\n\nINTEGRATIONS\n\n\n- Cloud Providers\n- [AWS](https://www.finout.io/aws-product-page)\n- [GCP](https://www.finout.io/gcp)\n- [Azure](https://www.finout.io/azure)\n- [OCI](https://www.finout.io/oracle-integration)\n- Cloud Services\n- [OpenAI](https://www.finout.io/openai)\n- [Anthropic](https://www.finout.io/anthropic-integration)\n- [Kubernetes](https://www.finout.io/kubernetes-waste-detection)\n- [Snowflake](https://www.finout.io/finout-snowflake-solution)\n- [Databricks](https://www.finout.io/finout-databricks-solution)\n- Dev Services\n- [Slack](https://www.finout.io/slack-and-finout-integration-track-usage-and-anomalies)\n- [Datadog](https://www.finout.io/finouts-datadog-integration)\n\nRESOURCES\n\n\n- [Product Overview](https://www.finout.io/openai)\n- [Documentation](https://docs.finout.io/)\n- [Customer Stories](https://www.finout.io/customer-stories)\n- [Blogs](https://www.finout.io/blog)\n- [Webinars](https://www.finout.io/events-and-webinars)\n- [eBooks](https://www.finout.io/ebooks)\n- [Tools](https://www.finout.io/tools)\n\nCOMPANY\n\n\n- [Contact Us](https://www.finout.io/contact)\n- [Pricing](https://www.finout.io/pricing)\n- [Careers Join us!](https://www.finout.io/careers)\n- [About Us](https://www.finout.io/about/)\n- [Media Kit](https://www.finout.io/media-kit)\n- [Compliance](https://www.finout.io/compliance)\n\n© Finout 2026. All Rights Reserved. [Privacy Policy](https://www.finout.io/privacy-policy/) [Terms of Use](https://www.finout.io/terms-of-use/)\n\n![cloud_native_logo_2](https://www.finout.io/hs-fs/hubfs/cloud_native_logo_2.png?width=193&height=36&name=cloud_native_logo_2.png)\n\n© Finout 2026. All Rights Reserved. [Privacy Policy](https://www.finout.io/privacy-policy/) [Terms of Use](https://www.finout.io/terms-of-use/)","metadata":{"twitter:domain":"www.finout.io","og:image:height":"327","description":"Azure OpenAI Service offers pricing models like standard, PTUs, and batch APIs, giving flexible cost control, scalability, and performance.\n","og:image":"https://www.finout.io/hubfs/Azure%20OpenAI%20Pricing.png","twitter:image":"https://www.finout.io/hubfs/Azure%20OpenAI%20Pricing.png","ogTitle":"Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs","og:image:width":"600","og:url":"https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs","language":"en-us","twitter:description":"Azure OpenAI Service offers pricing models like standard, PTUs, and batch APIs, giving flexible cost control, scalability, and performance.\n","og:description":"Azure OpenAI Service offers pricing models like standard, PTUs, and batch APIs, giving flexible cost control, scalability, and performance.\n","theme-color":"#1b1f1d","ogImage":"https://www.finout.io/hubfs/Azure%20OpenAI%20Pricing.png","twitter:card":"summary_large_image","twitter:title":"Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs","robots":"index,follow","og:title":"Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs","msapplication-TileColor":"#38b28e","google-site-verification":"tLvYJ_fHhx5vyqnl3l7UIr_-h397d2hBxU5mV4lZSCk","og:type":"article","ogUrl":"https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs","generator":"HubSpot","viewport":"width=device-width, initial-scale=1","title":"Understanding Azure OpenAI Pricing & 6 Ways to Cut Costs","ogDescription":"Azure OpenAI Service offers pricing models like standard, PTUs, and batch APIs, giving flexible cost control, scalability, and performance.\n","favicon":"https://www.finout.io/hubfs/favicons/favicon-32x32.png","scrapeId":"019c62eb-509c-7409-9bdc-1ac94d408a76","sourceURL":"https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs","url":"https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs","statusCode":200,"contentType":"text/html; charset=UTF-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"2bcdf72b-2851-4317-9412-ff0262f1636f","creditsUsed":1},"links":["https://www.finout.io/megabill/ai-virtual-tag","https://www.finout.io/blog","https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs#A","https://www.finout.io/book-a-demo-page?__hstc=252782486.7bead9410e1b900f5311ebcf5ec1bd75.1708868691142.1736084133978.1736089135157.392&__hssc=252782486.6.1736089135157&__hsfp=3435076530&hsutk=7bead9410e1b900f5311ebcf5ec1bd75&contentType=blog-post","https://www.finout.io/blog/top-6-ai-cost-drivers-and-genai-cost-examples-in-2026","https://www.finout.io/blog/best-kubernetes-cost-management-services-top-5-in-2026","https://www.finout.io/blog/ultimate-guide-to-finops-principles-phases-and-technology","https://www.finout.io/blog/snowflake-cortex-ai-generative-ai-inside-your-data-cloud-part-1","https://www.finout.io/blog/finops-tools-guide","https://www.finout.io/blog/best-kubernetes-cost-management-solutions-top-4-in-2026","https://www.finout.io/","https://x.com/finout_io","https://start-chat.com/slack/finout/HwVAXB","https://www.linkedin.com/company/finout-io/","https://www.facebook.com/finout.io","https://dribbble.com/Finout","https://www.finout.io/megabill","https://www.finout.io/virtual-tagging","https://www.finout.io/shared-cost","https://www.finout.io/financial-planning","https://www.finout.io/costguard","https://www.finout.io/costguard-scans","https://www.finout.io/anomaly-detection","https://www.finout.io/dashboards-and-reporting","https://www.finout.io/artificial-intelligence","https://www.finout.io/data-layer","https://www.finout.io/aws-product-page","https://www.finout.io/gcp","https://www.finout.io/azure","https://www.finout.io/oracle-integration","https://www.finout.io/openai","https://www.finout.io/anthropic-integration","https://www.finout.io/kubernetes-waste-detection","https://www.finout.io/finout-snowflake-solution","https://www.finout.io/finout-databricks-solution","https://www.finout.io/slack-and-finout-integration-track-usage-and-anomalies","https://www.finout.io/finouts-datadog-integration","https://docs.finout.io/","https://www.finout.io/customer-stories","https://www.finout.io/events-and-webinars","https://www.finout.io/ebooks","https://www.finout.io/tools","https://www.finout.io/contact","https://www.finout.io/pricing","https://www.finout.io/careers","https://www.finout.io/about/","https://www.finout.io/media-kit","https://www.finout.io/compliance","https://www.finout.io/privacy-policy/","https://www.finout.io/terms-of-use/"]},{"url":"https://developers.openai.com/api/docs/guides/rate-limits/","title":"Rate limits | OpenAI API","description":"Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.","position":5,"markdown":"## Search the API docs\n\nClose\n\nClear\n\nPrimary navigation\n\nClear\n\n### Get started\n\n- [Overview](https://developers.openai.com/api/docs)\n- [Quickstart](https://developers.openai.com/api/docs/quickstart)\n- [Models](https://developers.openai.com/api/docs/models)\n- [Pricing](https://developers.openai.com/api/docs/pricing)\n- [Libraries](https://developers.openai.com/api/docs/libraries)\n- [Docs MCP](https://developers.openai.com/resources/docs-mcp)\n- [Latest: GPT-5.2](https://developers.openai.com/api/docs/guides/latest-model)\n\n### Core concepts\n\n- [Text generation](https://developers.openai.com/api/docs/guides/text)\n- [Code generation](https://developers.openai.com/api/docs/guides/code-generation)\n- [Images and vision](https://developers.openai.com/api/docs/guides/images-vision)\n- [Audio and speech](https://developers.openai.com/api/docs/guides/audio)\n- [Structured output](https://developers.openai.com/api/docs/guides/structured-outputs)\n- [Function calling](https://developers.openai.com/api/docs/guides/function-calling)\n- [Responses API](https://developers.openai.com/api/docs/guides/migrate-to-responses)\n\n### Agents\n\n- [Overview](https://developers.openai.com/api/docs/guides/agents)\n- Build agents\n\n  - [Agent Builder](https://developers.openai.com/api/docs/guides/agent-builder)\n  - [Node reference](https://developers.openai.com/api/docs/guides/node-reference)\n  - [Safety in building agents](https://developers.openai.com/api/docs/guides/agent-builder-safety)\n  - [Agents SDK](https://developers.openai.com/api/docs/guides/agents-sdk)\n\n- Deploy in your product\n\n  - [ChatKit](https://developers.openai.com/api/docs/guides/chatkit)\n  - [Custom theming](https://developers.openai.com/api/docs/guides/chatkit-themes)\n  - [Widgets](https://developers.openai.com/api/docs/guides/chatkit-widgets)\n  - [Actions](https://developers.openai.com/api/docs/guides/chatkit-actions)\n  - [Advanced integration](https://developers.openai.com/api/docs/guides/custom-chatkit)\n\n- Optimize\n\n  - [Agent evals](https://developers.openai.com/api/docs/guides/agent-evals)\n  - [Trace grading](https://developers.openai.com/api/docs/guides/trace-grading)\n\n- [Voice agents](https://developers.openai.com/api/docs/guides/voice-agents)\n\n### Tools\n\n- [Using tools](https://developers.openai.com/api/docs/guides/tools)\n- [Connectors and MCP](https://developers.openai.com/api/docs/guides/tools-connectors-mcp)\n- [Skills](https://developers.openai.com/api/docs/guides/tools-skills)\n- [Shell](https://developers.openai.com/api/docs/guides/tools-shell)\n- [Web search](https://developers.openai.com/api/docs/guides/tools-web-search)\n- [Code interpreter](https://developers.openai.com/api/docs/guides/tools-code-interpreter)\n- File search and retrieval\n\n  - [File search](https://developers.openai.com/api/docs/guides/tools-file-search)\n  - [Retrieval](https://developers.openai.com/api/docs/guides/retrieval)\n\n- More tools\n\n  - [Image generation](https://developers.openai.com/api/docs/guides/tools-image-generation)\n  - [Computer use](https://developers.openai.com/api/docs/guides/tools-computer-use)\n  - [Local shell tool](https://developers.openai.com/api/docs/guides/tools-local-shell)\n  - [Apply patch](https://developers.openai.com/api/docs/guides/tools-apply-patch)\n\n### Run and scale\n\n- [Conversation state](https://developers.openai.com/api/docs/guides/conversation-state)\n- [Compaction](https://developers.openai.com/api/docs/guides/compaction)\n- [Background mode](https://developers.openai.com/api/docs/guides/background)\n- [Streaming](https://developers.openai.com/api/docs/guides/streaming-responses)\n- [Webhooks](https://developers.openai.com/api/docs/guides/webhooks)\n- [File inputs](https://developers.openai.com/api/docs/guides/pdf-files)\n- Prompting\n\n  - [Overview](https://developers.openai.com/api/docs/guides/prompting)\n  - [Prompt caching](https://developers.openai.com/api/docs/guides/prompt-caching)\n  - [Prompt engineering](https://developers.openai.com/api/docs/guides/prompt-engineering)\n\n- Reasoning\n\n  - [Reasoning models](https://developers.openai.com/api/docs/guides/reasoning)\n  - [Reasoning best practices](https://developers.openai.com/api/docs/guides/reasoning-best-practices)\n\n### Evaluation\n\n- [Getting started](https://developers.openai.com/api/docs/guides/evaluation-getting-started)\n- [Working with evals](https://developers.openai.com/api/docs/guides/evals)\n- [Prompt optimizer](https://developers.openai.com/api/docs/guides/prompt-optimizer)\n- [External models](https://developers.openai.com/api/docs/guides/external-models)\n- [Best practices](https://developers.openai.com/api/docs/guides/evaluation-best-practices)\n\n### Realtime API\n\n- [Overview](https://developers.openai.com/api/docs/guides/realtime)\n- Connect\n\n  - [WebRTC](https://developers.openai.com/api/docs/guides/realtime-webrtc)\n  - [WebSocket](https://developers.openai.com/api/docs/guides/realtime-websocket)\n  - [SIP](https://developers.openai.com/api/docs/guides/realtime-sip)\n\n- Usage\n\n  - [Using realtime models](https://developers.openai.com/api/docs/guides/realtime-models-prompting)\n  - [Managing conversations](https://developers.openai.com/api/docs/guides/realtime-conversations)\n  - [Webhooks and server-side controls](https://developers.openai.com/api/docs/guides/realtime-server-controls)\n  - [Managing costs](https://developers.openai.com/api/docs/guides/realtime-costs)\n  - [Realtime transcription](https://developers.openai.com/api/docs/guides/realtime-transcription)\n  - [Voice agents](https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/)\n\n### Model optimization\n\n- [Optimization cycle](https://developers.openai.com/api/docs/guides/model-optimization)\n- Fine-tuning\n\n  - [Supervised fine-tuning](https://developers.openai.com/api/docs/guides/supervised-fine-tuning)\n  - [Vision fine-tuning](https://developers.openai.com/api/docs/guides/vision-fine-tuning)\n  - [Direct preference optimization](https://developers.openai.com/api/docs/guides/direct-preference-optimization)\n  - [Reinforcement fine-tuning](https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning)\n  - [RFT use cases](https://developers.openai.com/api/docs/guides/rft-use-cases)\n  - [Best practices](https://developers.openai.com/api/docs/guides/fine-tuning-best-practices)\n\n- [Graders](https://developers.openai.com/api/docs/guides/graders)\n\n### Specialized models\n\n- [Image generation](https://developers.openai.com/api/docs/guides/image-generation)\n- [Video generation](https://developers.openai.com/api/docs/guides/video-generation)\n- [Text to speech](https://developers.openai.com/api/docs/guides/text-to-speech)\n- [Speech to text](https://developers.openai.com/api/docs/guides/speech-to-text)\n- [Deep research](https://developers.openai.com/api/docs/guides/deep-research)\n- [Embeddings](https://developers.openai.com/api/docs/guides/embeddings)\n- [Moderation](https://developers.openai.com/api/docs/guides/moderation)\n\n### Coding agents\n\n- [Codex cloud](https://developers.openai.com/codex/cloud)\n- [Agent internet access](https://developers.openai.com/codex/cloud/agent-internet)\n- [Codex CLI](https://developers.openai.com/codex/cli)\n- [Codex IDE](https://developers.openai.com/codex/ide)\n- [Codex changelog](https://developers.openai.com/codex/changelog)\n\n### Going live\n\n- [Production best practices](https://developers.openai.com/api/docs/guides/production-best-practices)\n- Latency optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/latency-optimization)\n  - [Predicted Outputs](https://developers.openai.com/api/docs/guides/predicted-outputs)\n  - [Priority processing](https://developers.openai.com/api/docs/guides/priority-processing)\n\n- Cost optimization\n\n  - [Overview](https://developers.openai.com/api/docs/guides/cost-optimization)\n  - [Batch](https://developers.openai.com/api/docs/guides/batch)\n  - [Flex processing](https://developers.openai.com/api/docs/guides/flex-processing)\n\n- [Accuracy optimization](https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy)\n- Safety\n\n  - [Safety best practices](https://developers.openai.com/api/docs/guides/safety-best-practices)\n  - [Safety checks](https://developers.openai.com/api/docs/guides/safety-checks)\n  - [Under 18 API Guidance](https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance)\n\n### Legacy APIs\n\n- Assistants API\n\n  - [Migration guide](https://developers.openai.com/api/docs/assistants/migration)\n  - [Deep dive](https://developers.openai.com/api/docs/assistants/deep-dive)\n  - [Tools](https://developers.openai.com/api/docs/assistants/tools)\n\n### Resources\n\n- [Terms and policies](https://openai.com/policies)\n- [Changelog](https://developers.openai.com/api/docs/changelog)\n- [Your data](https://developers.openai.com/api/docs/guides/your-data)\n- [Permissions](https://developers.openai.com/api/docs/guides/rbac)\n- [Rate limits](https://developers.openai.com/api/docs/guides/rate-limits)\n- [Deprecations](https://developers.openai.com/api/docs/deprecations)\n- [MCP for deep research](https://developers.openai.com/api/docs/mcp)\n- [Developer mode](https://developers.openai.com/api/docs/guides/developer-mode)\n- ChatGPT Actions\n\n  - [Introduction](https://developers.openai.com/api/docs/actions/introduction)\n  - [Getting started](https://developers.openai.com/api/docs/actions/getting-started)\n  - [Actions library](https://developers.openai.com/api/docs/actions/actions-library)\n  - [Authentication](https://developers.openai.com/api/docs/actions/authentication)\n  - [Production](https://developers.openai.com/api/docs/actions/production)\n  - [Data retrieval](https://developers.openai.com/api/docs/actions/data-retrieval)\n  - [Sending files](https://developers.openai.com/api/docs/actions/sending-files)\n\n[API Dashboard](https://platform.openai.com/login)\n\nSearch\n⌘\n\nK\n\nCopy PageMore page actions\n\nCopy PageMore page actions\n\nRate limits are restrictions that our API imposes on the number of times a user or client can\naccess our services within a specified period of time.\n\n## Why do we have rate limits?\n\nRate limits are a common practice for APIs, and they’re put in place for a few different reasons:\n\n- **They help protect against abuse or misuse of the API.** For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n- **Rate limits help ensure that everyone has fair access to the API.** If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.\n- **Rate limits can help OpenAI manage the aggregate load on its infrastructure.** If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n\nPlease work through this document in its entirety to better understand how\nOpenAI’s rate limit system works. We include code examples and possible\nsolutions to handle common issues. We also include details around how your\nrate limits are automatically increased in the usage tiers section below.\n\n## How do these rate limits work?\n\nRate limits are measured in five ways: **RPM** (requests per minute), **RPD** (requests per day), **TPM** (tokens per minute), **TPD** (tokens per day), and **IPM** (images per minute). Rate limits can be hit across any of the options depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the ChatCompletions endpoint and that would fill your limit (if your RPM was 20), even if you did not send 150k tokens (if your TPM limit was 150k) within those 20 requests.\n\n[Batch API](https://developers.openai.com/api/docs/api-reference/batch/create) queue limits are calculated based on the total number of input tokens queued for a given model. Tokens from pending batch jobs are counted against your queue limit. Once a batch job is completed, its tokens are no longer counted against that model’s limit.\n\nOther important things worth noting:\n\n- Rate limits are defined at the [organization level](https://developers.openai.com/api/docs/guides/production-best-practices) and at the project level, not user level.\n- Rate limits vary by the [model](https://developers.openai.com/api/docs/models) being used.\n- For long context models like GPT-4.1, there is a separate rate limit for long context requests. You can view these rate limits in [developer console](https://platform.openai.com/settings/organization/limits).\n- Limits are also placed on the total amount an organization can spend on the API each month. These are also known as “usage limits”.\n- Some model families have shared rate limits. Any models listed under a “shared limit” in your [organizations limit page](https://platform.openai.com/settings/organization/limits) share a rate limit between them. For example, if the listed shared TPM is 3.5M, all calls to any model in the given “shared limit” list will count towards that 3.5M.\n\n## Usage tiers\n\nYou can view the rate and usage limits for your organization under the [limits](https://platform.openai.com/settings/organization/limits) section of your account settings. As your spend on our API goes up, we automatically graduate you to the next usage tier. This usually results in an increase in rate limits across most models.\n\n| Tier | Qualification | Usage limits |\n| --- | --- | --- |\n| Free | User must be in an [allowed geography](https://developers.openai.com/api/docs/supported-countries) | $100 / month |\n| Tier 1 | $5 paid | $100 / month |\n| Tier 2 | $50 paid and 7+ days since first successful payment | $500 / month |\n| Tier 3 | $100 paid and 7+ days since first successful payment | $1,000 / month |\n| Tier 4 | $250 paid and 14+ days since first successful payment | $5,000 / month |\n| Tier 5 | $1,000 paid and 30+ days since first successful payment | $200,000 / month |\n\nTo view a high-level summary of rate limits per model, visit the [models page](https://developers.openai.com/api/docs/models).\n\n### Rate limits in headers\n\nIn addition to seeing your rate limit on your [account page](https://platform.openai.com/settings/organization/limits), you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.\n\nYou can expect to see the following header fields:\n\n| Field | Sample Value | Description |\n| --- | --- | --- |\n| x-ratelimit-limit-requests | 60 | The maximum number of requests that are permitted before exhausting the rate limit. |\n| x-ratelimit-limit-tokens | 150000 | The maximum number of tokens that are permitted before exhausting the rate limit. |\n| x-ratelimit-remaining-requests | 59 | The remaining number of requests that are permitted before exhausting the rate limit. |\n| x-ratelimit-remaining-tokens | 149984 | The remaining number of tokens that are permitted before exhausting the rate limit. |\n| x-ratelimit-reset-requests | 1s | The time until the rate limit (based on requests) resets to its initial state. |\n| x-ratelimit-reset-tokens | 6m0s | The time until the rate limit (based on tokens) resets to its initial state. |\n\n### Fine-tuning rate limits\n\nThe fine-tuning rate limits for your organization can be [found in the dashboard as well](https://platform.openai.com/settings/organization/limits), and can also be retrieved via API:\n\n```\ncurl https://api.openai.com/v1/fine_tuning/model_limits \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n```\n\n## Error mitigation\n\n### What are some steps I can take to mitigate this?\n\nThe OpenAI Cookbook has a [Python notebook](https://developers.openai.com/cookbook/examples/how_to_handle_rate_limits) that explains how to avoid rate limit errors, as well an example [Python script](https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py) for staying under rate limits while batch processing API requests.\n\nYou should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.\n\nTo protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.\n\n#### Retrying with exponential backoff\n\nOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:\n\n- Automatic retries means you can recover from rate limit errors without crashes or missing data\n- Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n- Adding random jitter to the delay helps retries from all hitting at the same time.\n\nNote that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n\nBelow are a few example solutions **for Python** that use exponential backoff.\n\nExample 1: Using the Tenacity library\n\nTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the `tenacity.retry` decorator. The below example uses the `tenacity.wait_random_exponential` function to add random exponential backoff to a request.\n\nUsing the Tenacity library\n\npython\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nfrom openai import OpenAI\nclient = OpenAI()\n\nfrom tenacity import (\nretry,\nstop_after_attempt,\nwait_random_exponential,\n) # for exponential backoff\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\nreturn client.completions.create(**kwargs)\n\ncompletion_with_backoff(model=\"gpt-4o-mini\", prompt=\"Once upon a time,\")\n```\n\nNote that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about\nits reliability or security.\n\nExample 2: Using the backoff library\n\nAnother python library that provides function decorators for backoff and retry is [backoff](https://pypi.org/project/backoff/):\n\nUsing the Tenacity library\n\npython\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nimport backoff\nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n\n@backoff.on_exception(backoff.expo, openai.RateLimitError)\ndef completions_with_backoff(**kwargs):\nreturn client.completions.create(**kwargs)\n\ncompletions_with_backoff(model=\"gpt-4o-mini\", prompt=\"Once upon a time,\")\n```\n\nLike Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\n\nExample 3: Manual backoff implementation\n\nIf you don’t want to use third-party libraries, you can implement your own backoff logic following this example:\n\nUsing manual backoff implementation\n\npython\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n# imports\nimport random\nimport time\n\nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n\n# define a retry decorator\n\ndef retry_with_exponential_backoff(\nfunc,\ninitial_delay: float = 1,\nexponential_base: float = 2,\njitter: bool = True,\nmax_retries: int = 10,\nerrors: tuple = (openai.RateLimitError,),\n):\n\"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\nreturn client.completions.create(**kwargs)\n```\n\nAgain, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.\n\n#### Reduce the `max_tokens` to match the size of your completions\n\nYour rate limit is calculated as the maximum of `max_tokens` and the estimated number of tokens based on the character count of your request. Try to set the `max_tokens` value as close to your expected response size as possible.\n\n#### Batching requests\n\nIf your use case does not require immediate responses, you can use the [Batch API](https://developers.openai.com/api/docs/guides/batch) to more easily submit and execute large collections of requests without impacting your synchronous request rate limits.\n\nFor use cases that _do_ requires synchronous responses, the OpenAI API has separate limits for **requests per minute** and **tokens per minute**.\n\nIf you’re hitting the limit on requests per minute but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.\n\nSending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string. [Learn more in the Batch API guide](https://developers.openai.com/api/docs/guides/batch).","metadata":{"twitter:url":"https://developers.openai.com/api/docs/guides/rate-limits/","twitter:card":"summary_large_image","twitter:title":"Rate limits | OpenAI API","twitter:description":"Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.","viewport":"width=device-width,initial-scale=1","og:type":"website","description":"Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.","title":"Rate limits | OpenAI API","ogUrl":"https://developers.openai.com/api/docs/guides/rate-limits/","og:image":"https://developers.openai.com/open-graph.png","og:description":"Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.","ogTitle":"Rate limits | OpenAI API","language":"en","og:url":"https://developers.openai.com/api/docs/guides/rate-limits/","twitter:image":"https://developers.openai.com/open-graph.png","ogDescription":"Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.","ogImage":"https://developers.openai.com/open-graph.png","og:title":"Rate limits | OpenAI API","generator":"Astro v5.16.15","favicon":"https://developers.openai.com/favicon.png","scrapeId":"019c62eb-509c-7409-9bdc-1d69ba7224e1","sourceURL":"https://developers.openai.com/api/docs/guides/rate-limits/","url":"https://developers.openai.com/api/docs/guides/rate-limits","statusCode":200,"contentType":"text/html; charset=utf-8","proxyUsed":"basic","cacheState":"hit","cachedAt":"2026-02-14T00:45:46.322Z","creditsUsed":1},"links":["https://developers.openai.com/api/docs","https://developers.openai.com/api/docs/quickstart","https://developers.openai.com/api/docs/models","https://developers.openai.com/api/docs/pricing","https://developers.openai.com/api/docs/libraries","https://developers.openai.com/resources/docs-mcp","https://developers.openai.com/api/docs/guides/latest-model","https://developers.openai.com/api/docs/guides/text","https://developers.openai.com/api/docs/guides/code-generation","https://developers.openai.com/api/docs/guides/images-vision","https://developers.openai.com/api/docs/guides/audio","https://developers.openai.com/api/docs/guides/structured-outputs","https://developers.openai.com/api/docs/guides/function-calling","https://developers.openai.com/api/docs/guides/migrate-to-responses","https://developers.openai.com/api/docs/guides/agents","https://developers.openai.com/api/docs/guides/agent-builder","https://developers.openai.com/api/docs/guides/node-reference","https://developers.openai.com/api/docs/guides/agent-builder-safety","https://developers.openai.com/api/docs/guides/agents-sdk","https://developers.openai.com/api/docs/guides/chatkit","https://developers.openai.com/api/docs/guides/chatkit-themes","https://developers.openai.com/api/docs/guides/chatkit-widgets","https://developers.openai.com/api/docs/guides/chatkit-actions","https://developers.openai.com/api/docs/guides/custom-chatkit","https://developers.openai.com/api/docs/guides/agent-evals","https://developers.openai.com/api/docs/guides/trace-grading","https://developers.openai.com/api/docs/guides/voice-agents","https://developers.openai.com/api/docs/guides/tools","https://developers.openai.com/api/docs/guides/tools-connectors-mcp","https://developers.openai.com/api/docs/guides/tools-skills","https://developers.openai.com/api/docs/guides/tools-shell","https://developers.openai.com/api/docs/guides/tools-web-search","https://developers.openai.com/api/docs/guides/tools-code-interpreter","https://developers.openai.com/api/docs/guides/tools-file-search","https://developers.openai.com/api/docs/guides/retrieval","https://developers.openai.com/api/docs/guides/tools-image-generation","https://developers.openai.com/api/docs/guides/tools-computer-use","https://developers.openai.com/api/docs/guides/tools-local-shell","https://developers.openai.com/api/docs/guides/tools-apply-patch","https://developers.openai.com/api/docs/guides/conversation-state","https://developers.openai.com/api/docs/guides/compaction","https://developers.openai.com/api/docs/guides/background","https://developers.openai.com/api/docs/guides/streaming-responses","https://developers.openai.com/api/docs/guides/webhooks","https://developers.openai.com/api/docs/guides/pdf-files","https://developers.openai.com/api/docs/guides/prompting","https://developers.openai.com/api/docs/guides/prompt-caching","https://developers.openai.com/api/docs/guides/prompt-engineering","https://developers.openai.com/api/docs/guides/reasoning","https://developers.openai.com/api/docs/guides/reasoning-best-practices","https://developers.openai.com/api/docs/guides/evaluation-getting-started","https://developers.openai.com/api/docs/guides/evals","https://developers.openai.com/api/docs/guides/prompt-optimizer","https://developers.openai.com/api/docs/guides/external-models","https://developers.openai.com/api/docs/guides/evaluation-best-practices","https://developers.openai.com/api/docs/guides/realtime","https://developers.openai.com/api/docs/guides/realtime-webrtc","https://developers.openai.com/api/docs/guides/realtime-websocket","https://developers.openai.com/api/docs/guides/realtime-sip","https://developers.openai.com/api/docs/guides/realtime-models-prompting","https://developers.openai.com/api/docs/guides/realtime-conversations","https://developers.openai.com/api/docs/guides/realtime-server-controls","https://developers.openai.com/api/docs/guides/realtime-costs","https://developers.openai.com/api/docs/guides/realtime-transcription","https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/","https://developers.openai.com/api/docs/guides/model-optimization","https://developers.openai.com/api/docs/guides/supervised-fine-tuning","https://developers.openai.com/api/docs/guides/vision-fine-tuning","https://developers.openai.com/api/docs/guides/direct-preference-optimization","https://developers.openai.com/api/docs/guides/reinforcement-fine-tuning","https://developers.openai.com/api/docs/guides/rft-use-cases","https://developers.openai.com/api/docs/guides/fine-tuning-best-practices","https://developers.openai.com/api/docs/guides/graders","https://developers.openai.com/api/docs/guides/image-generation","https://developers.openai.com/api/docs/guides/video-generation","https://developers.openai.com/api/docs/guides/text-to-speech","https://developers.openai.com/api/docs/guides/speech-to-text","https://developers.openai.com/api/docs/guides/deep-research","https://developers.openai.com/api/docs/guides/embeddings","https://developers.openai.com/api/docs/guides/moderation","https://developers.openai.com/codex/cloud","https://developers.openai.com/codex/cloud/agent-internet","https://developers.openai.com/codex/cli","https://developers.openai.com/codex/ide","https://developers.openai.com/codex/changelog","https://developers.openai.com/api/docs/guides/production-best-practices","https://developers.openai.com/api/docs/guides/latency-optimization","https://developers.openai.com/api/docs/guides/predicted-outputs","https://developers.openai.com/api/docs/guides/priority-processing","https://developers.openai.com/api/docs/guides/cost-optimization","https://developers.openai.com/api/docs/guides/batch","https://developers.openai.com/api/docs/guides/flex-processing","https://developers.openai.com/api/docs/guides/optimizing-llm-accuracy","https://developers.openai.com/api/docs/guides/safety-best-practices","https://developers.openai.com/api/docs/guides/safety-checks","https://developers.openai.com/api/docs/guides/safety-checks/under-18-api-guidance","https://developers.openai.com/api/docs/assistants/migration","https://developers.openai.com/api/docs/assistants/deep-dive","https://developers.openai.com/api/docs/assistants/tools","https://openai.com/policies","https://developers.openai.com/api/docs/changelog","https://developers.openai.com/api/docs/guides/your-data","https://developers.openai.com/api/docs/guides/rbac","https://developers.openai.com/api/docs/guides/rate-limits","https://developers.openai.com/api/docs/deprecations","https://developers.openai.com/api/docs/mcp","https://developers.openai.com/api/docs/guides/developer-mode","https://developers.openai.com/api/docs/actions/introduction","https://developers.openai.com/api/docs/actions/getting-started","https://developers.openai.com/api/docs/actions/actions-library","https://developers.openai.com/api/docs/actions/authentication","https://developers.openai.com/api/docs/actions/production","https://developers.openai.com/api/docs/actions/data-retrieval","https://developers.openai.com/api/docs/actions/sending-files","https://platform.openai.com/login","https://developers.openai.com/api/docs/api-reference/batch/create","https://platform.openai.com/settings/organization/limits","https://developers.openai.com/api/docs/supported-countries","https://developers.openai.com/cookbook/examples/how_to_handle_rate_limits","https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py","https://pypi.org/project/backoff/"]},{"url":"https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs-part-two","title":"How to Reduce 3rd Party API Costs: Part II - Lunar.dev","description":"Next you need to implement a rate limit for TPM, RPM, and RPD (in OpenAI) which means you need to understand both the rate limits in real-time and how close you ...","position":6,"markdown":"🙂\n\nMCP Risk Score Engine is available! [contact us for early (beta) access.](https://meetings-eu1.hubspot.com/eyal-solomon)\n\n🙂\n\n![How to Reduce 3rd Party API Costs: Part II ](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65cb8f03ff379afbc7553aec_1.webp)\n\n# How to Reduce 3rd Party API Costs: Part II\n\nWhile building middleware to control soaring costs and maintenance for OpenAI and such is tempting, it’s no easy feat. Since consumption is continuing to grow for scaling companies, our recommendation at the moment is to employ a dedicated infrastructure service.\n\n![Eyal Solomon, Co-Founder & CEO](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65c0db66ec6e719a385d88f1_eyal.png)\n\n### Eyal Solomon, Co-Founder & CEO\n\n#### October 1, 2023\n\n#### API Costs\n\nAt Lunar.dev our prime focus is efficient API [consumption management](https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs), and recently we’ve become acutely aware of the potential for massive overspends on OpenAI [API costs](https://www.thoughtspot.com/data-trends/ai/finops-for-llm).\n\nIn this post we’ll cover: Hidden Cloud Costs of Using [Open AI](https://openai.com/), Consumption Management Must-Haves like  Usage Visibility and API Consumption Controls and lastly, Optimisation Techniques, like Prompt Adaptation, LLM Cascade, and Caching (part of LLM Approximation).\n\nI will be focusing mostly on [OpenAI](https://openai.com/)’s [ChatGPT](https://chat.openai.com/), although the concepts I discuss will apply to other LLMs (large language models) too.\n\n‍\n\n## Hidden Cloud Costs of Using OpenAI\n\n‍\n\nGenerative AI is becoming the beating heart of development and RnD this year. Right now the biggest tech companies are collaborating with the biggest Gen AI providers. Microsoft has paired with [OpenAI](https://www.itpro.com/business/business-strategy/369850/microsofts-10b-openai-investment-could-end-ai-competition), Amazon announced this September (2023) that it will invest $4 billion in startup [Anthropic](https://techcrunch.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-ai-startup-anthropic/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAD1mZd8JNAiDmmKXGCOQiltRaO6oxJiCre6-pRqVwEQIpodg9KN0jnxmfxQ468Nt7n-el1xwtAS0nKo1tZcCvR0zgWrEw-dNf8GvhvrPbifzUE5oQ2ByyRHtEKZGovK0Liq7Xk26QYRzFqU_0qhw-wB5KdJYFZYqcEwVS1GSDezV), and Google has linked up with DeepMind. API calls to those Gen AI Cloud providers are expensive, so the companies that use them must look for ways to understand their costs so they can control them.\n\n‍\n\nWhile all these LLMs are similar, they’re also different in terms of the quality of their outputs and the value of their pricing, so if you’re using (or thinking of using) any of them in your production environment, you need to fully understand your API consumption and what it’s costing you.\n\n‍\n\n### A Maturing Market\n\n‍\n\nWhile [ChatGPT](https://chat.openai.com/) was once regarded as ‘nice-to-have’, its business model was not all that clear and many people just tinkered with it, but now it’s matured from a novelty into a pivotal tool for business. The biggest companies are consuming [ChatGPT](https://chat.openai.com/) (or other LLMs) in large volumes in production, and Open AI recently announced that the [enterprise-grade version of ChatGPT](https://openai.com/blog/introducing-chatgpt-enterprise) is on its way.\n\n‍\n\nThat’s great, but when every business is using the same game-changing technology, the success of one over another will come down to which of them can best control their costs and boost their efficiency.\n\n‍\n\nLet’s take a look at how costs can drastically affect outcomes in this hypothetical example. Let’s say we want Gen AI to summarise all of Wikipedia, preserving the meaning of each of its six million articles but reducing their word count from 750 to 375. Now let's say that each 750-word prompt (which is a standard ChatGPT benchmark) that we feed into each model will cost us 1000 tokens.\n\n‍\n\nHere’s how the costs would look for three different LLM models:\n\n‍\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/652bd8ef8bfc4089525d193c_3HGmWmU0TtqhmRhk4XpqqH0HleNi8Y1n0HE4n84lS2amjN-SSi3BaeicbL9mUUUsAqjFs2mcSQGJNTAQw271TclPhNMZS0Sdi92ZGqMQRiI-haKoizzQgUXaLjCjhDMs1pS_OT5R-I-ssO8Nj8TWY1A.png)\n\n‍\n\nCurie charges two dollars per million tokens, both for prompts and responses, so we’ll pay around $18,000 to summarize Wikipedia. [Ant](https://www.anthropic.com/) h [ropic](https://www.anthropic.com/) will cost us roughly $160,000, but the bill for [ChatGPT](https://chat.openai.com/) will dwarf them both at $360,000! These kinds of costs are just the tip of the iceberg in terms of data processing in today’s companies. When you consider that large and even medium-sized companies may routinely need to make even larger API requests, a 20x difference between two providers makes the need to control spending extremely urgent.\n\n‍\n\nTo do this you absolutely need a cost control strategy that provides visibility in real-time—because you can only control what you can see and understand. It should offer usage controls to impose boundaries on API calls and costs, and it should include optimization policies to get the best value out of every API call.\n\n‍\n\n## OpenAI Usage Visibility\n\n‍\n\n[OpenAI](https://openai.com/)’s management panel gives you some basic visibility that includes your monthly billing, and [Datadog](https://www.datadoghq.com/) [recently](https://www.datadoghq.com/blog/dash-2023-new-feature-roundup/) created its own dedicated dashboard for [OpenAI](https://openai.com/) (and you can [request access to the beta here](https://docs.google.com/forms/d/1uDlvDNCcrm8LB2yfl_qWUaWM87RTf4_Lj7i-RSKCBXM/viewform?edit_requested=true)), which includes things like real-time tracking of API calls and token requests. While this is an improvement, we believe that for the best visibility, you need to track [OpenAI](https://openai.com/)’s [three rate limits](https://platform.openai.com/docs/guides/rate-limits/overview) yourself, and they are:\n\n‍\n\n- RPD — requests per day\n- RPM — requests per minute\n- TPM — tokens per minute.\n\n‍\n\nYou should also track the different LLM models you’re using as well as response sizes since both add costs.\n\n‍\n\nDisplaying your own metrics for every API call and response on your own dedicated, real-time monitoring dashboard is the gold standard that will give you the necessary information to impose intelligent boundaries and so exercise greater cost control.\n\n‍\n\nWith any API call (and response) in [ChatGPT](https://chat.openai.com/) you can extract a ton of useful data from headers in real-time. Request limits, token limits, quotas remaining, completion tokens (for responses), and prompt tokens (for requests) are all available in real-time, giving you the ability to obtain highly detailed data about your consumption patterns.\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/66e1c2d9ea7bb519e2f6f5ec_65cb8f1b2bd24ad4fd2284fe_3%2520(1).png)\n\n‍\n\nAs a proof of concept for this, we at lunar.dev collaborated with the [AutoGPT](https://news.agpt.co/) community to measure the API’s real-time usage, byintegrating it with our [Egress API proxy](https://www.lunar.dev/product). As a result the Lunar.dev team discovered that with no (or poor performing) consumption limits in place on the client side the [ChatGPT](https://chat.openai.com/) autonomous agent made excessive API calls, and experienced exponential backoff. This resulted in excessive cost and poor performance and brought home to us the need to track calls using API consumption controls. It should come as no surprise that even in the official repository you will find warnings that ChatGPT is expensive and that you should monitor usage (see below).\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/652bd8f0659b8c3f189e8799_e78OzNM3rrb-2lD0QnTAIUmCQ6lqEK-2FO6ahHyuFspKBjYCDDSw_nA8XzoBVs4VRczWiC61VnG4SuHwyg2ayYlTUODE4aRjj-JZCKwOnKPj4grvYUwlcSkdCBGCOkBBBt6EX5qbK6Pspy3qJGp3yzc.png)\n\n‍\n\nWatch how this is done - the full demo by lunar.dev here:\n\n## Setting the Right Third-Party API Consumption Controls\n\n‍\n\n### Separate Usage Across Environments\n\nTo control consumption, you need to track API usage, but you also need to separate it out across environments. You can use the same API token across multiple environments, such as staging and production, but that can cause problems and exhaust that single resource. This is why it’s important to use a separate key for each environment.\n\n[ChatGPT](https://chat.openai.com/) has a memory, it stores your previous calls so it’s sometimes necessary to use the same API and functionality across different environments. This means you will need to share the same API key, and to do so you will need to generate sub-tokens from that API token, which is something the Lunar Proxy lets you do. Check out our quota allocation sandbox [here](https://docs.lunar.dev/0.10.x/additional-resources/lunar-sandbox) to see exactly how it’s done.\n\n‍\n\n#### Implement Rate Limits\n\nNext you need to implement a rate limit for TPM, RPM, and RPD (in [OpenAI](https://openai.com/)) which means you need to understand both the rate limits in real-time and how close you are to reaching them. Best practice would also recommend setting alerts and active controls to let you know when you reach them.\n\n‍\n\n#### Hard and Soft Limits\n\nA hard limit generates a 429 response, but a soft limit is more flexible. If you know the actual real-time rate limit, then you can set your own soft limits. OpenAI lets you do this from its control panel.\n\n‍\n\n#### Manage Retry Attempts\n\nIt is important to manage retries because even failed API calls will cost you and they can easily cascade.\n\n‍\n\n## API Consumption Optimisation Techniques for OpenAI\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/652bd8f03f9d613f3d5bae2d_TWTwwvi__8mof2k9fNMpPFPr5HAXosoGw_bJOPUxYMha0-LIK47Ul_QIgalyrgEUt3ym7VT9-loSWEk5fPCDT66mQJcfOv_UV-A345VC6ptJ1rF9cdo-6hLJVtbLzG-tB2yY_45CWwiP_-EgErYpMKU.png)\n\n‍\n\n#### Prompt Adaptation\n\nHere are some important points to remember:\n\n‍\n\n- Every prompt and every response costs you money.\n- The shorter the prompt, the fewer the tokens it uses and the less it will cost you.\n- The shorter the API response the less you will pay.\n\n‍\n\nPrompt Batching\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/66e1c2d9ea7bb519e2f6f5f2_65cb8f421781688ddca45815_6%2520(1).png)\n\n‍\n\nThis first prompt adaptation technique combines multiple inputs within a single call to reduce the frequency of API calls and make the best use of each token. Combining queries reduces the overhead in terms of metadata and the structure of the JSON, and also uses fewer tokens.\n\n‍\n\nPrompt Selection\n\nWith this approach your prompts include examples, usually in a prompt/answer format such as, “Here’s an example of the answer for X and Y, so now give me the answer for Z.” This is called [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot) and some LLM models provide tools to do this.\n\n‍\n\nDrop Unnecessary Words\n\nReduce your queries to the bare minimum, including unnecessary punctuation. There’s no need to be polite to an LLM and doing so will cost you more without improving the accuracy of the response. Reducing prompt tokens per API call results in reduced costs of between 30 and 50%.\n\n‍\n\n#### Model Chaining Optimizations\n\nAs the name suggests you can combine call batching, prompt optimization, and prompt selection, effectively chaining the logic before the API call is made. This is something that Lunar’s API Proxy lets you do, so you can chain and model all of those techniques before making an actual API call in the production environment.\n\n‍\n\n#### Optimization Completion\n\nAs well as optimizing the input you can do the same with the output, which you’ll recall also carries a cost to you in tokens and thus money. It’s possible to reduce the cost of responses without reducing their accuracy by limiting the maximum number of tokens to generate in the completion, and also the maximum number of completions to be generated for each prompt.\n\n‍\n\n#### LLM Cascading\n\nAs we saw at the beginning, LLMs can charge wildly different amounts for doing the same job. LLM cascading involves using the cheapest model for each task based on it being able to meet an accuracy threshold. If the cheapest one can do the job then that’s the one you use. If it can’t then the next least expensive one takes up the reins, and so on.\n\n![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/652bd8efc18971bacae33a34_tODxgLM58jnnHawoDkXfc2OSCp8XtVj_8wDzt-u6arllvJLdR6Ba5jkXjeGrKBiCPKBOsca70xosTkQvrnxaUc5Wxj_4tJq74QGxhcmSs1ojAg-O24M94xKnixTVqGaJ2z14cc4wvjT3em1evt2I-fg.png)\n\n‍\n\n#### Caching (LLM Approximation)\n\nThe idea here is that because online service users frequently access popular or trending content, cache systems can respond to this behavior by storing commonly accessed data.\n\nThis potentially gives you various queries and responses that you shouldn’t need to pay for more than once. If you can cache the response for the right query then you have no need to bother the API for an answer. This reduces outgoing API calls to LLMs so it has the potential to significantly reduce your costs.\n\n‍\n\nOne problem with this approach is that understanding the similarity between prompts is not always easy because LLM queries can be so complex and varied. Also, storage caches can become very large and then there’s the fact that caching is less effective for context-dependent outputs.\n\n‍\n\n## Conclusion\n\n‍\n\n- API consumption costs can and most probably will become a significant part of your cloud billing. It’s better to invest in active controls right now before it's too late.\n- After visibility, active controls are the next most important investment you can make to control your cloud spend.\n- In the next wave of GenAI, optimizing consumption is becoming a new frontier that needs to be addressed with cloud architecture adaptations.\n\n‍\n\nWhile building middleware to control soaring costs and maintenance for [OpenAI](https://openai.com/) and such is tempting, it’s no easy feat. Since consumption is continuing to grow for scaling companies, our recommendation at the moment is to employ a dedicated infrastructure service like the Lunar.dev Egress API Proxy to handle all the optimization policies that I mentioned. Since every API call is wired through the proxy, it’s only submitted once it’s been optimized. The API provider will only receive it in its most cost-effective and efficient form, or in some cases, it will be dealt with via a cached response – whichever is the most efficient!\n\n‍\n\nIntrigued? [Drop us an email](mailto:info@lunar.dev) and we’ll be happy to tell you more about how Lunar.dev can help you better manage your OpenAI API costs.\n\n‍\n\n## Ready to Start your journey?\n\nManage a single service and unlock API management at scale\n\n[Get Started](https://app.lunar.dev/)\n\n[![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/6966108847c39d21fac7d898_ChatGPT%20Image%20Jan%2012%2C%202026%2C%2008_01_47%20PM%20(1).png)](https://www.lunar.dev/post/mcp-prompts-at-runtime-how-agents-reason-execute-and-stay-accurate)\n\n[January 12, 2026\\\\\n**MCP Prompts at Runtime: How Agents Reason, Execute, and Stay Accurate**](https://www.lunar.dev/post/mcp-prompts-at-runtime-how-agents-reason-execute-and-stay-accurate)\n\n![Eyal Solomon, Co-Founder & CEO](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65c0db66ec6e719a385d88f1_eyal.png)\n\nEyal Solomon, Co-Founder & CEO\n\n[![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/6953904bd56f4e568e6ca6c3_cropped_404x233.png)](https://www.lunar.dev/post/emerging-patterns-and-practices-for-mcp-servers)\n\n[December 29, 2025\\\\\n**Emerging Patterns and Practices for MCP Servers**](https://www.lunar.dev/post/emerging-patterns-and-practices-for-mcp-servers)\n\n![Eyal Solomon, Co-Founder & CEO](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65c0db66ec6e719a385d88f1_eyal.png)\n\nEyal Solomon, Co-Founder & CEO\n\n[![](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/694bded284ac058cb444654b_ChatGPT%20Image%20Dec%2024%2C%202025%2C%2002_22_17%20PM-min%20(1).png)](https://www.lunar.dev/post/hibob-scales-ai-and-mcp-adoption-without-slowing-engineering)\n\n[December 24, 2025\\\\\n**HiBob scales AI and Model Context Protocol (MCP) adoption without slowing engineering**](https://www.lunar.dev/post/hibob-scales-ai-and-mcp-adoption-without-slowing-engineering)\n\n![Eyal Solomon, Co-Founder & CEO](https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65c0db66ec6e719a385d88f1_eyal.png)\n\nEyal Solomon, Co-Founder & CEO\n\n[![](https://cdn.prod.website-files.com/64953aa597d13b0acee8291d/65ad43c66d1cea5736731c02_exit.webp)](https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs-part-two#)\n\n## Get Early Access\n\nJoin the beta. Be the first to explore the free new features with lunar.dev's API Egress Proxy,","metadata":{"viewport":"width=device-width, initial-scale=1","ogTitle":"How to Reduce 3rd Party API Costs: Part II","twitter:image":"https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65cb8f03ff379afbc7553aec_1.webp","title":"How to Reduce 3rd Party API Costs: Part II","og:type":"website","language":"en","description":"Lunar.dev optimizes API usage, emphasizing cost control and effective OpenAI and LLM management. Read more here.","og:description":"Lunar.dev optimizes API usage, emphasizing cost control and effective OpenAI and LLM management. Read more here.","og:title":"How to Reduce 3rd Party API Costs: Part II","google-site-verification":"0YWxeaB77nGrRA3iutsg-jZqICG5vKaCEidAf3mRyZ4","twitter:card":"summary_large_image","ogDescription":"Lunar.dev optimizes API usage, emphasizing cost control and effective OpenAI and LLM management. Read more here.","twitter:title":"How to Reduce 3rd Party API Costs: Part II","og:image":"https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65cb8f03ff379afbc7553aec_1.webp","ogImage":"https://cdn.prod.website-files.com/649c330ec64b4a2c43b6fa63/65cb8f03ff379afbc7553aec_1.webp","twitter:description":"Lunar.dev optimizes API usage, emphasizing cost control and effective OpenAI and LLM management. Read more here.","favicon":"https://cdn.prod.website-files.com/64953aa597d13b0acee8291d/649c630b6ab492158a1914e5_Favicon.png","scrapeId":"019c62eb-509c-7409-9bdc-20ec70e4850a","sourceURL":"https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs-part-two","url":"https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs-part-two","statusCode":200,"contentType":"text/html; charset=utf-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"418a00a4-d1f8-4e82-ada1-babd735856d0","creditsUsed":1},"links":["https://meetings-eu1.hubspot.com/eyal-solomon","https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs","https://www.thoughtspot.com/data-trends/ai/finops-for-llm","https://openai.com/","https://chat.openai.com/","https://www.itpro.com/business/business-strategy/369850/microsofts-10b-openai-investment-could-end-ai-competition","https://techcrunch.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-ai-startup-anthropic/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAD1mZd8JNAiDmmKXGCOQiltRaO6oxJiCre6-pRqVwEQIpodg9KN0jnxmfxQ468Nt7n-el1xwtAS0nKo1tZcCvR0zgWrEw-dNf8GvhvrPbifzUE5oQ2ByyRHtEKZGovK0Liq7Xk26QYRzFqU_0qhw-wB5KdJYFZYqcEwVS1GSDezV","https://openai.com/blog/introducing-chatgpt-enterprise","https://www.anthropic.com/","https://www.datadoghq.com/","https://www.datadoghq.com/blog/dash-2023-new-feature-roundup/","https://docs.google.com/forms/d/1uDlvDNCcrm8LB2yfl_qWUaWM87RTf4_Lj7i-RSKCBXM/viewform?edit_requested=true","https://platform.openai.com/docs/guides/rate-limits/overview","https://news.agpt.co/","https://www.lunar.dev/product","https://docs.lunar.dev/0.10.x/additional-resources/lunar-sandbox","https://www.promptingguide.ai/techniques/fewshot","mailto:info@lunar.dev","https://app.lunar.dev/","https://www.lunar.dev/post/mcp-prompts-at-runtime-how-agents-reason-execute-and-stay-accurate","https://www.lunar.dev/post/emerging-patterns-and-practices-for-mcp-servers","https://www.lunar.dev/post/hibob-scales-ai-and-mcp-adoption-without-slowing-engineering","https://www.lunar.dev/post/how-to-reduce-3rd-party-api-costs-part-two#"]},{"url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","title":"Prompt caching with Azure OpenAI in Microsoft Foundry Models","description":"Prompt caching allows you to reduce overall request latency and cost for longer prompts that have identical content at the beginning of the ...","position":7,"markdown":"Table of contents Exit editor mode\n\nAsk LearnAsk LearnFocus mode\n\nTable of contents[Read in English](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic)Add to CollectionsAdd to plan[Edit](https://github.com/MicrosoftDocs/azure-ai-docs/blob/main/articles/ai-foundry/openai/how-to/prompt-caching.md)\n\n* * *\n\n#### Share via\n\n[Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dfacebook) [x.com](https://twitter.com/intent/tweet?original_referer=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dtwitter&tw_p=tweetbutton&url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dtwitter) [LinkedIn](https://www.linkedin.com/feed/?shareActive=true&text=%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dlinkedin) [Email](mailto:?subject=%5BShared%20Article%5D%20Prompt%20caching%20with%20Azure%20OpenAI%20in%20Microsoft%20Foundry%20Models%20-%20Azure%20OpenAI%20%7C%20Microsoft%20Learn&body=%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Demail)\n\n* * *\n\nCopy MarkdownPrint\n\n* * *\n\nNote\n\nAccess to this page requires authorization. You can try [signing in](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#) or changing directories.\n\n\nAccess to this page requires authorization. You can try changing directories.\n\n\n# Prompt caching\n\nFeedback\n\nSummarize this article for me\n\n\nPrompt caching allows you to reduce overall request latency and cost for longer prompts that have identical content at the beginning of the prompt. _\"Prompt\"_ in this context is referring to the input you send to the model as part of your chat completions request. Rather than reprocess the same input tokens over and over again, the service is able to retain a temporary cache of processed input token computations to improve overall performance. Prompt caching has no impact on the output content returned in the model response beyond a reduction in latency and cost. For supported models, cached tokens are billed at a [discount on input token pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) for Standard deployment types and up to [100% discount on input tokens](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/provisioned-throughput) for Provisioned deployment types.\n\nAzure AI Foundry Model prompt caches are cleared within 24 hours. Prompt caches aren't shared between Azure subscriptions.\n\n[Section titled: Supported models](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#supported-models)\n\n## Supported models\n\n- Prompt caching is supported with all Azure OpenAI models GPT-4o or newer.\n- Prompt caching applies to models that have chat-completion, completion, responses, or real-time operations. For models that don't have these operations, this feature isn't available.\n\n[Section titled: Getting started](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#getting-started)\n\n## Getting started\n\nTo take advantage of prompt caching, a request must meet both of these requirements:\n\n- A minimum of 1,024 tokens in length.\n- The first 1,024 tokens in the prompt must be identical.\n\nRequests are routed based on a hash of the initial prefix of a prompt. The hash typically uses the first 256 tokens, though the exact length varies depending on the model.\n\nWhen a match is found between the token computations in a prompt and the current content of the prompt cache, it's referred to as a cache hit. Cache hits will show up as [`cached_tokens`](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview#cached_tokens) under [`prompt_tokens_details`](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview#properties-for-prompt_tokens_details) in the chat completions response.\n\nJSONCopy\n\n```json\n{\n  \"created\": 1729227448,\n  \"model\": \"o1-2024-12-17\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": \"fp_50cdd5dc04\",\n  \"usage\": {\n    \"completion_tokens\": 1518,\n    \"prompt_tokens\": 1566,\n    \"total_tokens\": 3084,\n    \"completion_tokens_details\": {\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 576\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": 1408\n    }\n  }\n}\n```\n\nAfter the first 1,024 tokens cache hits will occur for every 128 additional identical tokens.\n\nA single character difference in the first 1,024 tokens will result in a cache miss which is characterized by a `cached_tokens` value of 0. Prompt caching is enabled by default with no additional configuration needed for supported models.\n\nIf you provide the `prompt_cache_key` parameter, it's combined with the prefix hash, allowing you to influence routing and improve cache hit rates. This is especially beneficial when many requests share long, common prefixes.\n\nIf requests for the same prefix and `prompt_cache_key` combination exceed a certain rate (approximately 15 requests per minute), some may overflow and get routed to additional machines, reducing cache effectiveness.\n\n[Section titled: What is cached?](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#what-is-cached)\n\n## What is cached?\n\nFeature support of o1-series models varies by model. For more information, see our dedicated [reasoning models guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning?view=foundry-classic).\n\nPrompt caching is supported for:\n\nExpand table\n\n| **Caching supported** | **Description** |\n| --- | --- |\n| **Messages** | The complete messages array: system, developer, user, and assistant content |\n| **Images** | Images included in user messages, both as links or as base64-encoded data. The detail parameter must be set the same across requests. |\n| **Tool use** | Both the messages array and tool definitions. |\n| **Structured outputs** | Structured output schema is appended as a prefix to the system message. |\n\nTo improve the likelihood of cache hits occurring, you should structure your requests such that repetitive content occurs at the beginning of the messages array.\n\n[Section titled: Can I disable prompt caching?](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#can-i-disable-prompt-caching)\n\n## Can I disable prompt caching?\n\nPrompt caching is enabled by default for all supported models. There's no opt-out support for prompt caching.\n\n**Note:** The author created this article with assistance from AI. [Learn more](https://learn.microsoft.com/principles-for-ai-generated-content)\n\n* * *\n\n## Feedback\n\nWas this page helpful?\n\n\nYesNoNo\n\nNeed help with this topic?\n\n\nWant to try using Ask Learn to clarify or guide you through this topic?\n\n\nAsk LearnAsk Learn\n\nSuggest a fix?\n\n* * *\n\n## Additional resources\n\nEvents\n\n\n[Agents League](https://aka.ms/agentsleague?wt.mc_id=agentsleague_learnpromo_1pevents_cxa_learncomm)\n\nFeb 16, 4 AM - Feb 27, 4 AM\n\n\nJoin the AI Agents Challenge from Feb 16-27. Build agents, watch live competitions, win prizes.\n\n\n[Learn more](https://aka.ms/agentsleague?wt.mc_id=agentsleague_learnpromo_1pevents_cxa_learncomm)\n\n* * *\n\n- Last updated on 01/12/2026\n\nAsk Learn is an AI assistant that can answer questions, clarify concepts, and define terms using trusted Microsoft documentation.\n\nPlease sign in to use Ask Learn.\n\n[Sign in](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#)","metadata":{"markdown_url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic&accept=text/markdown","feedback_help_link_type":"get-help-at-qna","og:url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","spProducts":["https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/0a5fc323-00ce-4c20-9095-41948f54c83f","https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/275ee921-eb79-47b7-891b-8d6470905e38"],"feedback_help_link_url":"https://learn.microsoft.com/answers/tags/133/azure","og:title":"Prompt caching with Azure OpenAI in Microsoft Foundry Models - Azure OpenAI","manager":"nitinme","asset_id":"ai-foundry/openai/how-to/prompt-caching","og:image":"https://learn.microsoft.com/en-us/media/open-graph-image.png","gitcommit":"https://github.com/MicrosoftDocs/azure-ai-docs-pr/blob/a7add8922a4225fe3c7ea7dc1f8fb81bb830330d/articles/ai-foundry/openai/how-to/prompt-caching.md","language":"en-us","moniker_range_name":"80e86b031682a59a12f884a1fb748bff","locale":"en-us","title":"Prompt caching with Azure OpenAI in Microsoft Foundry Models - Azure OpenAI | Microsoft Learn","feedback_product_url":"https://feedback.azure.com/d365community/forum/79b1327d-d925-ec11-b6e6-000d3a4f06a4","word_count":"569","document_version_independent_id":"ba016f1f-6fbe-6aec-2d76-16081982835a","ms.topic":"how-to","author":"mrbullwinkle","toc_rel":"../../toc.json","og:image:alt":"Microsoft Learn","twitter:card":"summary_large_image","git_commit_id":"a7add8922a4225fe3c7ea7dc1f8fb81bb830330d","schema":"Conceptual","ogDescription":"Learn how to use prompt caching with Azure OpenAI","learn_banner_products":"azure","ms.update-cycle":"90-days","item_type":"Content","previous_tlsh_hash":"A9710961850D4610FBC14B248DD6EC416DF5805BFAB4EEE420217CA1D90E3E234B5257EA6776A7A13B255A830292778F12E66F6CE0F833229151748C85DCAF86561A3B76E2","updated_at":"2026-02-11T06:03:00Z","ogTitle":"Prompt caching with Azure OpenAI in Microsoft Foundry Models - Azure OpenAI","ms.collection":"ce-skilling-ai-copilot, ce-skilling-fresh-tier0","og:description":"Learn how to use prompt caching with Azure OpenAI","feedback_system":"Standard","original_content_git_url":"https://github.com/MicrosoftDocs/azure-ai-docs-pr/blob/live/articles/ai-foundry/openai/how-to/prompt-caching.md","recommendation_types":["Training","Certification"],"ms.subservice":"azure-ai-foundry-openai","ogImage":"https://learn.microsoft.com/en-us/media/open-graph-image.png","default_moniker":"foundry-classic","ms.author":"mbullwin","github_feedback_content_git_url":"https://github.com/MicrosoftDocs/azure-ai-docs/blob/main/articles/ai-foundry/openai/how-to/prompt-caching.md","adobe-target":"true","ms.suite":"office","ogUrl":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","uhfHeaderId":"azure-ai-foundry","services":"cognitive-services","document_id":"638c1b7f-6e16-2998-9ad1-9184267bf385","source_path":"articles/ai-foundry/openai/how-to/prompt-caching.md","site_name":"Docs","page_type":"conceptual","permissioned-type":"public","monikerRange":"foundry-classic || foundry","color-scheme":"light dark","recommendations":"false","depot_name":"Learn.azure-ai","ms.service":"azure-ai-foundry","platform_id":"052b3d73-75ff-42ad-3168-0a449f457af5","description":"Learn how to use prompt caching with Azure OpenAI","twitter:site":"@MicrosoftLearn","ai-usage":"ai-assisted","monikers":["foundry","foundry-classic"],"og:type":"website","config_moniker_range":"foundry-classic || foundry","scope":"Azure","viewport":"width=device-width, initial-scale=1.0","cmProducts":["https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/8a6e4dad-7050-4ce7-83f9-eb4123577a54","https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/4068364a-4467-4da8-9afb-a1f7187827fd"],"breadcrumb_path":"../../../breadcrumb/azure-ai/toc.json","ms.date":"2025-11-08T00:00:00Z","scrapeId":"019c62eb-509c-7409-9bdc-24d37ad45918","sourceURL":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","statusCode":200,"contentType":"text/html","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"825bbd1d-a985-46b6-9f68-fff8e370615c","creditsUsed":1},"links":["https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic","https://github.com/MicrosoftDocs/azure-ai-docs/blob/main/articles/ai-foundry/openai/how-to/prompt-caching.md","https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dfacebook","https://twitter.com/intent/tweet?original_referer=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dtwitter&tw_p=tweetbutton&url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dtwitter","https://www.linkedin.com/feed/?shareActive=true&text=%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Dlinkedin","mailto:?subject=%5BShared%20Article%5D%20Prompt%20caching%20with%20Azure%20OpenAI%20in%20Microsoft%20Foundry%20Models%20-%20Azure%20OpenAI%20%7C%20Microsoft%20Learn&body=%0A%0D%0Ahttps%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fopenai%2Fhow-to%2Fprompt-caching%3Fview%3Dfoundry-classic%26WT.mc_id%3Demail","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#","https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/provisioned-throughput","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#supported-models","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#getting-started","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview#cached_tokens","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview#properties-for-prompt_tokens_details","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#what-is-cached","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning?view=foundry-classic","https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/prompt-caching?view=foundry-classic#can-i-disable-prompt-caching","https://learn.microsoft.com/principles-for-ai-generated-content","https://aka.ms/agentsleague?wt.mc_id=agentsleague_learnpromo_1pevents_cxa_learncomm"]},{"url":"https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/","title":"OpenAI Prompt Cache Monitoring | Towards Data Science","description":"API calls to supported models will automatically benefit from Prompt Caching on prompts longer than 1,024 tokens. The API caches the longest ...","position":8,"markdown":"![Revisit consent button](https://cdn-cookieyes.com/assets/images/revisit.svg)\n\nWe value your privacy\n\nWe use cookies to enhance your browsing experience, serve personalised ads or content, and analyse our traffic. By clicking \"Accept All\", you consent to our use of cookies.\n\nCustomiseReject AllAccept All\n\nPowered by [Visit CookieYes website](https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=fl-branding)\n\nCustomise Consent Preferences![](https://cdn-cookieyes.com/assets/images/close.svg)\n\nWe use cookies to help you navigate efficiently and perform certain functions. You will find detailed information about all cookies under each consent category below.\n\nThe cookies that are categorised as \"Necessary\" are stored on your browser as they are essential for enabling the basic functionalities of the site. ... Show more\n\nNecessaryAlways Active\n\nNecessary cookies are required to enable the basic features of this site, such as providing secure log-in or adjusting your consent preferences. These cookies do not store any personally identifiable data.\n\n- Cookie\n\n\\_\\_cf\\_bm\n\n- Duration\n\n1 hour\n\n- Description\n\nThis cookie, set by Cloudflare, is used to support Cloudflare Bot Management.\n\n\n- Cookie\n\nAWSALBCORS\n\n- Duration\n\n7 days\n\n- Description\n\nAmazon Web Services set this cookie for load balancing.\n\n\n- Cookie\n\n\\_cfuvid\n\n- Duration\n\nsession\n\n- Description\n\nCloudflare sets this cookie to track users across sessions to optimize user experience by maintaining session consistency and providing personalized services\n\n\n- Cookie\n\nli\\_gc\n\n- Duration\n\n6 months\n\n- Description\n\nLinkedin set this cookie for storing visitor's consent regarding using cookies for non-essential purposes.\n\n\n- Cookie\n\n\\_\\_hssrc\n\n- Duration\n\nsession\n\n- Description\n\nThis cookie is set by Hubspot whenever it changes the session cookie. The \\_\\_hssrc cookie set to 1 indicates that the user has restarted the browser, and if the cookie does not exist, it is assumed to be a new session.\n\n\n- Cookie\n\n\\_\\_hssc\n\n- Duration\n\n1 hour\n\n- Description\n\nHubSpot sets this cookie to keep track of sessions and to determine if HubSpot should increment the session number and timestamps in the \\_\\_hstc cookie.\n\n\n- Cookie\n\nwpEmojiSettingsSupports\n\n- Duration\n\nsession\n\n- Description\n\nWordPress sets this cookie when a user interacts with emojis on a WordPress site. It helps determine if the user's browser can display emojis properly.\n\n\n- Cookie\n\n\\_octo\n\n- Duration\n\n1 year\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\nlogged\\_in\n\n- Duration\n\n1 year\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\n\\_\\_Secure-YEC\n\n- Duration\n\npast\n\n- Description\n\nYouTube sets this cookie to stores the user's video player preferences using embedded YouTube video\n\n\n- Cookie\n\n\\_\\_eoi\n\n- Duration\n\n6 months\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nAWSALBTGCORS\n\n- Duration\n\n7 days\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\nlogin-status-p\n\n- Duration\n\npast\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nAWSALBTG\n\n- Duration\n\n7 days\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\ncsrf\\_token\n\n- Duration\n\nsession\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\ntoken\\_v2\n\n- Duration\n\n1 day\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nD\n\n- Duration\n\n1 year\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nPHPSESSID\n\n- Duration\n\nsession\n\n- Description\n\nThis cookie is native to PHP applications. The cookie stores and identifies a user's unique session ID to manage user sessions on the website. The cookie is a session cookie and will be deleted when all the browser windows are closed.\n\n\n- Cookie\n\nVISITOR\\_PRIVACY\\_METADATA\n\n- Duration\n\n6 months\n\n- Description\n\nYouTube sets this cookie to store the user's cookie consent state for the current domain.\n\n\n- Cookie\n\ncookietest\n\n- Duration\n\nsession\n\n- Description\n\nThe cookietest cookie is typically used to determine whether the user's browser accepts cookies, essential for website functionality and user experience.\n\n\n- Cookie\n\n\\_\\_Host-airtable-session\n\n- Duration\n\n1 year\n\n- Description\n\nThis cookie is used to enable us to integrate the services of Airtable.\n\n\n- Cookie\n\n\\_\\_Host-airtable-session.sig\n\n- Duration\n\n1 year\n\n- Description\n\nThis cookie is used to enable us to integrate the services of Airtable.\n\n\n- Cookie\n\nm\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nStripe sets this cookie for fraud prevention purposes. It identifies the device used to access the website, allowing the website to be formatted accordingly.\n\n\n- Cookie\n\nBIGipServer\\*\n\n- Duration\n\nsession\n\n- Description\n\nMarketo sets this cookie to collect information about the user's online activity and build a profile about their interests to provide advertisements relevant to the user.\n\n\n- Cookie\n\n\\_\\_cfruid\n\n- Duration\n\nsession\n\n- Description\n\nCloudflare sets this cookie to identify trusted web traffic.\n\n\n- Cookie\n\n\\_GRECAPTCHA\n\n- Duration\n\n6 months\n\n- Description\n\nGoogle Recaptcha service sets this cookie to identify bots to protect the website against malicious spam attacks.\n\n\n- Cookie\n\n\\_\\_Secure-YNID\n\n- Duration\n\n6 months\n\n- Description\n\nGoogle cookie used to protect user security and prevent fraud, especially during the login process.\n\n\n- Cookie\n\ncookieyes-consent\n\n- Duration\n\n1 year\n\n- Description\n\nCookieYes sets this cookie to remember users' consent preferences so that their preferences are respected on subsequent visits to this site. It does not collect or store any personal information about the site visitors.\n\n\nFunctional\n\nFunctional cookies help perform certain functionalities like sharing the content of the website on social media platforms, collecting feedback, and other third-party features.\n\n- Cookie\n\nBCTempID\n\n- Duration\n\n10 minutes\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\nBCSessionID\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nBlueconic sets this cookie as a unique identifier for the BlueConic profile.\n\n\n- Cookie\n\nlidc\n\n- Duration\n\n1 day\n\n- Description\n\nLinkedIn sets the lidc cookie to facilitate data center selection.\n\n\n- Cookie\n\nbrw\n\n- Duration\n\n1 year\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\nbrwConsent\n\n- Duration\n\n5 minutes\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nWMF-Uniq\n\n- Duration\n\n1 year\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nloom\\_anon\\_comment\n\n- Duration\n\n1 year\n\n- Description\n\nNo description available.\n\n\n- Cookie\n\nloom\\_referral\\_video\n\n- Duration\n\nsession\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nVISITOR\\_INFO1\\_LIVE\n\n- Duration\n\n6 months\n\n- Description\n\nA cookie set by YouTube to measure bandwidth that determines whether the user gets the new or old player interface.\n\n\n- Cookie\n\nyt-remote-connected-devices\n\n- Duration\n\nNever Expires\n\n- Description\n\nYouTube sets this cookie to store the user's video preferences using embedded YouTube videos.\n\n\n- Cookie\n\nytidb::LAST\\_RESULT\\_ENTRY\\_KEY\n\n- Duration\n\nNever Expires\n\n- Description\n\nThe cookie ytidb::LAST\\_RESULT\\_ENTRY\\_KEY is used by YouTube to store the last search result entry that was clicked by the user. This information is used to improve the user experience by providing more relevant search results in the future.\n\n\n- Cookie\n\nyt-remote-device-id\n\n- Duration\n\nNever Expires\n\n- Description\n\nYouTube sets this cookie to store the user's video preferences using embedded YouTube videos.\n\n\n- Cookie\n\nyt-remote-session-name\n\n- Duration\n\nsession\n\n- Description\n\nThe yt-remote-session-name cookie is used by YouTube to store the user's video player preferences using embedded YouTube video.\n\n\n- Cookie\n\nyt-remote-fast-check-period\n\n- Duration\n\nsession\n\n- Description\n\nThe yt-remote-fast-check-period cookie is used by YouTube to store the user's video player preferences for embedded YouTube videos.\n\n\n- Cookie\n\nyt-remote-session-app\n\n- Duration\n\nsession\n\n- Description\n\nThe yt-remote-session-app cookie is used by YouTube to store user preferences and information about the interface of the embedded YouTube video player.\n\n\n- Cookie\n\nyt-remote-cast-available\n\n- Duration\n\nsession\n\n- Description\n\nThe yt-remote-cast-available cookie is used to store the user's preferences regarding whether casting is available on their YouTube video player.\n\n\n- Cookie\n\nyt-remote-cast-installed\n\n- Duration\n\nsession\n\n- Description\n\nThe yt-remote-cast-installed cookie is used to store the user's video player preferences using embedded YouTube video.\n\n\n- Cookie\n\ncp\\_session\n\n- Duration\n\n3 months\n\n- Description\n\nCodepen sets this cookie for Help systems found in the website.\n\n\n- Cookie\n\nloid\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nThis cookie is set by the Reddit. The cookie enables the sharing of content from the website onto the social media platform.\n\n\nAnalytics\n\nAnalytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics such as the number of visitors, bounce rate, traffic source, etc.\n\n- Cookie\n\n\\_\\_hstc\n\n- Duration\n\n6 months\n\n- Description\n\nHubspot set this main cookie for tracking visitors. It contains the domain, initial timestamp (first visit), last timestamp (last visit), current timestamp (this visit), and session number (increments for each subsequent session).\n\n\n- Cookie\n\nhubspotutk\n\n- Duration\n\n6 months\n\n- Description\n\nHubSpot sets this cookie to keep track of the visitors to the website. This cookie is passed to HubSpot on form submission and used when deduplicating contacts.\n\n\n- Cookie\n\n\\_ga\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nGoogle Analytics sets this cookie to calculate visitor, session and campaign data and track site usage for the site's analytics report. The cookie stores information anonymously and assigns a randomly generated number to recognise unique visitors.\n\n\n- Cookie\n\n\\_ga\\_\\*\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nGoogle Analytics sets this cookie to store and count page views.\n\n\n- Cookie\n\n\\_\\_Host-psifi.analyticsTrace\n\n- Duration\n\n6 hours\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\n\\_\\_Host-psifi.analyticsTraceV2\n\n- Duration\n\n6 hours\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\n\\_gh\\_sess\n\n- Duration\n\nsession\n\n- Description\n\nGitHub sets this cookie for temporary application and framework state between pages like what step the user is on in a multiple step form.\n\n\n- Cookie\n\nYSC\n\n- Duration\n\nsession\n\n- Description\n\nYSC cookie is set by Youtube and is used to track the views of embedded videos on Youtube pages.\n\n\n- Cookie\n\najs\\_anonymous\\_id\n\n- Duration\n\n1 year\n\n- Description\n\nThis cookie is set by Segment to count the number of people who visit a certain site by tracking if they have visited before.\n\n\n- Cookie\n\nvuid\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nVimeo installs this cookie to collect tracking information by setting a unique ID to embed videos on the website.\n\n\nPerformance\n\nPerformance cookies are used to understand and analyse the key performance indexes of the website which helps in delivering a better user experience for the visitors.\n\n- Cookie\n\nAWSALB\n\n- Duration\n\n7 days\n\n- Description\n\nAWSALB is an application load balancer cookie set by Amazon Web Services to map the session to the target.\n\n\n- Cookie\n\nacq\n\n- Duration\n\npast\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nacq.sig\n\n- Duration\n\npast\n\n- Description\n\nDescription is currently not available.\n\n\n- Cookie\n\nptc\n\n- Duration\n\n2 years\n\n- Description\n\nNo description available.\n\n\nAdvertisement\n\nAdvertisement cookies are used to provide visitors with customised advertisements based on the pages you visited previously and to analyse the effectiveness of the ad campaigns.\n\n- Cookie\n\nmuc\\_ads\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nTwitter sets this cookie to collect user behaviour and interaction data to optimize the website.\n\n\n- Cookie\n\nguest\\_id\\_marketing\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nTwitter sets this cookie to identify and track the website visitor.\n\n\n- Cookie\n\nguest\\_id\\_ads\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nTwitter sets this cookie to identify and track the website visitor.\n\n\n- Cookie\n\npersonalization\\_id\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nTwitter sets this cookie to integrate and share features for social media and also store information about how the user uses the website, for tracking and targeting.\n\n\n- Cookie\n\nguest\\_id\n\n- Duration\n\n1 year 1 month 4 days\n\n- Description\n\nTwitter sets this cookie to identify and track the website visitor. It registers if a user is signed in to the Twitter platform and collects information about ad preferences.\n\n\n- Cookie\n\nbcookie\n\n- Duration\n\n1 year\n\n- Description\n\nLinkedIn sets this cookie from LinkedIn share buttons and ad tags to recognize browser IDs.\n\n\n- Cookie\n\n\\_\\_Secure-ROLLOUT\\_TOKEN\n\n- Duration\n\n6 months\n\n- Description\n\nYouTube sets this cookie to manage feature rollout and experimentation. It helps Google control which new features or interface changes are shown to users as part of testing and staged rollouts, ensuring consistent experience for a given user during an experiment.\n\n\n- Cookie\n\nyt.innertube::nextId\n\n- Duration\n\nNever Expires\n\n- Description\n\nYouTube sets this cookie to register a unique ID to store data on what videos from YouTube the user has seen.\n\n\n- Cookie\n\nyt.innertube::requests\n\n- Duration\n\nNever Expires\n\n- Description\n\nYouTube sets this cookie to register a unique ID to store data on what videos from YouTube the user has seen.\n\n\n- Cookie\n\nsession\\_tracker\n\n- Duration\n\nsession\n\n- Description\n\nThis cookie is set by the Reddit. This cookie is used to identify trusted web traffic. It also helps in adverstising on the website.\n\n\n- Cookie\n\nedgebucket\n\n- Duration\n\nsession\n\n- Description\n\nReddit sets this cookie to save the information about a log-on Reddit user, for the purpose of advertisement recommendations and updating the content.\n\n\n- Cookie\n\ndid\n\n- Duration\n\n1 year\n\n- Description\n\nArbor sets this cookie to show targeted ads to site visitors.This cookie expires after 2 months or 1 year.\n\n\nUncategorised\n\nOther uncategorised cookies are those that are being analysed and have not been classified into a category as yet.\n\nNo cookies to display.\n\nReject AllSave My PreferencesAccept All\n\nPowered by [Visit CookieYes website](https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=sl-branding)\n\n[Skip to content](https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/#wp--skip-link--target)\n\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n# OpenAI Prompt Cache Monitoring\n\nA worked example using Python and the chat completion API\n\n[Thomas Reid](https://towardsdatascience.com/author/thomas_reid/)\n\nDec 10, 2024\n\n10 min read\n\nShare\n\n![Image by AI (Dalle-3)](https://towardsdatascience.com/wp-content/uploads/2024/12/0ygrvmwtWhGnpxiJ4.jpg)Image by AI (Dalle-3)\n\nAs part of their recent DEV Day presentation, OpenAI announced that Prompt Caching was now available for various models. At the time of writing, those models were:-\n\n> GPT-4o, GPT-4o mini, o1-preview and o1-mini, as well as fine-tuned versions of those models.\n\nThis news shouldn’t be underestimated, as it will allow developers to save on costs and reduce application runtime latency.\n\nAPI calls to supported models will automatically benefit from Prompt Caching on prompts longer than 1,024 tokens. The API caches the longest prefix of a prompt that has been previously computed, starting at 1,024 tokens and increasing in 128-token increments. If you reuse prompts with common prefixes, OpenAI will automatically apply the Prompt Caching discount without requiring you to change your API integration.\n\nAs an OpenAI API developer, the only thing you may have to worry about is how to monitor your Prompt Caching use, i.e. check that it’s being applied.\n\nIn this article, I’ll show you how to do that using Python, a Jupyter Notebook and a chat completion example.\n\n### Install WSL2 Ubuntu\n\nI’m on Windows, but I’ll run my example code under WSL2 Ubuntu. Check out the link below for a comprehensive guide on installing WSL2 for Windows.\n\n> [**Installing WSL2 Ubuntu for Windows**](https://blog.stackademic.com/installing-wsl2-ubuntu-for-windows-81122c551bc2)\n\n### Setting up our development environment\n\nBefore developing like this, I always create a separate Python development environment where I can install any software needed and experiment with coding. Now, anything I do in this environment will be siloed and won’t impact my other projects.\n\nI use Miniconda for this, but there are many other ways to do it, so use whatever method you know best.\n\nIf you want to go down the Miniconda route and don’t already have it, you must install Miniconda first. Get it using this link,\n\n> [**Miniconda – Anaconda documentation**](https://docs.anaconda.com/miniconda)\n\nTo follow along with my example, you’ll need an OpenAI API key. Create an OpenAI account if you don’t already have one, then you can get a key from the OpenAI platform using the link below:\n\n[https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n\n**1/ Create our new dev environment and install the required libraries**\n\n```ruby\n(base) $ conda create -n oai_test python=3.11 -y\n(base) $ conda activate oai_test\n(oai_test) $ pip install openai --upgrade\n(oai_test) $ pip install jupyter\n```\n\n**2/ Start Jupyter**\n\nNow type in `jupyter notebook` into your command prompt. You should see a jupyter notebook open in your browser. If that doesn’t happen automatically, you’ll likely see a screenful of information after the `jupyter notebook` command. Near the bottom, there will be a URL that you should copy and paste into your browser to initiate the Jupyter Notebook.\n\nYour URL will be different to mine, but it should look something like this:-\n\n```bash\nhttp://127.0.0.1:8888/tree?token=3b9f7bd07b6966b41b68e2350721b2d0b6f388d248cc69\n```\n\n### The code\n\nPrompt caching is automatic so you don’t have to change your existing code base. But recall that it only kicks in when the combined system and user prompt are > 1024 tokens.\n\nOpenAI recommends structuring your prompts so that any static information is at the beginning and dynamic content towards the end. This ties in nicely with the static data being in the system prompt and the dynamic data in the user prompt. You don’t _**have**_ to do this, but it makes the most sense to do so.\n\nSo, let’s put all this together by showing a hypothetical example grounded in a real-use case study. In our hypothetical scenario, we’ll model a smart home system where you can remotely request actions to be taken in or around your home. For example, you might like your smart home system to turn on your lights, heating system, etc…. when you’re away from your house.\n\nOur code consists of two tools (functions) that the LLM can use. One does the actual switching on/off of a control device, and the other can do so in response to a timed event.\n\nAfter that, we have our system prompt, which clearly defines what the smart home system should be capable of and any rules/guidance it needs to perform its function.\n\nAdditionally, we have, in the first instance, a simple user prompt that requests the control system to turn on the house lights. We run this initial command and get a count of the total tokens in the prompts, the number of cached tokens and a few other data points.\n\nAfter this initial run, we ask the control system to perform a different task, and once again, we get various token counts for that operation.\n\n```swift\nfrom openai import OpenAI\nimport os\nimport json\nimport time\n\napi_key = \"YOUR_API_KEY_GOES_HERE\"\nclient = OpenAI( api_key=api_key)\n\n# Define tools (functions)\ntools = [\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"control_device\",\\\n            \"description\": \"Control a smart home device, such as turning it on/off or changing settings.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"device_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The unique identifier of the device to control.\"\\\n                    },\\\n                    \"action\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The action to perform (e.g., 'turn_on', 'turn_off', 'set_temperature').\"\\\n                    },\\\n                    \"value\": {\\\n                        \"type\": [\"string\", \"number\"],\\\n                        \"description\": \"Optional value for the action, such as temperature setting.\"\\\n                    }\\\n                },\\\n                \"required\": [\"device_id\", \"action\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    },\\\n    {\\\n        \"type\": \"function\",\\\n        \"function\": {\\\n            \"name\": \"set_schedule\",\\\n            \"description\": \"Set a schedule for a smart home device to perform an action at a specified time.\",\\\n            \"parameters\": {\\\n                \"type\": \"object\",\\\n                \"properties\": {\\\n                    \"device_id\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The unique identifier of the device to schedule.\"\\\n                    },\\\n                    \"action\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The action to perform (e.g., 'turn_on', 'turn_off').\"\\\n                    },\\\n                    \"schedule_time\": {\\\n                        \"type\": \"string\",\\\n                        \"description\": \"The time to perform the action, in ISO 8601 format or a natural language description.\"\\\n                    }\\\n                },\\\n                \"required\": [\"device_id\", \"action\", \"schedule_time\"],\\\n                \"additionalProperties\": False\\\n            }\\\n        }\\\n    }\\\n]\n\n# System message with guidelines\n# Expanded system message to exceed 1024 tokens\n# to make sure prompt caching enabled\nmessages = [\\\n    {\\\n        \"role\": \"system\",\\\n        \"content\": (\\\n            \"You are a smart home assistant that helps users control their smart home devices securely and efficiently. \"\\\n            \"Your goals are to execute user commands, provide device statuses, and manage schedules while ensuring safety and privacy. \"\\\n            \"Always confirm actions with the user before executing them, especially for critical devices like security systems or door locks. \"\\\n            \"Maintain a friendly and professional tone, adapting to the user's level of technical expertise.nn\"\\\n            # Begin expansion\\\n            \"Important guidelines to follow:nn\"\\\n            \"1. **User Privacy and Security**: Handle all personal and device information confidentially. \"\\\n            \"Verify the user's identity if necessary before performing sensitive actions. Never share personal data with unauthorized parties. \"\\\n            \"Ensure that all communications comply with data protection laws and regulations.nn\"\\\n            \"2. **Confirmation Before Actions**: Always confirm the user's intent before executing actions that affect their devices. \"\\\n            \"For example, if a user asks to unlock the front door, verify their identity and confirm the action to prevent unauthorized access.nn\"\\\n            \"3. **Error Handling**: If an action cannot be completed, politely inform the user and suggest alternative solutions. \"\\\n            \"Provide clear explanations for any issues, and guide the user through troubleshooting steps if appropriate.nn\"\\\n            \"4. **Safety Measures**: Ensure that commands do not compromise safety. \"\\\n            \"Avoid setting temperatures beyond safe limits, and alert the user if a requested action might be unsafe. \"\\\n            \"For instance, if the user tries to turn off security cameras, remind them of potential security risks.nn\"\\\n            \"5. **No Unauthorized Access**: Do not control devices without explicit user permission. \"\\\n            \"Ensure that any scheduled tasks or automated routines are clearly communicated and approved by the user.nn\"\\\n            \"6. **Clear Communication**: Use simple language and avoid technical jargon unless the user is familiar with it. \"\\\n            \"Explain any technical terms if necessary, and ensure that instructions are easy to understand.nn\"\\\n            \"7. **Compliance**: Adhere to all relevant laws, regulations, and company policies regarding smart home operations. \"\\\n            \"Stay updated on changes to regulations that may affect how devices should be controlled or monitored.nn\"\\\n            \"8. **Accurate Information**: Provide precise device statuses and avoid speculation. \"\\\n            \"If unsure about a device's status, inform the user and suggest ways to verify or troubleshoot the issue.nn\"\\\n            \"9. **Accessibility Considerations**: Be mindful of users with disabilities. \"\\\n            \"Ensure that instructions and responses are accessible, and offer alternative interaction methods if needed.nn\"\\\n            \"10. **Personalization**: Adapt to the user's preferences and prior interactions. \"\\\n            \"Remember frequent commands and offer suggestions based on usage patterns, while respecting privacy settings.nn\"\\\n            \"11. **Timeouts and Idle States**: If a session is idle for a prolonged period, securely end the session to protect user data. \"\\\n            \"Notify the user when the session is about to expire and provide options to extend it if necessary.nn\"\\\n            \"12. **Multi-User Environments**: Recognize when multiple users may be interacting with the system. \"\\\n            \"Manage profiles separately to ensure personalized experiences and maintain privacy between users.nn\"\\\n            \"13. **Energy Efficiency**: Promote energy-saving practices. \"\\\n            \"If a user forgets to turn off devices, gently remind them or offer to automate energy-saving routines.nn\"\\\n            \"14. **Emergency Protocols**: Be prepared to assist during emergencies. \"\\\n            \"Provide quick access to emergency services if requested, and understand basic protocols for common emergencies.nn\"\\\n            \"15. **Continuous Learning**: Stay updated with the latest device integrations and features. \"\\\n            \"Inform users about new capabilities that may enhance their smart home experience.nn\"\\\n            \"16. **Language and Cultural Sensitivity**: Be aware of cultural differences and language preferences. \"\\\n            \"Support multiple languages if possible and be sensitive to cultural norms in communication.nn\"\\\n            \"17. **Proactive Assistance**: Anticipate user needs by offering helpful suggestions. \"\\\n            \"For example, if the weather forecast indicates rain, suggest closing windows or adjusting irrigation systems.nn\"\\\n            \"18. **Logging and Monitoring**: Keep accurate logs of actions taken, while ensuring compliance with privacy policies. \"\\\n            \"Use logs to help troubleshoot issues but never share log details with unauthorized parties.nn\"\\\n            \"19. **Third-Party Integrations**: When interacting with third-party services, ensure secure connections and compliance with their terms of service. \"\\\n            \"Inform users when third-party services are involved.nn\"\\\n            \"20. **Disaster Recovery**: In case of system failures, have protocols in place to restore functionality quickly. \"\\\n            \"Keep the user informed about outages and provide estimated resolution times.nn\"\\\n        )\\\n    },\\\n    {\\\n        \"role\": \"user\",\\\n        \"content\": \"Hi, could you please turn on the living room lights?\"\\\n    }\\\n]\n# Function to run completion with the provided message history and tools\ndef completion_run(messages, tools):\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        tools=tools,\n        messages=messages,\n        tool_choice=\"required\"\n    )\n    usage_data = json.dumps(completion.to_dict(), indent=4)\n    return usage_data\n\n# Main function to handle the runs\ndef main(messages, tools):\n    # Run 1: Initial query\n    print(\"Run 1:\")\n    run1 = completion_run(messages, tools)\n    print(run1)\n\n    # Delay for 3 seconds\n    time.sleep(3)\n\n    # Append user_query2 to the message history\n    user_query2 = {\n        \"role\": \"user\",\n        \"content\": \"Actually, could you set the thermostat to 72 degrees at 6 PM every day?\"\n    }\n    messages.append(user_query2)\n\n    # Run 2: With appended query\n    print(\"nRun 2:\")\n    run2 = completion_run(messages, tools)\n    print(run2)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main(messages, tools)\n```\n\nAnd our output is:-\n\n```swift\nRun 1:\n{\n    \"id\": \"chatcmpl-AFePFIyWQtNJ4txIGcLbXZaZleEZv\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": null,\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\",\\\n                \"tool_calls\": [\\\n                    {\\\n                        \"id\": \"call_m4V9sn2PY7X3EapH7ph1K8t9\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\"device_id\":\"living_room_lights\",\"action\":\"turn_on\"}\",\\\n                            \"name\": \"control_device\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    }\\\n                ]\\\n            }\\\n        }\\\n    ],\n    \"created\": 1728293605,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_f85bea6784\",\n    \"usage\": {\n        \"completion_tokens\": 21,\n        \"prompt_tokens\": 1070,\n        \"total_tokens\": 1091,\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        },\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 0\n        }\n    }\n}\n\nRun 2:\n{\n    \"id\": \"chatcmpl-AFePJwIczKSjJnvwed7wpyRI7gLWU\",\n    \"choices\": [\\\n        {\\\n            \"finish_reason\": \"stop\",\\\n            \"index\": 0,\\\n            \"logprobs\": null,\\\n            \"message\": {\\\n                \"content\": null,\\\n                \"refusal\": null,\\\n                \"role\": \"assistant\",\\\n                \"tool_calls\": [\\\n                    {\\\n                        \"id\": \"call_PjCse4kD4QJxYcFuZ7KlqJAc\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\"device_id\": \"living_room_lights\", \"action\": \"turn_on\"}\",\\\n                            \"name\": \"control_device\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    },\\\n                    {\\\n                        \"id\": \"call_GOr7qfGUPD0ZV9gAgUktyKj6\",\\\n                        \"function\": {\\\n                            \"arguments\": \"{\"device_id\": \"thermostat\", \"action\": \"set_temperature\", \"schedule_time\": \"2023-10-23T18:00:00\"}\",\\\n                            \"name\": \"set_schedule\"\\\n                        },\\\n                        \"type\": \"function\"\\\n                    }\\\n                ]\\\n            }\\\n        }\\\n    ],\n    \"created\": 1728293609,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_f85bea6784\",\n    \"usage\": {\n        \"completion_tokens\": 75,\n        \"prompt_tokens\": 1092,\n        \"total_tokens\": 1167,\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0\n        },\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 1024\n        }\n    }\n}\n```\n\nWe can see that in Run 1, the **`cached_tokens`** count is zero, which is to be expected. However, in Run 2, the **\\`cached\\_tokens** \\` count is 1024. This indicates that caching took place.\n\n### Summary\n\nPrompt caching is a very useful new addition to OpenAI’s capabilities. It can save on application run times by reducing latency and your token costs. So it’s important to monitor if and when it’s being used and investigate why it isn’t if you think it should be being used.\n\nSo, using code, as I’ve shown above, you can effectively monitor your system and intervene when you think prompt caching isn’t being applied. It would be fairly straightforward to send an automated message to yourself or to a team to indicate a potential caching issue.\n\n> \\_That’s all from me for now. I hope you found this article useful. If you did, please check out my profile page at [this link](https://medium.com/@thomas_reid). From there, you can see my other published stories and subscribe to get notified when I post new content.\\_\n>\n> _I know times are tough and wallets constrained, but if you got real value from this article, please consider [buying me a wee dram](https://ko-fi.com/taupirho)._\n\nIf you liked this content, Medium thinks you’ll find these articles interesting, too.\n\n> [**Structured output in the OpenAI API**](https://ai.gopubby.com/structured-output-in-the-openai-api-4dd2ecb1703e)\n>\n> [**Introducing the more-itertools Python library**](https://ai.gopubby.com/introducing-the-more-itertools-python-library-e5a24a901979)\n\n* * *\n\nWritten By\n\nThomas Reid\n\n[See all from Thomas Reid](https://towardsdatascience.com/author/thomas_reid/)\n\n[AI](https://towardsdatascience.com/tag/ai/), [Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Llm](https://towardsdatascience.com/tag/llm/), [OpenAI](https://towardsdatascience.com/tag/openai/), [Prompt Caching](https://towardsdatascience.com/tag/prompt-caching/)\n\nShare This Article\n\n- [Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&title=OpenAI%20Prompt%20Cache%20Monitoring)\n- [Share on LinkedIn](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&title=OpenAI%20Prompt%20Cache%20Monitoring)\n- [Share on X](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&text=OpenAI%20Prompt%20Cache%20Monitoring)\n\nTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.\n\n[Write for TDS](https://towardsdatascience.com/questions-96667b06af5/)\n\n## Related Articles\n\n- ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n\n\n\n\n\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n\n\n\n\nStep-by-step code guide to building a Convolutional Neural Network\n\n\n\n\n\n\n\n\n\n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\n\n\n\n\n\nAugust 20, 2024\n\n\n\n\n6 min read\n\n- ## [What Do Large Language Models “Understand”?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n\n\n\n\nA deep dive on the meaning of understanding and how it applies to LLMs\n\n\n\n\n\n\n\n\n\n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\n\n\n\n\n\nAugust 21, 2024\n\n\n\n\n31 min read\n\n- ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n\n\n\n\n\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n\n\n\n\nA beginner’s guide to forecast reconciliation\n\n\n\n\n\n\n\n\n\n[Dr. Robert Kübler](https://towardsdatascience.com/author/dr-robert-kuebler/)\n\n\n\n\n\nAugust 20, 2024\n\n\n\n\n13 min read\n\n- ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n\n\n\n\n\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n\n[Machine Learning](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\n\n\n\n\n\nFeature engineering, structuring unstructured data, and lead scoring\n\n\n\n\n\n\n\n\n\n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\n\n\n\n\n\nAugust 21, 2024\n\n\n\n\n7 min read\n\n- ![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/07/1Zf6XTb6jDQXVOt-N9S_YTg.png)\n\n\n\n\n\n## [Deep Dive into LSTMs & xLSTMs by Hand ✍️](https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%ef%b8%8f-c33e638bebb1/)\n\n[Deep Learning](https://towardsdatascience.com/category/artificial-intelligence/deep-learning/)\n\n\n\n\n\nExplore the wisdom of LSTM leading into xLSTMs - a probable competition to the present-day LLMs\n\n\n\n\n\n\n\n\n\n[Srijanie Dey, PhD](https://towardsdatascience.com/author/srijanie-dey/)\n\n\n\n\n\nJuly 9, 2024\n\n\n\n\n13 min read\n\n- ![Photo by Rohan Makhecha on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/03/0B5fQH3hQtbD7a_CC-scaled.jpg)\n\n\n\n\n\n## [Check Your Biases](https://towardsdatascience.com/check-your-biases-ba2685a7f799/)\n\n[Natural Language Processing](https://towardsdatascience.com/category/artificial-intelligence/nlp/)\n\n\n\n\n\nSymbolic Engines and Unexpected Results - A Personal Coding Experience\n\n\n\n\n\n\n\n\n\n[Veronica Villa](https://towardsdatascience.com/author/veronica-villa/)\n\n\n\n\n\nMarch 17, 2022\n\n\n\n\n9 min read\n\n- ![Photo by Alina Grubnyak on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/04/1nZQIsnR6zn6s0UJVXQ7krA.jpeg)\n\n\n\n\n\n## [Does Your Company Have a Data Strategy?](https://towardsdatascience.com/does-your-company-have-a-data-strategy-e8944dbec4ae/)\n\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n\n\n\n\n\nThis sophistication matrix can show you where you need to go\n\n\n\n\n\n\n\n\n\n[Kate Minogue](https://towardsdatascience.com/author/kminoguem/)\n\n\n\n\n\nApril 8, 2024\n\n\n\n\n9 min read\n\n\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.","metadata":{"ogTitle":"OpenAI Prompt Cache Monitoring | Towards Data Science","generator":"dominant-color-images 1.2.0","ogDescription":"A worked example using Python and the chat completion API","twitter:creator":"@TDataScience","title":"OpenAI Prompt Cache Monitoring | Towards Data Science","ogImage":"https://towardsdatascience.com/wp-content/uploads/2024/12/0ygrvmwtWhGnpxiJ4.jpg","og:type":"article","viewport":["width=device-width, initial-scale=1","width=device-width, initial-scale=1.0, viewport-fit=cover"],"og:description":"A worked example using Python and the chat completion API","og:image:height":"1024","language":"en-US","ogLocale":"en_US","article:published_time":"2024-12-10T15:01:48+00:00","og:image":"https://towardsdatascience.com/wp-content/uploads/2024/12/0ygrvmwtWhGnpxiJ4.jpg","twitter:data1":"Thomas Reid","twitter:site":"@TDataScience","og:locale":"en_US","article:modified_time":"2025-01-13T13:35:28+00:00","og:site_name":"Towards Data Science","og:image:type":"image/jpeg","twitter:label1":"Written by","twitter:data2":"13 minutes","og:image:width":"1792","ogSiteName":"Towards Data Science","msapplication-TileImage":"https://towardsdatascience.com/wp-content/uploads/2025/02/cropped-Favicon-270x270.png","modifiedTime":"2025-01-13T13:35:28+00:00","twitter:label2":"Est. reading time","og:url":"https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/","twitter:card":"summary_large_image","author":"Thomas Reid","ogUrl":"https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/","publishedTime":"2024-12-10T15:01:48+00:00","robots":"index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1","og:title":"OpenAI Prompt Cache Monitoring | Towards Data Science","favicon":"https://towardsdatascience.com/wp-content/uploads/2025/02/cropped-Favicon-32x32.png","scrapeId":"019c62eb-509c-7409-9bdc-298a4c0b8eaa","sourceURL":"https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/","url":"https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/","statusCode":200,"contentType":"text/html; charset=UTF-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"8205ef00-7861-4dc0-977e-b02fad565f1e","creditsUsed":1},"links":["https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=fl-branding","https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=sl-branding","https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0/#wp--skip-link--target","https://towardsdatascience.com/category/artificial-intelligence/","https://towardsdatascience.com/author/thomas_reid/","https://blog.stackademic.com/installing-wsl2-ubuntu-for-windows-81122c551bc2","https://docs.anaconda.com/miniconda","https://platform.openai.com/api-keys","https://medium.com/@thomas_reid","https://ko-fi.com/taupirho","https://ai.gopubby.com/structured-output-in-the-openai-api-4dd2ecb1703e","https://ai.gopubby.com/introducing-the-more-itertools-python-library-e5a24a901979","https://towardsdatascience.com/tag/ai/","https://towardsdatascience.com/tag/artificial-intelligence/","https://towardsdatascience.com/tag/llm/","https://towardsdatascience.com/tag/openai/","https://towardsdatascience.com/tag/prompt-caching/","https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&title=OpenAI%20Prompt%20Cache%20Monitoring","https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&title=OpenAI%20Prompt%20Cache%20Monitoring","https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fopenai-prompt-cache-monitoring-7cb8df21d0d0%2F&text=OpenAI%20Prompt%20Cache%20Monitoring","https://towardsdatascience.com/questions-96667b06af5/","https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/","https://towardsdatascience.com/author/shreya-rao/","https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/","https://towardsdatascience.com/author/tarikdzekman/","https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/","https://towardsdatascience.com/author/dr-robert-kuebler/","https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/","https://towardsdatascience.com/category/artificial-intelligence/machine-learning/","https://towardsdatascience.com/author/shawhin/","https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%ef%b8%8f-c33e638bebb1/","https://towardsdatascience.com/category/artificial-intelligence/deep-learning/","https://towardsdatascience.com/author/srijanie-dey/","https://towardsdatascience.com/check-your-biases-ba2685a7f799/","https://towardsdatascience.com/category/artificial-intelligence/nlp/","https://towardsdatascience.com/author/veronica-villa/","https://towardsdatascience.com/does-your-company-have-a-data-strategy-e8944dbec4ae/","https://towardsdatascience.com/author/kminoguem/"]},{"url":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","title":"Controlling Costs When Using OpenAI API - Medium","description":"Cashing · The matched segment must be at least 1024 tokens long to be eligible for caching. · Token matching occurs in increments of 128 tokens.","position":9,"markdown":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Controlling Costs When Using OpenAI API\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:32:32/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---byline--fd5a038fa391---------------------------------------)\n\n[Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---byline--fd5a038fa391---------------------------------------)\n\nFollow\n\n7 min read\n\n·\n\nNov 16, 2024\n\n49\n\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3Dfd5a038fa391&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=---header_actions--fd5a038fa391---------------------post_audio_button------------------)\n\nShare\n\nThe OpenAI API offers a powerful way to integrate the capabilities of LLMs into your applications. However, frequent usage can lead to escalating costs. This article explores effective strategies to manage and reduce these expenses while ensuring you achieve your desired results.\n\nPress enter or click to view image in full size\n\n![](https://miro.medium.com/v2/resize:fit:700/1*ATGwuqkZNjcKn_JneBwVJA.png)\n\n## **Cost and token awareness**\n\n### **Be mindful of the cost**\n\nThe first step in managing the cost of using the OpenAI API is to stay informed about your usage. OpenAI’s dashboard provides detailed insights under the **Usage** tab, including the daily number of API calls, tokens consumed, and associated costs. Regularly monitoring your spending can encourage better practices and help you take steps to minimize unnecessary expenses.\n\nYou can access the usage dashboard directly [here](https://platform.openai.com/usage)\n\nModel pricing can be found at: [https://openai.com/api/pricing/](https://openai.com/api/pricing/)\n\n### **Count your tokens**\n\nTo keep track of token usage, you can use the **tiktoken** package provided by OpenAI, which is available for Python( [tiktoken for python](https://github.com/openai/tiktoken/blob/main/README.md)). Tokenization is based on the encoding specific to the model being used, notably, the 4o family of models uses a different encoding than earlier versions\n\n```\nimport tiktoken\n\ndef get_number_of_tokens(prompt_text, model_name):\n  # function to get the number of tokens used in the prompt\n  encoding = tiktoken.encoding_for_model('gpt-4o-mini') # firsr get the encoding used by the model.\n  list_of_tokens = encoding.encode(prompt_text) #returns a list of integers\n  return len(list_of_tokens)\n```\n\n### Be mindful of hidden tokens\n\n- **Functions** — When using functions with the OpenAI API, it’s important to note that all function names, descriptions, and additional parameters are included in the input token count. If multiple functions are used, the cumulative token usage can become significant.\n\n\\- OpenAI provides code [here](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) to help count these input tokens.\n- **Memory** — Frameworks like LangChain, which are commonly used for building AI agents, come with built-in memory mechanisms. These frameworks collect information from previous calls, responses, and even internal discussions, storing it as context for subsequent calls. While this functionality is powerful, it also increases input token usage. To control costs, consider managing memory effectively by employing strategies such as **buffering**, **summarization**, or **selective retention** to limit the amount of context carried forward.\n- **o1 and o1-mini models** — As we transition into the era of “thinking models,” it’s essential to understand the concept of **reasoning tokens.** These tokens, while not part of the output seen by the user, are utilized internally for the model’s reasoning processes and are still counted as output tokens, thus can significantly increase costs.\n\n### Check the Actual Usage Data\n\nThe usage data from an API call provides a detailed breakdown of all relevant cost factors, including input tokens, caching (discussed later), and reasoning tokens. This data helps you better understand and optimize token usage.\n\n```\ndef create_chat_completion_object(query):\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are a helpful assistant.\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"{query}\",\\\n            }\\\n        ],\n    )\n    return response\n\n# Example Usage\nchat_object = create_chat_completion_object(\"What is the capital of India?\")\nprint('Response:', chat_object.choices[0].message.content)\nprint('Usage:', chat_object.usage)\n```\n\nOutput:\n\n```\nResponse:  The capital of India is New Delhi.\nUsage: CompletionUsage(completion_tokens=8, prompt_tokens=24, total_tokens=32,\nprompt_tokens_details={'cached_tokens': 0, 'audio_tokens': 0},\ncompletion_tokens_details={'reasoning_tokens': 0, 'audio_tokens': 0,\n'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0})\n```\n\n## Reducing cost **for a set input**\n\nIf you’re unable to minimize the number of input and output tokens, OpenAI offers two effective methods to significantly reduce operational costs: **Caching** and **Batching**.\n\n### Cashing\n\nCaching is a powerful feature designed to optimize API calls involving large contexts. A potential use case for caching arises when you need to ask detailed, complex questions about a lengthy article. Attempting to process the entire article and all questions within a single API call can result in reduced performance. By leveraging caching, you can reuse the initial context (the article) across multiple API calls each with a different question, significantly lowering costs while maintaining efficiency.\n\nTo address this use case, OpenAI automatically caches prompts longer than **1024 tokens**. When a subsequent API call contains an initial prompt segment that matches the cached data exactly (both text and position), the system retrieves the cached portion instead of reprocessing it.\n\n**How It Works:**\n\n- The matched segment must be at least **1024 tokens long** to be eligible for caching.\n- Token matching occurs in **increments of 128 tokens**.\n- Cached tokens are billed at **50% of the standard input token cost.**\n\n**Key Points to Remember:**\n\n- Once the text deviates from the cached result, the remaining tokens are processed and billed as regular input tokens.\n- Cached data remains active for **5–10 minutes of inactivity.**\n- The number of cached tokens used will be reflected in the chat completion object (as shown earlier).\n\nFor more details see OpenAI’s [Prompt Caching Guide](https://platform.openai.com/docs/guides/prompt-caching).\n\n### Batching\n\nBatching allows you to send multiple requests for chat or embedding jobs simultaneously, significantly reducing the per-request cost. OpenAI discounts both **input and output tokens by 50%** for batched jobs and promises to complete them within a 24-hour timeframe (But usually finishes them faster).\n\n## Get Michael Shapiro MD MSc’s stories in your inbox\n\nJoin Medium for free to get updates from this writer.\n\nSubscribe\n\nSubscribe\n\n**Applications:**\n\n- **Batch Embedding:** Ideal for processing large-scale data efficiently. For a detailed guide on optimizing embedding speed and cost, check out my extensive guide [here](https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a).\n- **Batch Chat Requests:** Chat calls can also be batched using OpenAI’s framework extensively explained in the OpenAI guide [https://platform.openai.com/docs/guides/batch](https://platform.openai.com/docs/guides/batch).\n\n## Strategies for Managing Prompts and API Calls\n\n### Limiting Input Tokens using Context Filtering\n\nWhen working with a large corpus of text as context for an API call, much of the information may be irrelevant. Including such lengthy content increases costs and can negatively impact performance. To address this, consider employing one of the following context filtering strategies:\n\n**Embedding-Based Filtering:**\n\n- Split the corpus into manageable chunks, with each chunk serving as a candidate for inclusion in the final context.\n- Use an **embedding model** to generate embeddings for each chunk and for the user query.\n- Compare the embeddings to identify the chunks most relevant to the query.\n- Assemble only the relevant chunks into the context for the API call.\n\n**Judgment-Based Filtering Using a Cheaper Model:**\n\n- Split the data into chunks.\n- Use a **less expensive model** (e.g., _gpt-4o-mini_) to act as a \"judge\" to evaluate whether a chunk is relevant to the query.\n- Collect only the chunks deemed relevant and include them in the final prompt sent to a more powerful (and more expensive) model.\n\nThese tactics ensure that only the most relevant information is processed by the API, helping to reduce token usage and improve performance. They can also be used in tandem when dealing with very large context filtering.\n\nPress enter or click to view image in full size\n\n![](https://miro.medium.com/v2/resize:fit:700/1*e2Rkqc4xyFqfvwaQ00rTNQ.png)\n\nPlease see [my article about RAG](https://medium.com/@mikehpg/introduction-to-medical-text-analysis-with-llm-rag-integration-86df12588a10) for more details about using embedding for retrieval.\n\n### **Limiting Output Tokens by Output Structuring**\n\nReducing output tokens is a key strategy for optimizing costs as output tokens cost 3 time more than input tokens. One effective way to do this is by structuring the output to include only the information you need. OpenAI’s latest models, such as gpt-4o and gpt-4o-mini, support structured output, allowing you to ensure that the API generates responses in a precise, efficient format without unnecessary tokens. The latest implementation of structured output using Pydantic makes this setup easy to use and straightforward.\n\n**Use Cases for Structured Output:**\n\n1. **_Classification/Validation Tasks:_** When working with a list of data points that need to be classified into groups or validated (e.g., binary “True/False”), structured output can minimize token usage. This is particularly advantageous when processing large datasets where concise outputs are essential.\n2. **_Data Extraction:_** If your goal is to extract specific details, such as numeric values, structured output can ensure only the required information is returned. This reduces redundant tokens and focuses on the key data points you need.\n3. **_Chain of Models:_** In workflows where the output of one model serves as the input for another, structured output ensures compatibility and minimizes overhead by restricting the response to just the necessary data. This efficiency is critical for maintaining performance and managing costs in multi-step pipelines.\n\nHere is an example of how to use the structured output for a classification task:\n\n```\nfrom pydantic import BaseModel\n\nclass custom_output(BaseModel):\n    a: list[bool]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\"role\": \"system\",\\\n        \"content\":\\\n        '''You will recieve a list of animals,\\\n        for each animal return True if they are a mammal,\\\n        else return False, output as a list of booleans nothing else.'''\\\n        },\\\n        {\"role\": \"user\",\\\n         \"content\":\\\n         '''Lion, Eagle, Dolphin, Crocodile, Elephant, Snake, Kangaroo, Frog,\\\n         Tiger, Penguin, Whale, Shark, Giraffe, Octopus, Bear, Turtle, Deer,\\\n         Lizard, Rabbit, Butterfly'''},\\\n    ],\n    response_format=custom_output,\n)\n\nresult = completion.choices[0].message.parsed\nprint(result.a)\nprint(completion.usage)\n```\n\nOutput:\n\n```\n[True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False]\nCompletionUsage(completion_tokens=24, prompt_tokens=138, total_tokens=162, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n```\n\nThe efficiency is well demonstrated in this example — processing a list of 20 queries returned only **24 output tokens.**\n\n**Remarks:**\n\n1. Using _structured output_ adds a small number of input tokens (approximately 30 in the example above).\n2. The _Pydantic-based structured output_ works with OpenAI Python package version **0.14 and above.**\n\nFor more details on structured outputs, refer to OpenAI’s documentation: [Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs).\n\n## Conclusion\n\nEffectively managing costs while using the OpenAI API is crucial, especially when working with frequent or large-scale operations. By adopting the strategies outlined in this article, such as monitoring usage, leveraging caching and batching, filtering context, and using structured outputs, you can significantly reduce expenses without compromising performance.\n\n[OpenAI](https://medium.com/tag/openai?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Llm Applications](https://medium.com/tag/llm-applications?source=post_page-----fd5a038fa391---------------------------------------)\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:48:48/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---post_author_info--fd5a038fa391---------------------------------------)\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:64:64/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---post_author_info--fd5a038fa391---------------------------------------)\n\nFollow\n\n[**Written by Michael Shapiro MD MSc**](https://medium.com/@mikehpg?source=post_page---post_author_info--fd5a038fa391---------------------------------------)\n\n[120 followers](https://medium.com/@mikehpg/followers?source=post_page---post_author_info--fd5a038fa391---------------------------------------)\n\n· [26 following](https://medium.com/@mikehpg/following?source=post_page---post_author_info--fd5a038fa391---------------------------------------)\n\nI am an internal medicine physician and a senior data scientist, I write about LLMs & other machine learning tools and their potential to improve healthcare.\n\nFollow\n\n## No responses yet\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\nWrite a response\n\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=---post_responses--fd5a038fa391---------------------respond_sidebar------------------)\n\nCancel\n\nRespond\n\n## More from Michael Shapiro MD MSc\n\n![Graph Neural Networks for Knowledge Graphs](https://miro.medium.com/v2/resize:fit:679/format:webp/1*Bf5IyKsXF1pYew85L5kLlA.png)\n\n[![Towards AI](https://miro.medium.com/v2/resize:fill:20:20/1*JyIThO-cLjlChQLb6kSlVQ.png)](https://medium.com/towards-artificial-intelligence?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nIn\n\n[Towards AI](https://medium.com/towards-artificial-intelligence?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nby\n\n[Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[**A Practical Guide to training Graph Neural Networks with Knowledge Graphs for extracting node embedding.**](https://medium.com/towards-artificial-intelligence/graph-neural-networks-for-knowledge-graphs-d26352a4f5e8?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nAug 4, 2025\n\n[A clap icon114\\\\\n\\\\\nA response icon2](https://medium.com/towards-artificial-intelligence/graph-neural-networks-for-knowledge-graphs-d26352a4f5e8?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n![Ensuring Privacy and Data Safety with OpenAI](https://miro.medium.com/v2/resize:fit:679/format:webp/1*K2ZZeLFaKypS2sbBSEIp3Q.png)\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:20:20/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[**Comprehensive guide on OpenAI’s chatGPT and API data privacy & safety: encryption, data retention, compliance & risk mitigation.**](https://medium.com/@mikehpg/ensuring-privacy-and-data-safety-with-openai-a-comprehensive-guide-5a744e2c6416?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nDec 10, 2024\n\n[A clap icon11](https://medium.com/@mikehpg/ensuring-privacy-and-data-safety-with-openai-a-comprehensive-guide-5a744e2c6416?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n![Tutorial: Batch embedding with OpenAI API](https://miro.medium.com/v2/resize:fit:679/format:webp/1*Bh6gWtK0N8gQVD7UFFanWg.jpeg)\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:20:20/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[**Making numerous calls to the OpenAI Embedding API can be time-consuming. While asynchronous methods can speed up the process, OpenAI has…**](https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nJul 14, 2024\n\n[A clap icon49\\\\\n\\\\\nA response icon4](https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n![Creating a Custom GPT with RAG](https://miro.medium.com/v2/resize:fit:679/format:webp/1*jBp0k7Jmyl3thjBmdBclug.png)\n\n[![Michael Shapiro MD MSc](https://miro.medium.com/v2/resize:fill:20:20/1*Fx0bHwA2BvuFT4rLf59BOQ.jpeg)](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[**How to boost a custom GPT with RAG, combining AI’s conversational skills with precise, up-to-date medical knowledge.**](https://medium.com/@mikehpg/creating-a-custom-gpt-with-rag-2441fcabe40f?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\nAug 9, 2024\n\n[A clap icon16\\\\\n\\\\\nA response icon1](https://medium.com/@mikehpg/creating-a-custom-gpt-with-rag-2441fcabe40f?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------)\n\n[See all from Michael Shapiro MD MSc](https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391---------------------------------------)\n\n## Recommended from Medium\n\n![Local LLMs That Can Replace Claude Code](https://miro.medium.com/v2/resize:fit:679/format:webp/0*JpX_vOrpLzFhJfJM.png)\n\n[![Agent Native](https://miro.medium.com/v2/resize:fill:20:20/1*dt5tcaKMBhB6JboQ9lIEAA.jpeg)](https://medium.com/@agentnativedev?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[Agent Native](https://medium.com/@agentnativedev?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**Small team of engineers can easily burn >$2K/mo on Anthropic’s Claude Code (Sonnet/Opus 4.5). As budgets are tight, you might be wondering…**](https://medium.com/@agentnativedev/local-llms-that-can-replace-claude-code-6f5b6cac93bf?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nJan 20\n\n[A clap icon1K\\\\\n\\\\\nA response icon32](https://medium.com/@agentnativedev/local-llms-that-can-replace-claude-code-6f5b6cac93bf?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n![Stanford Just Killed Prompt Engineering With 8 Words (And I Can’t Believe It Worked)](https://miro.medium.com/v2/resize:fit:679/format:webp/1*va3sFwIm26snbj5ly9ZsgA.jpeg)\n\n[![Generative AI](https://miro.medium.com/v2/resize:fill:20:20/1*M4RBhIRaSSZB7lXfrGlatA.png)](https://medium.com/generative-ai?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nIn\n\n[Generative AI](https://medium.com/generative-ai?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nby\n\n[Adham Khaled](https://medium.com/@adham__khaled__?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**ChatGPT keeps giving you the same boring response? This new technique unlocks 2× more creativity from ANY AI model — no training required…**](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nOct 19, 2025\n\n[A clap icon23K\\\\\n\\\\\nA response icon618](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n![I Tested Opus 4.6 In Claude Code: How to Use Opus 4.6 in Claude Code](https://miro.medium.com/v2/resize:fit:679/format:webp/1*Km0lD-ytmkgZApZZcxvmAg.png)\n\n[![Joe Njenga](https://miro.medium.com/v2/resize:fill:20:20/1*0Hoc7r7_ybnOvk1t8yR3_A.jpeg)](https://medium.com/@joe.njenga?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[Joe Njenga](https://medium.com/@joe.njenga?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**If you hate generating AI code slop at a slow speed, Claude Opus in Claude Code is the fastest workflow for speed, efficiency, and…**](https://medium.com/@joe.njenga/i-tested-opus-4-6-in-claude-code-how-to-use-opus-4-6-in-claude-code-f5e289956f8f?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n4d ago\n\n[A clap icon33](https://medium.com/@joe.njenga/i-tested-opus-4-6-in-claude-code-how-to-use-opus-4-6-in-claude-code-f5e289956f8f?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n![I Built an AI Company with OpenClaw + Vercel + Supabase — Two Weeks Later, They Run It Themselves](https://miro.medium.com/v2/resize:fit:679/format:webp/1*4Wb78Tvg-lICb9hX7t0Xqg.png)\n\n[![Coding Nexus](https://miro.medium.com/v2/resize:fill:20:20/1*KCZtO6-wFqmTaMmbTMicbw.png)](https://medium.com/coding-nexus?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nIn\n\n[Coding Nexus](https://medium.com/coding-nexus?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nby\n\n[Civil Learning](https://medium.com/@civillearning?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**6 AI agents, 1 VPS, 1 Supabase database — the journey from “agents can talk” to “agents run the website autonomously”.**](https://medium.com/coding-nexus/i-built-an-ai-company-with-openclaw-vercel-supabase-two-weeks-later-they-run-it-themselves-514cf3db07e6?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n4d ago\n\n[A clap icon467\\\\\n\\\\\nA response icon17](https://medium.com/coding-nexus/i-built-an-ai-company-with-openclaw-vercel-supabase-two-weeks-later-they-run-it-themselves-514cf3db07e6?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n![Running OpenCode with Local LLMs on an M3 Ultra 512GB](https://miro.medium.com/v2/resize:fit:679/format:webp/1*tk-YrzEfJ684LGITGLiumw.jpeg)\n\n[![Diko Ko](https://miro.medium.com/v2/resize:fill:20:20/1*6qAlAYyluLek-2mS4EOkgg.jpeg)](https://medium.com/@diko?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[Diko Ko](https://medium.com/@diko?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**1\\. Introduction**](https://medium.com/@diko/running-opencode-with-local-llms-on-an-m3-ultra-512gb-63ca7d0b299f?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nFeb 8\n\n[A clap icon168\\\\\n\\\\\nA response icon1](https://medium.com/@diko/running-opencode-with-local-llms-on-an-m3-ultra-512gb-63ca7d0b299f?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n![Finally, A Native Agentic Framework for SLMs](https://miro.medium.com/v2/resize:fit:679/format:webp/1*VkziJnpANRbBVwoAYaax5Q.jpeg)\n\n[![Pawel](https://miro.medium.com/v2/resize:fill:20:20/1*axcXEgSlEMqpEtRExV2r2Q.jpeg)](https://medium.com/@meshuggah22?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[Pawel](https://medium.com/@meshuggah22?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[**I’ve been looking for this for a while — an agentic framework that doesn’t assume you’re running proprietary LLMs through an API.**](https://medium.com/@meshuggah22/finally-a-native-agentic-framework-for-slms-55858d4284c5?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\nFeb 7\n\n[A clap icon122\\\\\n\\\\\nA response icon2](https://medium.com/@meshuggah22/finally-a-native-agentic-framework-for-slms-55858d4284c5?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------)\n\n[See more recommendations](https://medium.com/?source=post_page---read_next_recirc--fd5a038fa391---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----fd5a038fa391---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----fd5a038fa391---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fd5a038fa391---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----fd5a038fa391---------------------------------------)\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)","metadata":{"ogUrl":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","ogSiteName":"Medium","twitter:image:src":"https://miro.medium.com/v2/resize:fit:1200/1*ATGwuqkZNjcKn_JneBwVJA.png","title":"Controlling Costs When Using OpenAI API | by Michael Shapiro MD MSc | Medium","author":"Michael Shapiro MD MSc","al:android:url":"medium://p/fd5a038fa391","ogTitle":"Controlling Costs When Using OpenAI API","al:ios:app_name":"Medium","al:android:package":"com.medium.reader","publishedTime":"2024-11-19T19:18:07.953Z","al:ios:url":"medium://p/fd5a038fa391","al:android:app_name":"Medium","og:description":"Learn strategies to optimize costs when using OpenAI API, including caching, batching, structured outputs, and token management tips.","al:web:url":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","language":"en","ogDescription":"Learn strategies to optimize costs when using OpenAI API, including caching, batching, structured outputs, and token management tips.","ogImage":"https://miro.medium.com/v2/resize:fit:1200/1*ATGwuqkZNjcKn_JneBwVJA.png","og:url":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","twitter:data1":"7 min read","article:published_time":"2024-11-19T19:18:07.953Z","fb:app_id":"542599432471018","viewport":"width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1","twitter:app:id:iphone":"828256236","og:image":"https://miro.medium.com/v2/resize:fit:1200/1*ATGwuqkZNjcKn_JneBwVJA.png","article:author":"https://medium.com/@mikehpg","og:type":"article","twitter:site":"@Medium","apple-itunes-app":"app-id=828256236, app-argument=/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391, affiliate-data=pt=698524&ct=smart_app_banner&mt=8","twitter:app:url:iphone":"medium://p/fd5a038fa391","twitter:description":"Learn strategies to optimize costs when using OpenAI API, including caching, batching, structured outputs, and token management tips.","og:site_name":"Medium","twitter:app:name:iphone":"Medium","al:ios:app_store_id":"828256236","og:title":"Controlling Costs When Using OpenAI API","robots":"index,noarchive,follow,max-image-preview:large","description":"Controlling Costs When Using OpenAI API The OpenAI API offers a powerful way to integrate the capabilities of LLMs into your applications. However, frequent usage can lead to escalating costs. This …","referrer":"unsafe-url","theme-color":"#000000","twitter:title":"Controlling Costs When Using OpenAI API","twitter:card":"summary_large_image","twitter:label1":"Reading time","favicon":"https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19","scrapeId":"019c62eb-509c-7409-9bdc-2e350aab8669","sourceURL":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","url":"https://medium.com/@mikehpg/controlling-cost-when-using-openai-api-fd5a038fa391","statusCode":200,"contentType":"text/html; charset=utf-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"9395f87d-f268-4960-be78-04988cb59ca7","creditsUsed":1},"links":["https://medium.com/sitemap/sitemap.xml","https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------","https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=post_page---top_nav_layout_nav-----------------------global_nav------------------","https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------","https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------","https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------","https://medium.com/@mikehpg?source=post_page---byline--fd5a038fa391---------------------------------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ffd5a038fa391&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&user=Michael+Shapiro+MD+MSc&userId=7549ec00d9e3&source=---header_actions--fd5a038fa391---------------------clap_footer------------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd5a038fa391&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=---header_actions--fd5a038fa391---------------------bookmark_footer------------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3Dfd5a038fa391&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=---header_actions--fd5a038fa391---------------------post_audio_button------------------","https://platform.openai.com/usage","https://openai.com/api/pricing/","https://github.com/openai/tiktoken/blob/main/README.md","https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken","https://platform.openai.com/docs/guides/prompt-caching","https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a","https://platform.openai.com/docs/guides/batch","https://medium.com/@mikehpg/introduction-to-medical-text-analysis-with-llm-rag-integration-86df12588a10","https://platform.openai.com/docs/guides/structured-outputs","https://medium.com/tag/openai?source=post_page-----fd5a038fa391---------------------------------------","https://medium.com/tag/llm-applications?source=post_page-----fd5a038fa391---------------------------------------","https://medium.com/@mikehpg?source=post_page---post_author_info--fd5a038fa391---------------------------------------","https://medium.com/@mikehpg/followers?source=post_page---post_author_info--fd5a038fa391---------------------------------------","https://medium.com/@mikehpg/following?source=post_page---post_author_info--fd5a038fa391---------------------------------------","https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--fd5a038fa391---------------------------------------","https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcontrolling-cost-when-using-openai-api-fd5a038fa391&source=---post_responses--fd5a038fa391---------------------respond_sidebar------------------","https://medium.com/towards-artificial-intelligence?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/towards-artificial-intelligence/graph-neural-networks-for-knowledge-graphs-d26352a4f5e8?source=post_page---author_recirc--fd5a038fa391----0---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd26352a4f5e8&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgraph-neural-networks-for-knowledge-graphs-d26352a4f5e8&source=---author_recirc--fd5a038fa391----0-----------------bookmark_preview----326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg/ensuring-privacy-and-data-safety-with-openai-a-comprehensive-guide-5a744e2c6416?source=post_page---author_recirc--fd5a038fa391----1---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a744e2c6416&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fensuring-privacy-and-data-safety-with-openai-a-comprehensive-guide-5a744e2c6416&source=---author_recirc--fd5a038fa391----1-----------------bookmark_preview----326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a?source=post_page---author_recirc--fd5a038fa391----2---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95da95c9778a&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Ftutorial-batch-embedding-with-openai-api-95da95c9778a&source=---author_recirc--fd5a038fa391----2-----------------bookmark_preview----326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg/creating-a-custom-gpt-with-rag-2441fcabe40f?source=post_page---author_recirc--fd5a038fa391----3---------------------326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2441fcabe40f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mikehpg%2Fcreating-a-custom-gpt-with-rag-2441fcabe40f&source=---author_recirc--fd5a038fa391----3-----------------bookmark_preview----326f3348_0633_4591_9e46_c06b4e6e539e--------------","https://medium.com/@mikehpg?source=post_page---author_recirc--fd5a038fa391---------------------------------------","https://medium.com/@agentnativedev?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@agentnativedev/local-llms-that-can-replace-claude-code-6f5b6cac93bf?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f5b6cac93bf&operation=register&redirect=https%3A%2F%2Fagentnativedev.medium.com%2Flocal-llms-that-can-replace-claude-code-6f5b6cac93bf&source=---read_next_recirc--fd5a038fa391----0-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/generative-ai?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@adham__khaled__?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8349d6524d2b&operation=register&redirect=https%3A%2F%2Fgenerativeai.pub%2Fstanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b&source=---read_next_recirc--fd5a038fa391----1-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@joe.njenga?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@joe.njenga/i-tested-opus-4-6-in-claude-code-how-to-use-opus-4-6-in-claude-code-f5e289956f8f?source=post_page---read_next_recirc--fd5a038fa391----0---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5e289956f8f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40joe.njenga%2Fi-tested-opus-4-6-in-claude-code-how-to-use-opus-4-6-in-claude-code-f5e289956f8f&source=---read_next_recirc--fd5a038fa391----0-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/coding-nexus?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@civillearning?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/coding-nexus/i-built-an-ai-company-with-openclaw-vercel-supabase-two-weeks-later-they-run-it-themselves-514cf3db07e6?source=post_page---read_next_recirc--fd5a038fa391----1---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F514cf3db07e6&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoding-nexus%2Fi-built-an-ai-company-with-openclaw-vercel-supabase-two-weeks-later-they-run-it-themselves-514cf3db07e6&source=---read_next_recirc--fd5a038fa391----1-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@diko?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@diko/running-opencode-with-local-llms-on-an-m3-ultra-512gb-63ca7d0b299f?source=post_page---read_next_recirc--fd5a038fa391----2---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63ca7d0b299f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40diko%2Frunning-opencode-with-local-llms-on-an-m3-ultra-512gb-63ca7d0b299f&source=---read_next_recirc--fd5a038fa391----2-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@meshuggah22?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/@meshuggah22/finally-a-native-agentic-framework-for-slms-55858d4284c5?source=post_page---read_next_recirc--fd5a038fa391----3---------------------d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55858d4284c5&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40meshuggah22%2Ffinally-a-native-agentic-framework-for-slms-55858d4284c5&source=---read_next_recirc--fd5a038fa391----3-----------------bookmark_preview----d59acd53_c5c1_4412_872f_63bb8bae91ff--------------","https://medium.com/?source=post_page---read_next_recirc--fd5a038fa391---------------------------------------","https://help.medium.com/hc/en-us?source=post_page-----fd5a038fa391---------------------------------------","https://status.medium.com/?source=post_page-----fd5a038fa391---------------------------------------","https://medium.com/about?autoplay=1&source=post_page-----fd5a038fa391---------------------------------------","https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----fd5a038fa391---------------------------------------","mailto:pressinquiries@medium.com","https://blog.medium.com/?source=post_page-----fd5a038fa391---------------------------------------","https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fd5a038fa391---------------------------------------","https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----fd5a038fa391---------------------------------------","https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fd5a038fa391---------------------------------------","https://speechify.com/medium?source=post_page-----fd5a038fa391---------------------------------------","https://www.google.com/intl/en/policies/privacy/","https://www.google.com/intl/en/policies/terms/"]},{"url":"https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models","title":"Prompt Caching with OpenAI, Anthropic, and Google Models","description":"Supports prompts that are 1,024 tokens or longer. Decreases latency by up to 80% and reduces costs by 50% for cached tokens. Cache lifespan: ...","position":10,"markdown":"# Prompt Caching with OpenAI, Anthropic, and Google Models\n\nLast updated on\n\nOctober 23, 2025\n\nContents\n\n[What is prompt caching](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#what-is-prompt-caching)\n\n[Prompt caching vs. classic caching](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-vs.-classic-caching)\n\n[Why use prompt caching?](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#why-use-prompt-caching?)\n\n[How does prompt caching work?](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#how-does-prompt-caching-work?)\n\n[How prompt caching works, technically](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#how-prompt-caching-works,-technically)\n\n[Prompt caching with OpenAI](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-with-openai)\n\n[Prompt caching with Anthropic’s Claude](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-with-anthropic%E2%80%99s-claude)\n\n[Context caching with Google’s Gemini](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#context-caching-with-google%E2%80%99s-gemini)\n\n[Comparing prompt caching across model providers](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#comparing-prompt-caching-across-model-providers)\n\n[Best practices for effective prompt caching](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#best-practices-for-effective-prompt-caching)\n\n[Real-world prompt cache examples](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#real-world-prompt-cache-examples-)\n\n[Wrapping up](https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#wrapping-up)\n\nPrompt caching is one of the easiest ways to reduce costs and latency when prompting LLMs.\n\nBut each model provider handles prompt caching slightly differently. Navigating the intricacies of each provider is challenging, given that the information is usually spread across various pieces of documentation.\n\nIn a similar fashion to our [model card directory](https://www.prompthub.us/resources/llm-model-card-directory), where we pulled info on various models and providers, this post aims to centralize prompt caching-related information for OpenAI, Anthropic, and Google models. Additionally, we’ll go over some high-level information and prompt caching best practices.\n\n‍\n\nView transcript\n\nHey everyone, how's it going? Dan here from PromptHub. Hope you're having a great week and happy Friday. Today, we're going to talk about how you can save money and reduce latency with prompt caching. We'll look at it from the perspectives of OpenAI, Anthropic, and Google.\n\n### What is Prompt Caching?\n\nPrompt caching is a feature offered by model providers that optimizes requests by reusing parts of prompts that are frequently repeated. For example, if you have a chatbot with a large system message, you can cache that system message so that only new user messages are processed instead of reprocessing the entire system instructions each time.\n\nUnlike traditional caching in software development, which stores outputs like images or API results, prompt caching stores inputs (e.g., parts of a prompt) since LLM outputs are dynamic and constantly changing.\n\n### Why Use Prompt Caching?\n\n- **Reduce Latency:** By reusing cached components, response times improve significantly.\n- **Lower Costs:** Cached tokens are cheaper to process compared to non-cached tokens.\n\n### How Does Prompt Caching Work?\n\nModel providers look for exact matches in the prefix of your prompt. For instance, if you repeatedly send the same system message followed by different user messages, the system message can be cached. However, the prefix must match exactly for a cache hit to occur.\n\nExample:\n\n- **First Request:** \"Act like a financial assistant...\" (system message)\n- **Second Request:** If the same prefix is used, it matches and is cached. If even one token differs, it will not match.\n\n### Provider-Specific Details\n\n#### OpenAI\n\n- No code changes or additional API headers required.\n- Supports prompts that are 1,024 tokens or longer.\n- Decreases latency by up to 80% and reduces costs by 50% for cached tokens.\n- Cache lifespan: Typically 5–10 minutes, extendable to an hour during off-peak times.\n- Works automatically with models like GPT-4, GPT-4 Turbo, and 01 Preview.\n- Static components should be at the top (e.g., system message) and dynamic content (e.g., user messages) at the bottom to increase cache hit rates.\n\n#### Anthropic\n\n- Requires additional API headers and manual configuration using a \"cache-control\" parameter.\n- Supports up to four cache breakpoints, processed in the order of tools, system, and messages.\n- Cache lifespan: 5 minutes, refreshed with each use.\n- Pricing: Writing to the cache is slightly more expensive, but hitting the cache is significantly cheaper (90% cost reduction).\n- Available for newer models like Claude 3.5 and Claude 3.0.\n\n#### Google\n\n- Uses the term \"context caching\" instead of \"prompt caching.\"\n- Requires manual setup, similar to Anthropic.\n- Cache lifespan: Default is 1 hour but can be customized.\n- Pricing: Based on token storage duration and token count, with a cheap rate for cached tokens.\n- Supported on stable versions of Gemini 1.5 models (not available for beta versions).\n\n### Best Practices for Prompt Caching\n\n- Place static content (e.g., system messages) at the top of the prompt and dynamic content (e.g., user inputs) at the bottom to maximize cache hits.\n- Use delimiters to clearly separate static and dynamic content.\n- Test and monitor cache performance regularly to identify optimization opportunities.\n\n### Conclusion\n\nPrompt caching is an effective way to reduce costs and improve latency when working with LLMs. By understanding how caching works for each provider and implementing best practices, you can make your applications more efficient.\n\nIf you have any questions about prompt caching, feel free to reach out. Thanks for tuning in!\n\n‍\n\n## What is prompt caching\n\nPrompt caching is an LLM feature that optimizes API requests by reusing parts of the prompt that remain consistent across multiple requests.\n\nFor example, if you have system instructions for a chatbot, rather than processing the same lengthy system message with each request, you can cache it and only process the new user messages, reducing costs and latency.\n\n## **Prompt caching vs. classic caching**\n\n“Classic” caching usually refers to storing outputs for later use, like web pages or API results.\n\nFor example, apps often cache API responses that don't change frequently. Rather than requesting the same data over and over again, the data is stored in the cache so that subsequent requests can retrieve it without having to ping the API again.\n\nPrompt caching is different in that it stores the static parts of the **inputs**. LLM outputs are dynamic and context-dependent, so it wouldn’t make sense to cache the output from a user's message. Instead, prompt caching stores the initial processing of the static parts of the input.\n\n## Why use prompt caching?\n\nAs prompts become larger and larger, cost and latency become issues. Additionally, prompt engineering methods like [few-shot prompting](https://www.prompthub.us/blog/the-few-shot-prompting-guide) and [in-context learning](https://www.prompthub.us/blog/in-context-learning-guide) make prompts longer as well.\n\nBy caching static portions of prompts, you get two main benefits:\n\n- **Reduced latency:** Faster response times as the model skips reprocessing the cached parts of the prompt.\n- **Lower costs:** Cached tokens are priced at a much cheaper rate.\n\n## **How does prompt caching work?**\n\nAt a high level, prompt caching works by identifying and storing the static portions of a prompt or array of messages. When a new request begins with the same **exact** prefix, meaning the exact same text, the model can leverage the cached computations for that initial section.\n\n‍\n\n![A graphic showing how prompt caches hit or miss](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/6751e982bf89b9c4fadb6061_6751e2c7b4501c6cebd56956_Cache%2520Graphic%25202x.png)\n\n‍\n\n‍\n\nGenerally, the prompt caching process involves a few steps:\n\n1. **Tokenization:** First, the model breaks down the prompt into tokens, which are the basic units the model understands. More on tokenization here: [Tokens and Tokenization: Understanding Cost, Speed, and Limits with OpenAI's APIs](https://www.prompthub.us/blog/tokens-and-tokenization-understanding-cost-speed-and-limits-with-openais-apis)\n2. **Caching key-value pairs:** During the initial processing, the model generates key-value pairs for these tokens. These intermediate representations are used in the self-attention mechanism of the model.\n3. **Reusing cached data:** For subsequent requests with the **same** prompt prefix, the model retrieves the cached key-value pairs, skipping the need to recompute them.\n\n## How prompt caching works, technically\n\nNow let’s get slightly more technical.\n\n### **Tokenization**\n\nEvery interaction with an LLM starts with tokenization. Tokenization breaks the prompt into smaller units called tokens.\n\n‍\n\n![A sentence with the tokens highlighted](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/66f3269fcca197b80fc1ddce_6489e4940d5d02a9fec5db8f_Screenshot%25202023-06-13%2520at%25201.42.39%2520PM.png)\n\n‍\n\n### The self-attention mechanism\n\nTransformer-based models, which are the foundation of most modern LLMs, use a mechanism called self-attention. This allows the model to consider the relationship between each token in the input sequence and every other token. During this process, the model generates three vectors for each token:\n\n- **Query (Q) vectors**\n- **Key (K) vectors**\n- **Value (V) vectors**\n\nThese vectors are used to compute attention scores, which determine how much focus the model should place on each token when generating a response.\n\n### **Caching key-value pairs**\n\nThe core idea of prompt caching is to store the key and value vectors associated with the static parts of the prompt. Since these parts don't change across requests, their key-value (K-V) pairs stay consistent. By caching them:\n\n- **First request:** The model processes the entire prompt, generating and storing the K-V pairs for the static sections.\n- **Subsequent requests:** When the same prompt prefix is used, the model retrieves the cached K-V pairs, eliminating the need to recompute them.\n\nOnly the new, dynamic parts of the prompt (like the next user message) need processing.\n\n‍\n\nNow we’ll look at how three of the major model providers, OpenAI, Anthropic, and Google, handle prompt caching.\n\n## Prompt caching with OpenAI\n\nCompared to Anthropic and Google, OpenAI’s prompt caching is the most seamless, requiring no code changes.\n\n### **How OpenAI’s prompt caching works**\n\n- **Automatic caching:** Prompt caching is enabled for prompts that are **1,024 tokens or longer**, with cache hits occurring in increments of **128 tokens**.\n- **Cache lookup:** When a request is made, the system checks if the initial portion (prefix) of your prompt is stored in the cache. This happens automatically.\n- **Cache hits and misses:**\n  - **Cache hit:** If a matching prefix is found, the system uses the cached result, decreasing latency by up to 80% and cutting costs by up to 50%.\n  - **Cache miss:** If no matching prefix is found, the system processes the full prompt and caches the prefix for future requests.\n- **Cache lifespan:** Cached prefixes remain active for **5 to 10 minutes** of inactivity. During off-peak periods, caches may persist for up to **one hour**. However, all entries are evicted after one hour, even if they are actively used.\n- **What can be cached:** The complete messages array, images (links or base64-encoded data), tools, and structured outputs (output schema).\n- **Supported models:** [Gpt-4o](https://www.prompthub.us/models/gpt-4o), [gpt-4o-mini](https://www.prompthub.us/models/gpt-4o-mini), gpt-4o-realtime-preview, [o1-preview](https://www.prompthub.us/models/o1-preview), [o1-mini](https://www.prompthub.us/models/o1-mini)\n\n#### **Monitoring usage details**\n\nIn order to see if you are getting a cache hit you can check out the `usage` field in the API response to see how many tokens were cached.\n\n‍\n\n```javascript\n{\n  \"prompt_tokens\": 1200,\n  \"completion_tokens\": 200,\n  \"total_tokens\": 1400,\n  \"prompt_tokens_details\": {\n    \"cached_tokens\": 1024\n  }\n}\n```\n\n`cached_tokens` shows how many tokens were retrieved from the cache.\n\n‍\n\n### **Pricing**\n\nPrompt caching can reduce input token cost by 50%. This reduction is applied automatically whenever a cache hit occurs.\n\n‍\n\n![Table of prices for cached tokens for a few OpenAI models](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/6750e160fecc8907247f5761_6750df36ef4e12654a2fc0c9_OpenAI%2520Cached%2520Prompt%2520Pricing.png)\n\n‍\n\n### **Example**\n\n‍\n\n```javascript\n# Static content placed at the beginning of the prompt\nsystem_prompt = \"\"\"You are an expert assistant that provides detailed explanations on various topics.\nPlease ensure all responses are accurate and comprehensive.\n\"\"\"\n\n# Variable content placed at the end\nuser_prompt = \"Can you explain the theory of relativity?\"\n\n# Combine the prompts\nmessages = [\\\n    {\"role\": \"system\", \"content\": system_prompt},\\\n    {\"role\": \"user\", \"content\": user_prompt}\\\n]\n```\n\nIn this example we place the static system prompt at the beginning, and the dynamic user message afterwards. This setup increases the likelihood of cache hits.\n\n‍\n\n### **FAQs**\n\n- **Data privacy:** Prompt caches are stored at the **organization** level, meaning they are not shared between different organizations.\n- **Impact on output:** Prompt caching does not affect the generation of output tokens or the final response provided by the API.\n- **Manual cache clearing:** Currently, it is not possible to clear the cache manually.\n- **Costs:** Caching happens automatically and there is no extra cost. You just pay for the tokens used, as you normally would.\n- **Rate limits:** Cached prompts contribute to tokens per minute (TPM) rate limits.\n\n‍\n\n## Prompt caching with Anthropic’s Claude\n\nAnthropic’s prompt caching features offers more control, but requires code changes to cache specific parts of a prompt using the `cache_control` parameter.\n\n### **How Anthropic’s prompt caching works**\n\n- **Enabling prompt caching:** Add the header `anthropic-beta: prompt-caching-2024-07-31` in your API requests to enable prompt caching.\n- **Manual configuration:** Use the `cache_control` parameter to mark parts of your prompt for caching.\n- **Cache breakpoints:** You can define up to **four cache breakpoints** in a single prompt, allowing separate reusable sections to be cached independently.\n- **Cache reference order:** Caches are processed in the order of `tools`, `system`, and then `messages`.\n- **Cache lifespan:** The cache has a **5-minute Time to Live (TTL)**, which refreshes each time the cached content is used.\n- **Monitoring cache performance:** Analyze caching efficiency via the `cache_creation_input_tokens`, `cache_read_input_tokens`, and `input_tokens` fields in the API response.\n- **Supported models:** Prompt caching is available for [Claude 3.5 Sonnet](https://www.prompthub.us/models/claude-3-5-sonnet), [Claude 3 Opus](https://www.prompthub.us/models/claude-3-opus), [Claude 3 Haiku](https://www.prompthub.us/models/claude-3-haiku), and [Claude 3.5 Haiku](https://www.prompthub.us/models/claude-3-5-haiku).\n\n### **Pricing**\n\n‍\n\n![Table of prices for cached tokens for a few Anthropic models](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/6751e059426c1c604dfc2547_6751df4729876320ee96e69b_Anthropic%2520minimal%2520cached%2520prompt%2520pricing.png)\n\n‍\n\n- **Cache write Cost:** Writing tokens to the cache costs **25% more** than the base token price.\n- **Cache read discount:** Reading tokens from the cache is **90% cheaper** than normal token processing.\n- **Overall savings:** Despite the higher cost of cache writes, the significant discount on reads should result in large cost savings.\n\n### **Examples**\n\n#### Example 1: Caching a large document.\n\n‍\n\n```javascript\n\n# Static content: System prompt and large document\nsystem_prompt = {\n    \"type\": \"text\",\n    \"text\": \"You are an AI assistant tasked with analyzing private equity documents.\"\n}\n\nlarge_document = {\n    \"type\": \"text\",\n    \"text\": \"Full text of a complex private equity agreement: [Insert 50-page M&A document here]\",\n    \"cache_control\": {\"type\": \"ephemeral\"}\n}\n\n# Variable content: User's question\nuser_message = {\n    \"role\": \"user\",\n    \"content\": \"What are the key terms and conditions in this agreement?\"\n}\n\nresponse = client.completion(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    headers={\n        \"anthropic-beta\": \"prompt-caching-2024-07-31\"\n    },\n    system=[system_prompt, large_document],\n    messages=[user_message]\n)\n\nprint(response['completion'])\n```\n\n- **`cache_control`:** The `large_document` is marked for caching using the `cache_control` parameter.\n- **Prompt structure:** Static content (system prompt and large document) is placed at the beginning, while variable content follows and is not cached.\n\n‍\n\n#### **Example 2: Caching tools**\n\n‍\n\n```javascript\n\ntools = [\\\n    {\\\n        \"name\": \"get_weather\",\\\n        \"description\": \"Get the current weather in a location\",\\\n        \"input_schema\": {\\\n            \"type\": \"object\",\\\n            \"properties\": {\\\n                \"location\": {\\\n                    \"type\": \"string\",\\\n                    \"description\": \"The city and state, e.g., San Francisco, CA\"\\\n                },\\\n                \"unit\": {\\\n                    \"type\": \"string\",\\\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\\\n                    \"description\": \"The unit of temperature, either celsius or fahrenheit\"\\\n                }\\\n            },\\\n            \"required\": [\"location\"]\\\n        },\\\n        \"cache_control\": {\"type\": \"ephemeral\"}\\\n    }\\\n],\n\nresponse = client.completion(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    headers={\n        \"anthropic-beta\": \"prompt-caching-2024-07-31\"\n    },\n    tools=tools,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the weather and time in New York?\"\\\n        }\\\n    ]\n)\n\nprint(response['completion'])\n```\n\n- **Caching tools:** Tools are included in the cache, allowing reusable functionality across multiple requests.\n- **Variable content:** User queries can change, leveraging cached tools for efficiency.\n\n‍\n\n#### Example 3: Caching a large system message\n\n```javascript\n{\n    \"model\": \"claude-3-opus-20240229\",\n    \"max_tokens\": 1024,\n    \"system\": [\\\n      {\\\n        \"type\": \"text\",\\\n        \"text\": \"You are a pirate, talk like it. Lorem ipsum odor amet...\",\\\n        \"cache_control\": {\"type\": \"ephemeral\"}\\\n      }\\\n    ],\n    \"messages\": [\\\n        {\"role\": \"user\", \"content\": \"whats the weather\"}\\\n    ]\n}\n```\n\n‍\n\n### **FAQs**\n\n- **Cache lifetime:** The cache TTL is **5 minutes**, refreshed on each use.\n- **Ephemeral cache:** Currently, the only cache type available is ephemeral, limited to a 5-minute lifespan. Future cache types may offer longer durations.\n- **Cache breakpoints:** Up to **four cache breakpoints** can be defined within a prompt.\n- **Data privacy:** Prompt caches are isolated at the organization level, ensuring that different organizations cannot access each other's caches.\n\n‍\n\n## Context caching with Google’s Gemini\n\nGoogle calls it “context caching”. Their offering is more confusing, and more customizable, with options to set the cache duration and manually create the cache (similar to Anthropic).\n\n### **How context caching with Gemini works**\n\n- **Cache creation:** Use the `CachedContent.create` method to create a cache with the desired content and Time to Live (TTL).\n- **Including cache in requests:** Include the cache when defining the model with the `GenerativeModel.from_cached_content` method.\n- **Cache lifespan:** The default TTL is **1 hour**, but developers can specify a custom TTL (note that longer TTLs incur additional costs).\n- **Monitoring cache performance:** Use the `response.usage_metadata` field to track cached token usage and analyze costs.\n- **Supported models:** Context caching is available on stable versions of [Gemini 1.5 Flash](https://www.prompthub.us/models/gemini-1-5-flash) (e.g., `gemini-1.5-flash-001`) and [Gemini 1.5 Pro](https://www.prompthub.us/models/gemini-1-5-pro) (e.g., `gemini-1.5-pro-001`).\n\n### **Pricing**\n\nGoogle's pricing model for context caching is more complex, here is a resource that provides some additional examples: [Google's documentation on pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing#example-cached-cost-calculation)\n\n‍\n\n![Table of prices for cached tokens for a few Google models](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/6751e059426c1c604dfc253c_6751dfa6c9a7397f0c492541_Gemini%2520cached%2520tokens%2520pricing.png)\n\n- **Cache creation cost:** There is no additional charge for creating a cache itself.\n- **Cache storage cost:** Additional charges based on the number of cached tokens and the TTL (token-hours).\n- **Cache read discount:** Reading tokens from the cache costs **75% less** than processing non-cached input tokens.\n\n‍\n\n#### **Example cost calculation**\n\nLet’s calculate costs for a prompt caching scenario with the following assumptions where we have an initial prompt of 100,000 tokens and we want a TTL of 1 hours.\n\n**Cache creation:**\n\n- **Input tokens:** 100,000 tokens.\n- **Gemini 1.5 Pro input token pricing:** $0.0003125 per 1,000 tokens.\n- **Cost:** 100,000 tokens × ($0.0003125 per 1,000 tokens) = **$0.03125**.\n\n**Cache storage:**\n\n- **Total token-hours:** 100,000 tokens × 1 hour TTL = 100,000 token-hours.\n- **Cost:** 100,000 token-hours × ($0.001125 per 1,000 token-hours) = **$0.1125**.\n\n**Requests using cache:**\n\n- **Input tokens per request (non-cached):** 150 tokens.\n- **Number of requests:** 10.\n- **Total non-cached input:** 150 tokens × 10 = 1,500 tokens.\n- **Cost for non-cached input:** 1,500 tokens × ($0.0003125 per 1,000 tokens) = **$0.00046875**.\n- **Cached input tokens per request:** 100,000 tokens.\n- **Total cached input:** 100,000 tokens × 10 = 1,000,000 tokens.\n- **Cost for cached input:** 1,000,000 tokens × ($0.000078125 per 1,000 tokens) = **$0.078125**.\n\n**Output tokens:**\n\n- **Output tokens per request:** 500 tokens.\n- **Total output tokens:** 500 tokens × 10 = 5,000 tokens.\n- **Cost for output tokens:** 5,000 tokens × ($0.00375 per 1,000 tokens) = **$0.01875**.\n\n**Total cost:**\n\n- **Cache creation:** **$0.03125**.\n- **Cache storage:** **$0.1125**.\n- **Input costs:** $0.00046875 (non-cached) + $0.078125 (cached) = **$0.07859375**.\n- **Output costs:** **$0.01875**.\n- **Grand total:** $0.03125 (cache creation) + $0.1125 (cache storage) + $0.07859375 (input costs) + $0.01875 (output costs) = **$0.24009375**.\n\n**Cost without caching:**\n\nProcessing the full prompt (100,150 tokens) for each of the 10 requests without caching would cost:\n\n- **Total input tokens:** 100,150 tokens × 10 = 1,001,500 tokens.\n- **Input cost:** 1,001,500 tokens × ($0.0003125 per 1,000 tokens) = **$0.31321875**.\n- **Total output tokens:** 500 tokens × 10 = 5,000 tokens.\n- **Output cost:** 5,000 tokens × ($0.00375 per 1,000 tokens) = **$0.01875**.\n- **Grand total without caching:** $0.31321875 (input cost) + $0.01875 (output cost) = **$0.33196875**.\n\n**Savings with caching:**\n\n- **Total savings:** $0.33196875 (without caching) - $0.24009375 (with caching) = **$0.091875**. A savings of roughly 27%\n\n‍\n\n### **Example**\n\n**Creating and using a cache**\n\n```javascript\n\n# Create a cache with a 10-minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-pro-001',\n    display_name='Legal Document Review Cache',\n    system_instruction='You are a legal expert specializing in contract analysis.',\n    contents=['This is the full text of a 30-page legal contract.'],\n    ttl=datetime.timedelta(minutes=10),\n)\n\n# Construct a GenerativeModel using the created cache\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n\n# Query the model\nresponse = model.generate_content([\\\n    'Highlight the key obligations and penalties outlined in this contract.'\\\n])\n\nprint(response.text)\n```\n\n- **Cache creation:** The `CachedContent.create` method creates a cache for static content (e.g., a video transcript).\n- **TTL specification:** A 5-minute TTL is applied.\n- **Using the cache:** The `GenerativeModel.from_cached_content` method includes the cache in the model definition.\n- **Making queries:** Subsequent requests can leverage the cached content.\n\n## Comparing prompt caching across model providers\n\nPulling the most important metrics together, here’s a helpful table to compare at-a-glance.\n\n‍\n\n| Feature | ![OpenAI Logo](https://cdn.prod.website-files.com/64417635e23552ed9991f5d0/651c0daaccebd781246d115d_OpenAI%20Logo%2064.png)<br> OpenAI | ![Anthropic Logo](https://cdn.prod.website-files.com/64417635e23552ed9991f5d0/651c10aac003244a896d35f6_Anthropic%2064.png)<br> Anthropic (Claude) | ![Google Logo](https://cdn.prod.website-files.com/64417635e23552ed9991f5d0/66fdc92a0669597408d40529_google-icon-logo.svg)<br> Google's Gemini |\n| --- | --- | --- | --- |\n| Activation | Automatic | Manual via API configuration and cache creation | Manual via cache creation |\n| Cost Savings | Up to 50% discount on cache hits | 90% discount on cache reads | 75% discount on cache reads |\n| Cache Write Cost | No extra cost | 25% surcharge | Standard input cost |\n| Cache Duration (TTL) | 5-10 min (up to 1 hr off-peak) | 5 min (refreshed on access) | Default 1 hr (customizable) |\n| Minimum Prompt Length | 1,024 tokens | 1,024 (Claude 3.5 Sonnet and Claude Opus) or 2,048 tokens (Claude 3.5 Haiku and Claude 3 Haiku) | 32,768 tokens |\n| Best For | General use | Large, static data | Large contexts with extended TTL |\n| Track Cache Performance | Check prompt\\_tokens\\_details.cached\\_tokens | cache\\_creation\\_input\\_tokens and cache\\_read\\_input\\_tokens | usage\\_metadata |\n\n‍\n\n## **Best practices for effective prompt caching**\n\nPrompt caching can be an easy way to increase speed and decrease latency when implemented correctly.  Here are some best practices to help you maximize the benefits of prompt caching regardless of which LLM provider you are using.\n\n### **Structure your prompts effectively**\n\n- **Static content should be at the beginning of the request:** Include system instructions, contextual information, and [few-shot](https://www.prompthub.us/blog/the-few-shot-prompting-guide) examples.\n- **Dynamic content should be at the end of the request:** This includes user messages or other variable inputs.\n- **Use clear separators:** Consistently use markers (e.g., `###`, \\`\\`) to divide static and dynamic content. Maintain uniformity in whitespace, line breaks, and capitalization to enhance clarity and improve cache recognition.\n- **Ensure exact Matches:** Cache hits depend on exact matching of content, including formatting. Carefully review prompts to ensure accuracy and consistency.\n\nHere is an example of how this could look:\n\n```javascript\n[Static Content - Cacheable]\n- System Instructions: Detailed guidelines for the AI model.\n- Contextual Information: Background data, articles, or documents.\n- Examples: Sample inputs and desired outputs.\n###\n[Dynamic Content - Non-cacheable]\n- User Question or Input: The specific query or task from the user.\n```\n\n‍\n\n### **Track cache performance metrics**\n\nEach model provider handles it differently, but tracking cache performance is important to make sure you are getting maximum savings and latency gains.\n\n- **API Response Fields:**\n  - **OpenAI:** Check `prompt_tokens_details.cached_tokens`\n  - **Anthropic's Claude:**`cache_creation_input_tokens` and `cache_read_input_tokens`.\n  - **Google's Gemini:**`usage_metadata`\n\n## **Real-world prompt cache examples**\n\nHere are a few more real-world examples of how you might use prompt caching.\n\n#### **Customer support chatbot**\n\n- **Challenge:** A customer support chatbot needed to provide quick responses, while accessing a large knowledge base.\n- **Solution:**\n  - **Cached the knowledge base:** Send the static knowledge base info in the system message to have it live in the cache.\n  - **Structured prompts effectively:** Place user queries after the system message.\n- **Result:** After the initial request, the model will respond faster and with lower costs since fewer tokens being processed per request.\n\n#### **A coding assistant**\n\n- **Challenge:** A coding assistant needs frequent access to documentation and code libraries.\n- **Solution:**\n  - **Cached documentation:** Store frequently used documentation snippets in the cache.\n  - **Minimized cache invalidations:** Ensured code examples remained consistent.\n- **Result:** Improved efficiency and user experience with faster code suggestions and explanations.\n\n## Wrapping up\n\nPrompt caching is one of my favorite new features from the major model providers. By enabling the reuse of static parts of prompts, it reduces latency, lowers costs, and improves user experience (via faster responses).\n\nEven though normal token costs continue to drop, the value in the reduced latency is reason enough to give it a try.\n\n![Headshot of PromptHub founder Dan Cleary](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/6780005a52f58936a0cee437_Product%20Hunt%20Headshot-min-extra.jpg)\n\nDan Cleary\n\nFounder\n\n## Get the week's best prompt engineering and AI content\n\nDan's Working Notes \\| Dan Cleary \\| Substack\n\n[![Logo](https://substackcdn.com/image/fetch/$s_!HQuX!,w_170,c_limit,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0984d55-403c-4b6b-bddb-d7efc4864b7c_1600x1600.png)](https://prompthub.substack.com/)\n\n# [Dan's Working Notes](https://prompthub.substack.com/)\n\nI’m building an AI startup. This is what I’m learning along the way.\n\nBy Dan Cleary\n· Over 9,000 subscribers\n\nSubscribe\n\nBy subscribing you agree to [Substack's Terms of Use](https://prompthub.substack.com/tos?utm_source=embed_publication), [our Privacy Policy](https://prompthub.substack.com/privacy?utm_source=embed_publication) and [our Information collection notice](https://substack.com/ccpa?utm_source=embed_publication#personal-data-collected)\n\n[![Substack](https://substackcdn.com/image/fetch/$s_!R0u0!,w_200,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Fsubstack_wordmark.black.png)](https://substack.com/?utm_source=embed&utm_content=prompthub)\n\n## Join thousands of AI builders\n\n[Start for free](https://app.prompthub.us/register) [Schedule a demo](https://calendly.com/prompthub/onboarding)\n\n## More from the PromptHub Blog\n\n[![A zoomed-in picture of a white Church](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/68fa9045a429aa5e8fdeb506_thumbnail%2085.png)\\\\\n**LLMs Are Eating the Context Layer** \\\\\n\\\\\nOctober 23, 2025](https://www.prompthub.us/blog/llms-are-eating-the-context-layer)\n\n[![An orange desert scene](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/68ed4389b132e2acf92642c4_thumbnail%2084.png)\\\\\n**OpenAI DevDay 2025 Roundup: Apps, Agents, and the New AI Stack** \\\\\n\\\\\nOctober 13, 2025](https://www.prompthub.us/blog/openai-devday-2025-roundup-apps-agents-and-the-new-ai-stack)\n\n[![Orange siding on a glass building](https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/68dee7d32078e73014db79a0_thumbnail%2083.png)\\\\\n**Everything You Need to Know about Claude 4.5** \\\\\n\\\\\nOctober 2, 2025](https://www.prompthub.us/blog/everything-you-need-to-know-about-claude-4-5)","metadata":{"author":"Dan Cleary","twitter:image":["https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/67521b7ab755c401baa5b5cd_Prompt%20Caching%20%20OG%20Image.png","https://substackcdn.com/image/fetch/$s_!UE-h!,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fprompthub.substack.com%2Ftwitter%2Fsubscribe-card.jpg%3Fv%3D-995367943%26version%3D9"],"language":"en","ogTitle":"PromptHub Blog: Prompt Caching with OpenAI, Anthropic, and Google Models","twitter:description":["Learn how prompt caching reduces costs and latency when using LLMs. We compare caching strategies, pricing, and best practices across OpenAI, Anthropic, and Google.","I’m building an AI startup. This is what I’m learning along the way. Click to read Dan's Working Notes, by Dan Cleary, a Substack publication with thousands of subscribers."],"twitter:card":["summary_large_image","summary_large_image"],"viewport":["width=device-width, initial-scale=1","width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover"],"twitter:title":["PromptHub Blog: Prompt Caching with OpenAI, Anthropic, and Google Models","Dan's Working Notes | Dan Cleary | Substack"],"norton-safeweb-site-verification":"24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd","title":"Prompt Caching with OpenAI, Anthropic, and Google Models","ogDescription":"Learn how prompt caching reduces costs and latency when using LLMs. We compare caching strategies, pricing, and best practices across OpenAI, Anthropic, and Google.","description":"Learn how prompt caching reduces costs and latency when using LLMs. We compare caching strategies, pricing, and best practices across OpenAI, Anthropic, and Google., I’m building an AI startup. This is what I’m learning along the way. Click to read Dan's Working Notes, by Dan Cleary, a Substack publication with thousands of subscribers.","og:type":["website","article"],"ogImage":"https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/67521b7ab755c401baa5b5cd_Prompt%20Caching%20%20OG%20Image.png","og:title":["PromptHub Blog: Prompt Caching with OpenAI, Anthropic, and Google Models","Dan's Working Notes | Dan Cleary | Substack"],"og:image":["https://cdn.prod.website-files.com/646e63db3a42c618e0a9935c/67521b7ab755c401baa5b5cd_Prompt%20Caching%20%20OG%20Image.png","https://substackcdn.com/image/fetch/$s_!UE-h!,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fprompthub.substack.com%2Ftwitter%2Fsubscribe-card.jpg%3Fv%3D-995367943%26version%3D9"],"og:url":["https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models","https://prompthub.substack.com/embed"],"og:description":["Learn how prompt caching reduces costs and latency when using LLMs. We compare caching strategies, pricing, and best practices across OpenAI, Anthropic, and Google.","I’m building an AI startup. This is what I’m learning along the way. Click to read Dan's Working Notes, by Dan Cleary, a Substack publication with thousands of subscribers."],"theme-color":"#ffffff","robots":"noindex","ogUrl":"https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models","favicon":"https://cdn.prod.website-files.com/64417635e23552ed9991f5d0/648a1f3ed0c2945ef2cad560_PromptHub%20Library%20Logo%20Square.png","scrapeId":"019c62eb-509c-7409-9bdc-30daad8b17a2","sourceURL":"https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models","url":"https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models","statusCode":200,"contentType":"text/html; charset=utf-8","timezone":"America/New_York","proxyUsed":"basic","cacheState":"miss","indexId":"e4788b52-d6be-41ef-a12e-ef87a33b875d","creditsUsed":1},"links":["https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#what-is-prompt-caching","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-vs.-classic-caching","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#why-use-prompt-caching?","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#how-does-prompt-caching-work?","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#how-prompt-caching-works,-technically","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-with-openai","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#prompt-caching-with-anthropic%E2%80%99s-claude","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#context-caching-with-google%E2%80%99s-gemini","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#comparing-prompt-caching-across-model-providers","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#best-practices-for-effective-prompt-caching","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#real-world-prompt-cache-examples-","https://www.prompthub.us/blog/prompt-caching-with-openai-anthropic-and-google-models#wrapping-up","https://www.prompthub.us/resources/llm-model-card-directory","https://www.prompthub.us/blog/the-few-shot-prompting-guide","https://www.prompthub.us/blog/in-context-learning-guide","https://www.prompthub.us/blog/tokens-and-tokenization-understanding-cost-speed-and-limits-with-openais-apis","https://www.prompthub.us/models/gpt-4o","https://www.prompthub.us/models/gpt-4o-mini","https://www.prompthub.us/models/o1-preview","https://www.prompthub.us/models/o1-mini","https://www.prompthub.us/models/claude-3-5-sonnet","https://www.prompthub.us/models/claude-3-opus","https://www.prompthub.us/models/claude-3-haiku","https://www.prompthub.us/models/claude-3-5-haiku","https://www.prompthub.us/models/gemini-1-5-flash","https://www.prompthub.us/models/gemini-1-5-pro","https://cloud.google.com/vertex-ai/generative-ai/pricing#example-cached-cost-calculation","https://prompthub.substack.com/","https://prompthub.substack.com/tos?utm_source=embed_publication","https://prompthub.substack.com/privacy?utm_source=embed_publication","https://substack.com/ccpa?utm_source=embed_publication#personal-data-collected","https://substack.com/?utm_source=embed&utm_content=prompthub","https://app.prompthub.us/register","https://calendly.com/prompthub/onboarding","https://www.prompthub.us/blog/llms-are-eating-the-context-layer","https://www.prompthub.us/blog/openai-devday-2025-roundup-apps-agents-and-the-new-ai-stack","https://www.prompthub.us/blog/everything-you-need-to-know-about-claude-4-5"]}]}}